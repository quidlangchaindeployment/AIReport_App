# ---
# app.py (AI Data Analysis App - Refactored Version)
#
# ã“ã®ã‚³ãƒ¼ãƒ‰ã¯ã€å•†ç”¨åˆ©ç”¨ãŒå®¹æ˜“ãªå¯›å®¹ãªï¼ˆpermissiveï¼‰ãƒ©ã‚¤ã‚»ãƒ³ã‚¹
# (ä¾‹: MIT, Apache License 2.0, BSD) ã®ä¸‹ã§åˆ©ç”¨å¯èƒ½ãª
# ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã€ã¾ãŸã¯ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã«ä¾å­˜ã—ãªã„ã‚³ãƒ¼ãƒ‰ã®ã¿ã‚’ä½¿ç”¨ã—ã¦å®Ÿè£…ã•ã‚Œã¦ã„ã¾ã™ã€‚
# GPL, AGPL, SSPLãªã©ã®ã‚³ãƒ”ãƒ¼ãƒ¬ãƒ•ãƒˆåŠ¹æœã‚’æŒã¤ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¯ä½¿ç”¨ã—ã¦ã„ã¾ã›ã‚“ã€‚
# ---

# --- 1. ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ---
import streamlit as st
import pandas as pd
import numpy as np
import os
import re
import json
import logging
import time
import spacy
import altair as alt
import networkx as nx
from networkx.algorithms import community
from pyvis.network import Network
import streamlit.components.v1 as components
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from io import StringIO, BytesIO
from typing import Optional, Dict, List, Any, Union
import random  # (â˜…) Tipsãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã®ãŸã‚ã«è¿½åŠ 
from dotenv import load_dotenv  # (â˜…) .envèª­ã¿è¾¼ã¿ã®ãŸã‚ã«è¿½åŠ 
import matplotlib
matplotlib.use('Agg') # (â˜…) Streamlitã®ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã§å‹•ä½œã•ã›ã‚‹ãŸã‚ã®ãŠã¾ã˜ãªã„
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import base64
from wordcloud import WordCloud

# (â˜…) --- matplotlib æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š ---
# Dockerfileã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ãŸIPAãƒ•ã‚©ãƒ³ãƒˆã®ãƒ‘ã‚¹ã‚’æŒ‡å®š
# (â˜…) ã”æ³¨æ„: ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§å®Ÿè¡Œã™ã‚‹å ´åˆã€ã“ã®ãƒ•ã‚©ãƒ³ãƒˆãƒ‘ã‚¹ãŒç•°ãªã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
# (â˜…) Dockerç’°å¢ƒ (Debianãƒ™ãƒ¼ã‚¹) ã‚’å‰æã¨ã—ã¦ã„ã¾ã™ã€‚
try:
    # (â˜…) Dockerfileå†…ã®ãƒ‘ã‚¹
    font_path = '/usr/share/fonts/opentype/ipafont-gothic/ipagp.ttf' 
    if os.path.exists(font_path):
        fm.fontManager.addfont(font_path)
        plt.rcParams['font.family'] = 'IPAGothic'
        # logger.info(f"æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆ '{font_path}' ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")
    else:
        # (â˜…) ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ (ç’°å¢ƒã«ã‚ˆã£ã¦èª¿æ•´ãŒå¿…è¦)
        logger.warning(f"æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆ '{font_path}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚°ãƒ©ãƒ•ã®æ—¥æœ¬èªãŒæ–‡å­—åŒ–ã‘ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
        # (â˜…) ä»£æ›¿ãƒ•ã‚©ãƒ³ãƒˆã®æ¤œç´¢ (ã‚„ã‚„æ™‚é–“ãŒã‹ã‹ã‚‹ãŒå …ç‰¢)
        try:
            jp_font = fm.findfont(fm.FontProperties(family='IPAexGothic'))
            plt.rcParams['font.family'] = 'IPAexGothic'
            logger.info(f"ä»£æ›¿ãƒ•ã‚©ãƒ³ãƒˆ '{jp_font}' ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
        except:
             logger.error("ä»£æ›¿ã®æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã‚‚è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
             plt.rcParams['font.family'] = 'sans-serif'
except Exception as e:
    logger.error(f"matplotlibæ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®šã‚¨ãƒ©ãƒ¼: {e}")

# (â˜…) LangChain / Google Generative AI ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
# ãƒ©ã‚¤ã‚»ãƒ³ã‚¹: Apache License 2.0
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser

# (â˜…) Step D (PowerPointç”Ÿæˆ) ã§å¿…è¦ã¨ãªã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
# ãƒ©ã‚¤ã‚»ãƒ³ã‚¹: MIT License
try:
    import pptx
    from pptx import Presentation
    from pptx.util import Inches, Pt
except ImportError:
    st.error(
        "PowerPointç”Ÿæˆãƒ©ã‚¤ãƒ–ãƒ©ãƒª(python-pptx)ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
        "pip install python-pptx ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚"
    )

# (â˜…) Step D (ãƒ‰ãƒ©ãƒƒã‚°ï¼†ãƒ‰ãƒ­ãƒƒãƒ—UI) ã§å¿…è¦ã¨ãªã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
# ãƒ©ã‚¤ã‚»ãƒ³ã‚¹: MIT License
try:
    from streamlit_sortables import sort_items
except ImportError:
    st.error(
        "UIãƒ©ã‚¤ãƒ–ãƒ©ãƒª(streamlit-sortables)ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
        "pip install streamlit-sortables ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚"
    )

# æ—¢å­˜ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒª (openpyxl, ja_core_news_sm) ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    import openpyxl
except ImportError:
    st.error("Excel (openpyxl) ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚`pip install openpyxl` ã—ã¦ãã ã•ã„ã€‚")
try:
    import ja_core_news_sm
except ImportError:
    st.error("spaCyæ—¥æœ¬èªãƒ¢ãƒ‡ãƒ« (ja_core_news_sm) ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚`python -m spacy download ja_core_news_sm` ã—ã¦ãã ã•ã„ã€‚")

# --- 2. (â˜…) å®šæ•°å®šç¾© ---

# (â˜…) è¦ä»¶ã«åŸºã¥ãã€ä½¿ç”¨ã™ã‚‹AIãƒ¢ãƒ‡ãƒ«ã‚’å®šæ•°ã¨ã—ã¦å®šç¾©
MODEL_FLASH_LITE = "gemini-2.5-flash-lite" # Step A, B (é«˜é€Ÿãƒ»åŠ¹ç‡çš„)
MODEL_FLASH = "gemini-2.5-flash"         # Step D (ä»£æ›¿)
MODEL_PRO = "gemini-2.5-pro"             # Step C, D (é«˜å“è³ª)

# ãƒãƒƒãƒã‚µã‚¤ã‚ºã¨å¾…æ©Ÿæ™‚é–“ (KISS)
FILTER_BATCH_SIZE = 50
FILTER_SLEEP_TIME = 6.1  # Rate Limit å¯¾ç­– (10 requests per 60 seconds)
TAGGING_BATCH_SIZE = 10
TAGGING_SLEEP_TIME = 6.1  # Rate Limit å¯¾ç­–

# åœ°åè¾æ›¸
try:
    from geography_db import JAPAN_GEOGRAPHY_DB
except ImportError:
    st.error("åœ°åè¾æ›¸ãƒ•ã‚¡ã‚¤ãƒ« (geography_db.py) ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
    JAPAN_GEOGRAPHY_DB = {}


# --- 3. ãƒ­ã‚¬ãƒ¼è¨­å®š ---
class StreamlitLogHandler(logging.Handler):
    """Streamlitã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã«ãƒ­ã‚°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ ã™ã‚‹ãƒãƒ³ãƒ‰ãƒ©"""
    def __init__(self):
        super().__init__()
        if 'log_messages' not in st.session_state:
            st.session_state.log_messages = []

    def emit(self, record):
        log_entry = self.format(record)
        st.session_state.log_messages.append(log_entry)
        st.session_state.log_messages = st.session_state.log_messages[-500:]

logger = logging.getLogger(__name__)
if not logger.handlers:
    logger.setLevel(logging.INFO)
    handler = StreamlitLogHandler()
    handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
    logger.addHandler(handler)


# --- 4. (â˜…) AIãƒ¢ãƒ‡ãƒ«ãƒ»NLPãƒ¢ãƒ‡ãƒ«ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç† ---

# (â˜…) è¦ä»¶ã«åŸºã¥ãã€ç•°ãªã‚‹ãƒ¢ãƒ‡ãƒ«åã‚’æŒ‡å®šã—ã¦LLMã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹é–¢æ•°ã«åˆ·æ–°
@st.cache_resource(ttl=3600)
def get_llm(
    model_name: str, 
    temperature: float = 0.0,
    timeout_seconds: int = 120  # (â˜…) --- ä¿®æ­£: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå¼•æ•°ã‚’è¿½åŠ  (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ120ç§’) ---
) -> Optional[ChatGoogleGenerativeAI]:
    """
    æŒ‡å®šã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«åã€æ¸©åº¦ã€ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã§LLM (Google Gemini) ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ãƒ»ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚‹ã€‚
    """
    try:
        api_key = os.getenv("GOOGLE_API_KEY")
        if not api_key:
            logger.error(f"get_llm: GOOGLE_API_KEY ãŒã‚ã‚Šã¾ã›ã‚“ (Model: {model_name})")
            return None

        llm = ChatGoogleGenerativeAI(
            model=model_name,
            temperature=temperature,
            convert_system_message_to_human=True,
            api_key=api_key,
            request_timeout=timeout_seconds # (â˜…) --- ä¿®æ­£: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’æ¸¡ã™ ---
        )
        logger.info(f"LLM Model ({model_name}) loaded successfully (Timeout: {timeout_seconds}s).")
        return llm
    except Exception as e:
        logger.error(f"LLM ({model_name}) ã®åˆæœŸåŒ–ã«å¤±æ•—: {e}", exc_info=True)
        st.error(f"AIãƒ¢ãƒ‡ãƒ« ({model_name}) ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return None

@st.cache_resource
def load_spacy_model() -> Optional[spacy.language.Language]:
    """spaCyã®æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«(ja_core_news_sm)ã‚’ãƒ­ãƒ¼ãƒ‰ãƒ»ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚‹"""
    try:
        logger.info("Loading spaCy model (ja_core_news_sm)...")
        nlp = spacy.load("ja_core_news_sm")
        logger.info("spaCy model loaded successfully.")
        return nlp
    except Exception as e:
        logger.error(f"Failed to load spaCy model: {e}", exc_info=True)
        return None

@st.cache_data(ttl=3600) # 1æ™‚é–“ã‚­ãƒ£ãƒƒã‚·ãƒ¥
def get_analysis_tips_list_from_ai() -> List[str]:
    """
    (â˜…) å¾…æ©Ÿæ™‚é–“ä¸­ã«è¡¨ç¤ºã™ã‚‹ã€Œãƒ‡ãƒ¼ã‚¿åˆ†æã«é–¢ã™ã‚‹Tipsã€ã‚’AIã§ç”Ÿæˆã™ã‚‹ã€‚
    ãƒ¢ãƒ‡ãƒ«: MODEL_FLASH_LITE (gemini-2.5-flash-lite)
    """
    logger.info("get_analysis_tips_list_from_ai: AI (Flash Lite) ã§TIPSã‚’ç”Ÿæˆã—ã¾ã™ã€‚")
    llm = get_llm(model_name=MODEL_FLASH_LITE, temperature=0.5)
    if llm is None:
        return ["AIãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚"]

    prompt = PromptTemplate.from_template(
        """
        ã‚ãªãŸã¯ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ãƒ†ã‚£ã‚¹ãƒˆã®ãƒ¡ãƒ³ã‚¿ãƒ¼ã§ã™ã€‚
        ãƒ‡ãƒ¼ã‚¿åˆ†æã®åˆå¿ƒè€…ã‹ã‚‰ä¸­ç´šè€…ã«å‘ã‘ã¦ã€å½¹ç«‹ã¤ã€Œãƒ’ãƒ³ãƒˆã‚„TIPSã€ã‚’ã€10å€‹ã€‘ã€JSONã®ãƒªã‚¹ãƒˆå½¢å¼ã§ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
        
        # æŒ‡ç¤º:
        1. å„TIPSã¯ã€1ã€œ2æ–‡ã®ç°¡æ½”ãªæ—¥æœ¬èªã®æ–‡å­—åˆ—ã«ã™ã‚‹ã“ã¨ã€‚
        2. ä¾‹: ã€Œ'å¹³å‡å€¤'ã ã‘ã§ãªã'ä¸­å¤®å€¤'ã‚‚è¦‹ã‚‹ã“ã¨ã§ã€å¤–ã‚Œå€¤ã®å½±éŸ¿ã‚’æŠŠæ¡ã§ãã¾ã™ã€‚ã€
        3. ä¾‹: ã€Œãƒ‡ãƒ¼ã‚¿ã‚’å¯è¦–åŒ–ã™ã‚‹å‰ã«ã€ã¾ãšãƒ‡ãƒ¼ã‚¿ã®'æ¬ æå€¤'ã¨'å‹'ã‚’ç¢ºèªã—ã¾ã—ã‚‡ã†ã€‚ã€
        4. å‡ºåŠ›ã¯JSONãƒªã‚¹ãƒˆå½¢å¼ï¼ˆ ["TIPS1", "TIPS2", ...] ï¼‰ã®ã¿ã€‚
        
        # å›ç­” (JSONãƒªã‚¹ãƒˆå½¢å¼ã®ã¿):
        """
    )
    chain = prompt | llm | StrOutputParser()
    
    try:
        response_str = chain.invoke({})
        logger.debug(f"AI TIPSç”Ÿæˆ(ç”Ÿ): {response_str}")
        
        # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚„ä¸è¦ãªãƒ†ã‚­ã‚¹ãƒˆã‚’é™¤å»ã—ã€JSONã®ã¿ã‚’æŠ½å‡º
        match = re.search(r'\[.*\]', response_str, re.DOTALL)
        if not match:
            logger.warning("AIãŒTIPSãƒªã‚¹ãƒˆ(JSON)ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
            return ["åˆ†æTIPSã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚"]
        
        json_str = match.group(0)
        tips_list = json.loads(json_str)
        
        if isinstance(tips_list, list) and all(isinstance(tip, str) for tip in tips_list):
            logger.info(f"AI TIPS {len(tips_list)}ä»¶ã®ç”Ÿæˆã«æˆåŠŸã€‚")
            return tips_list
        else:
            raise Exception("AIã®å›ç­”ãŒæ–‡å­—åˆ—ã®ãƒªã‚¹ãƒˆå½¢å¼ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
            
    except Exception as e:
        logger.error(f"AI TIPSç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        return [
            "TIPSã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚",
            "ãƒ‡ãƒ¼ã‚¿åˆ†æã¯ã€ã¾ãšç›®çš„ï¼ˆKGI/KPIï¼‰ã‚’æ˜ç¢ºã«ã™ã‚‹ã“ã¨ã‹ã‚‰å§‹ã¾ã‚Šã¾ã™ã€‚",
            "ã€Œãªãœï¼Ÿã€ã‚’5å›ç¹°ã‚Šè¿”ã™ã“ã¨ã§ã€åˆ†æã®çœŸã®ç›®çš„ã«ãŸã©ã‚Šç€ãã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚",
            "è‰¯ã„åˆ†æã¯ã€è‰¯ã„ã€Œå•ã„ã€ã‹ã‚‰ç”Ÿã¾ã‚Œã¾ã™ã€‚",
            "ãƒ‡ãƒ¼ã‚¿ã¯ã€Œé›†ã‚ã‚‹ã€ã“ã¨ã‚ˆã‚Šã€Œã©ã†ä½¿ã†ã‹ã€ãŒé‡è¦ã§ã™ã€‚"
        ]

# --- 5. ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ãƒ˜ãƒ«ãƒ‘ãƒ¼ ---
def read_file(file: st.runtime.uploaded_file_manager.UploadedFile) -> (Optional[pd.DataFrame], Optional[str]):
    """ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«(Excel/CSV)ã‚’Pandas DataFrameã¨ã—ã¦èª­ã¿è¾¼ã‚€"""
    file_name = file.name
    logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿é–‹å§‹: {file_name}")
    try:
        if file_name.endswith('.csv'):
            # æ–‡å­—ã‚³ãƒ¼ãƒ‰ã‚’è‡ªå‹•åˆ¤åˆ¥
            try:
                content = file.getvalue().decode('utf-8-sig')
            except UnicodeDecodeError:
                logger.warning(f"UTF-8-SIGãƒ‡ã‚³ãƒ¼ãƒ‰å¤±æ•—ã€‚CP932ã§å†è©¦è¡Œ: {file_name}")
                content = file.getvalue().decode('cp932')
            df = pd.read_csv(StringIO(content))

        elif file_name.endswith(('.xlsx', '.xls')):
            df = pd.read_excel(BytesIO(file.getvalue()), engine='openpyxl')
        else:
            msg = f"ã‚µãƒãƒ¼ãƒˆå¤–ã®ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼: {file_name}"
            logger.warning(msg)
            return None, msg

        logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿æˆåŠŸ: {file_name}")
        return df, None
    except Exception as e:
        logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ ({file_name}): {e}", exc_info=True)
        st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ã€Œ{file_name}ã€ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        return None, f"èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}"


# --- 6. (â˜…) Step A: AIã‚¿ã‚°ä»˜ã‘é–¢é€£é–¢æ•° ---
# (è¦ä»¶: Step Aã¯ gemini-2.5-flash-lite ã‚’ä½¿ç”¨)

def get_dynamic_categories(analysis_prompt: str) -> Optional[Dict[str, str]]:
    """
    (Step A) ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®åˆ†ææŒ‡é‡ã«åŸºã¥ãã€AIãŒå‹•çš„ãªã‚«ãƒ†ã‚´ãƒªã‚’JSONå½¢å¼ã§ç”Ÿæˆã™ã‚‹ã€‚
    (â˜…) ãƒ¢ãƒ‡ãƒ«: MODEL_FLASH_LITE (gemini-2.5-flash-lite)
    """
    # (â˜…) Step A ã®è¦ä»¶ã«åŸºã¥ãã€FLASH_LITE ãƒ¢ãƒ‡ãƒ«ã‚’æ˜ç¤ºçš„ã«æŒ‡å®š
    llm = get_llm(model_name=MODEL_FLASH_LITE, temperature=0.0)
    if llm is None:
        logger.error("get_dynamic_categories: LLM (Flash Lite) ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")
        st.error("AIãƒ¢ãƒ‡ãƒ«(Flash Lite)ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§APIã‚­ãƒ¼ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
        return None

    logger.info("å‹•çš„ã‚«ãƒ†ã‚´ãƒªç”ŸæˆAI (Flash Lite) ã‚’å‘¼ã³å‡ºã—...")
    prompt = PromptTemplate.from_template(
        """
        ã‚ãªãŸã¯ãƒ‡ãƒ¼ã‚¿åˆ†æã®ã‚¹ã‚­ãƒ¼ãƒè¨­è¨ˆè€…ã§ã™ã€‚ã€Œåˆ†ææŒ‡é‡ã€ã‚’èª­ã¿ã€ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æŠ½å‡ºã™ã‚‹ã¹ãã€Œãƒˆãƒ”ãƒƒã‚¯ã®ã‚«ãƒ†ã‚´ãƒªã€ã‚’è€ƒæ¡ˆã—ã¦ãã ã•ã„ã€‚ã€Œå¸‚åŒºç”ºæ‘ã€ã¯å¿…é ˆã‚«ãƒ†ã‚´ãƒªã¨ã—ã¦è‡ªå‹•ã§è¿½åŠ ã•ã‚Œã‚‹ãŸã‚ã€ãã‚Œä»¥å¤–ã®ã‚«ãƒ†ã‚´ãƒªã‚’å®šç¾©ã—ã¦ãã ã•ã„ã€‚
        # æŒ‡ç¤º: 1.ã€Œåˆ†ææŒ‡é‡ã€ã®ãƒˆãƒ”ãƒƒã‚¯ã‚’ã‚«ãƒ†ã‚´ãƒªåŒ– 2.å„ã‚«ãƒ†ã‚´ãƒªã®èª¬æ˜è¨˜è¿° 3.å³æ ¼ãªJSONè¾æ›¸å‡ºåŠ› 4.åœ°åã‚«ãƒ†ã‚´ãƒªç¦æ­¢ 5.è©²å½“ãªã‘ã‚Œã°ç©ºJSON
        # åˆ†ææŒ‡é‡:{user_prompt}
        # å›ç­” (JSONè¾æ›¸å½¢å¼):
        """
    )
    chain = prompt | llm | StrOutputParser()
    try:
        response_str = chain.invoke({"user_prompt": analysis_prompt})
        logger.debug(f"AIã‚«ãƒ†ã‚´ãƒªå®šç¾©(ç”Ÿ): {response_str}")

        # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚„ä¸è¦ãªãƒ†ã‚­ã‚¹ãƒˆã‚’é™¤å»ã—ã€JSONã®ã¿ã‚’æŠ½å‡º
        match = re.search(r'\{.*\}', response_str, re.DOTALL)
        if not match:
            logger.warning("AIãŒJSONå½¢å¼ã§å¿œç­”ã—ã¾ã›ã‚“ã§ã—ãŸã€‚")
            return None
        
        json_str = match.group(0).replace("'", '"')
        try:
            categories = json.loads(json_str)
            return categories
        except json.JSONDecodeError as json_e:
            logger.error(f"AIå¿œç­”ã®JSONãƒ‘ãƒ¼ã‚¹å¤±æ•—: {json_e} - Raw: {json_str}")
            return None
            
    except Exception as e:
        logger.error(f"AIã‚«ãƒ†ã‚´ãƒªç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        st.error(f"AIã‚«ãƒ†ã‚´ãƒªç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return None

def filter_relevant_data_by_ai(df_batch: pd.DataFrame, analysis_prompt: str) -> pd.DataFrame:
    """
    (Step A) AIã‚’ä½¿ã„ã€åˆ†ææŒ‡é‡ã¨ç„¡é–¢ä¿‚ãªè¡Œã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹ (relevant: true/false)ã€‚
    (â˜…) ãƒ¢ãƒ‡ãƒ«: MODEL_FLASH_LITE (gemini-2.5-flash-lite)
    (â˜…) è¦ä»¶: é€²æ—è¡¨ç¤º (ã“ã®é–¢æ•°ã¯ãƒãƒƒãƒå‡¦ç†ã®ä¸€éƒ¨ã¨ã—ã¦å‘¼ã°ã‚Œã€å‘¼ã³å‡ºã—å…ƒã®
          `render_step_a` å†…ã® `update_progress_ui` ã§é€²æ—ãŒè¡¨ç¤ºã•ã‚Œã‚‹)
    """
    # (â˜…) Step A ã®è¦ä»¶ã«åŸºã¥ãã€FLASH_LITE ãƒ¢ãƒ‡ãƒ«ã‚’æ˜ç¤ºçš„ã«æŒ‡å®š
    llm = get_llm(model_name=MODEL_FLASH_LITE, temperature=0.0)
    if llm is None:
        logger.error("filter_relevant_data_by_ai: LLM (Flash Lite) ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")
        st.error("AIãƒ¢ãƒ‡ãƒ«(Flash Lite)ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚APIã‚­ãƒ¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        return pd.DataFrame()  # ç©ºã®DF

    logger.debug(f"{len(df_batch)}ä»¶ AIé–¢é€£æ€§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° (Flash Lite) é–‹å§‹...")

    # ãƒ†ã‚­ã‚¹ãƒˆãŒé•·ã™ãã‚‹å ´åˆã€å…ˆé ­500æ–‡å­—ã«åˆ‡ã‚Šè©°ã‚ã‚‹
    input_texts_jsonl = df_batch.apply(
        lambda row: json.dumps(
            {"id": row['id'], "text": str(row['ANALYSIS_TEXT_COLUMN'])[:500]},
            ensure_ascii=False
        ),
        axis=1
    ).tolist()

    prompt = PromptTemplate.from_template(
        """
        ã‚ãªãŸã¯ãƒ‡ãƒ¼ã‚¿åˆ†æã®ã‚­ãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼ã§ã™ã€‚ã€Œåˆ†ææŒ‡é‡ã€ã«åŸºã¥ãã€ã€Œãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿(JSONL)ã€ã®å„è¡ŒãŒåˆ†æå¯¾è±¡ã¨ã—ã¦ã€é–¢é€£ã—ã¦ã„ã‚‹ã‹ (relevant: true)ã€‘ã€ã€ç„¡é–¢ä¿‚ã‹ (relevant: false)ã€‘ã‚’åˆ¤å®šã—ã¦ãã ã•ã„ã€‚
        # åˆ†ææŒ‡é‡ (Analysis Scope):
        {analysis_prompt}
        # æŒ‡ç¤º:
        1. ã€Œåˆ†ææŒ‡é‡ã€ã¨ã€å¼·ãé–¢é€£ã€‘ã™ã‚‹æŠ•ç¨¿ã®ã¿ã‚’ `true` ã¨ã™ã‚‹ã€‚
        2. å˜ãªã‚‹å®£ä¼ã€æŒ¨æ‹¶ã®ã¿ã€æŒ‡é‡ã¨ç„¡é–¢ä¿‚ãªåœ°åŸŸã®è¨€åŠã¯ `false` ã¨ã™ã‚‹ã€‚
        3. å‡ºåŠ›ã¯ã€JSONLå½¢å¼ã®ã¿ã€‘ï¼ˆid ã¨ relevant (boolean) ã‚’å«ã‚€è¾æ›¸ï¼‰ã€‚
        # ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ (JSONL):
        {text_data_jsonl}
        # å›ç­” (JSONLå½¢å¼ã®ã¿):
        """
    )
    chain = prompt | llm | StrOutputParser()
    
    try:
        invoke_params = {
            "analysis_prompt": analysis_prompt,
            "text_data_jsonl": "\n".join(input_texts_jsonl)
        }
        response_str = chain.invoke(invoke_params)
        
        results = []
        # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ ````jsonl ... ``` ã‚’é™¤å»
        match = re.search(r'```(?:jsonl|json)?\s*([\s\S]*?)\s*```', response_str, re.DOTALL)
        jsonl_content = match.group(1).strip() if match else response_str.strip()

        for line in jsonl_content.strip().split('\n'):
            cleaned_line = line.strip()
            if not cleaned_line: continue
            try:
                data = json.loads(cleaned_line)
                # relevant ãŒ "true" (str) ã‚„ true (bool) ãªã©æºã‚‰ããŒã‚ã‚‹ãŸã‚å …ç‰¢ã«å‡¦ç†
                is_relevant = False
                if isinstance(data.get("relevant"), bool):
                    is_relevant = data.get("relevant")
                elif isinstance(data.get("relevant"), str):
                    is_relevant = data.get("relevant").lower() == 'true'
                
                results.append({"id": data.get("id"), "relevant": is_relevant})
            except (json.JSONDecodeError, AttributeError) as json_e:
                logger.warning(f"AIãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å›ç­”ãƒ‘ãƒ¼ã‚¹å¤±æ•—: {cleaned_line} - Error: {json_e}")
                # ãƒ‘ãƒ¼ã‚¹å¤±æ•—æ™‚ã¯ã€IDãŒç‰¹å®šã§ãã‚Œã°é–¢é€£ã‚ã‚Š(True)ã¨ã—ã¦ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                id_match = re.search(r'"id":\s*(\d+)', cleaned_line)
                if id_match:
                    results.append({"id": int(id_match.group(1)), "relevant": True})

        return pd.DataFrame(results) if results else pd.DataFrame(columns=['id', 'relevant'])
        
    except Exception as e:
        logger.error(f"AIãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãƒãƒƒãƒå‡¦ç†ä¸­ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        st.error(f"AIãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å®‰å…¨å´ã«å€’ã—ã€ã™ã¹ã¦é–¢é€£ã‚ã‚Š(True)ã¨ã—ã¦è¿”ã™
        return df_batch[['id']].copy().assign(relevant=True)

def perform_ai_tagging(
    df_batch: pd.DataFrame,
    categories_to_tag: Dict[str, str],
    analysis_prompt: str = ""
) -> pd.DataFrame:
    """
    (Step A) ãƒ†ã‚­ã‚¹ãƒˆã®ãƒãƒƒãƒã‚’å—ã‘å–ã‚Šã€AIãŒã€æŒ‡å®šã•ã‚ŒãŸã‚«ãƒ†ã‚´ãƒªå®šç¾©ã€‘ã«åŸºã¥ã„ã¦ç›´æ¥ã‚¿ã‚°ä»˜ã‘ã‚’è¡Œã†
    (â˜…) ãƒ¢ãƒ‡ãƒ«: MODEL_FLASH_LITE (gemini-2.5-flash-lite)
    (â˜…) è¦ä»¶: é€²æ—è¡¨ç¤º (ã“ã®é–¢æ•°ã¯ãƒãƒƒãƒå‡¦ç†ã®ä¸€éƒ¨ã¨ã—ã¦å‘¼ã°ã‚Œã€å‘¼ã³å‡ºã—å…ƒã®
          `render_step_a` å†…ã® `update_progress_ui` ã§é€²æ—ãŒè¡¨ç¤ºã•ã‚Œã‚‹)
    """
    # (â˜…) Step A ã®è¦ä»¶ã«åŸºã¥ãã€FLASH_LITE ãƒ¢ãƒ‡ãƒ«ã‚’æ˜ç¤ºçš„ã«æŒ‡å®š
    llm = get_llm(model_name=MODEL_FLASH_LITE, temperature=0.0)
    if llm is None:
        logger.error("perform_ai_tagging: LLM (Flash Lite) ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")
        st.error("AIãƒ¢ãƒ‡ãƒ«(Flash Lite)ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚APIã‚­ãƒ¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        return pd.DataFrame()

    logger.info(f"{len(df_batch)}ä»¶ AIã‚¿ã‚°ä»˜ã‘ (Flash Lite) é–‹å§‹ (ã‚«ãƒ†ã‚´ãƒª: {list(categories_to_tag.keys())})")

    # åœ°åè¾æ›¸ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æº–å‚™ (åˆ†ææŒ‡é‡ã«é–¢é€£ã™ã‚‹åœ°åã®ã¿ã‚’AIã«æ¸¡ã™)
    geo_context_str = "{}"
    if JAPAN_GEOGRAPHY_DB and "å¸‚åŒºç”ºæ‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰" in categories_to_tag:
        try:
            relevant_geo_db = {}
            prompt_lower = analysis_prompt.lower()
            
            # (åœ°åè¾æ›¸ã®ã‚­ãƒ¼ã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ä¸¡æ–¹ã«å«ã¾ã‚Œã‚‹ä¸»è¦ãªãƒ’ãƒ³ãƒˆ)
            hints = ["åºƒå³¶", "ç¦å²¡", "å¤§é˜ª", "æ±äº¬", "åŒ—æµ·é“", "æ„›çŸ¥", "å®®åŸ", "æœ­å¹Œ", "æ¨ªæµœ", "åå¤å±‹", "äº¬éƒ½", "ç¥æˆ¸", "ä»™å°"]
            keys_found = [
                key for key in JAPAN_GEOGRAPHY_DB.keys()
                if any(h in key.lower() for h in hints) and any(h in prompt_lower for h in hints)
            ]
            # ç‰¹å®šã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€é–¢é€£ã™ã‚‹ã‚­ãƒ¼ã‚’å¼·åˆ¶çš„ã«è¿½åŠ 
            if "åºƒå³¶" in prompt_lower: keys_found.extend(["åºƒå³¶çœŒ", "åºƒå³¶å¸‚"])
            if "æ±äº¬" in prompt_lower: keys_found.extend(["æ±äº¬éƒ½", "æ±äº¬23åŒº"])
            if "å¤§é˜ª" in prompt_lower: keys_found.extend(["å¤§é˜ªåºœ", "å¤§é˜ªå¸‚"])

            for key in set(keys_found): # é‡è¤‡å‰Šé™¤
                if key in JAPAN_GEOGRAPHY_DB:
                    relevant_geo_db[key] = JAPAN_GEOGRAPHY_DB[key]
            
            #  relevant_geo_db ãŒç©ºã®å ´åˆã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ (ä¸»è¦éƒ½å¸‚)
            if not relevant_geo_db:
                logger.warning("åœ°åè¾æ›¸ã®çµã‚Šè¾¼ã¿ãƒ’ãƒ³ãƒˆãªã—ã€‚ä¸»è¦éƒ½å¸‚ã®ã¿æ¸¡ã—ã¾ã™ã€‚")
                default_keys = ["æ±äº¬éƒ½", "æ±äº¬23åŒº", "å¤§é˜ªåºœ", "å¤§é˜ªå¸‚", "åºƒå³¶çœŒ", "åºƒå³¶å¸‚", "ç¦å²¡çœŒ", "ç¦å²¡å¸‚"]
                for key in default_keys:
                    if key in JAPAN_GEOGRAPHY_DB:
                        relevant_geo_db[key] = JAPAN_GEOGRAPHY_DB[key]

            geo_context_str = json.dumps(relevant_geo_db, ensure_ascii=False, indent=2)
            
            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒå¤§ãã™ãã‚‹å ´åˆã€ã‚­ãƒ¼ã®ã¿ã«ç¸®å°
            if len(geo_context_str) > 5000:
                logger.warning(f"åœ°åè¾æ›¸ãŒå¤§ãã™ã ({len(geo_context_str)}B)ã€‚ã‚­ãƒ¼ã®ã¿ã«ç¸®å°ã€‚")
                geo_context_str = json.dumps(list(relevant_geo_db.keys()), ensure_ascii=False)
                
            logger.info(f"AIã«æ¸¡ã™åœ°åè¾æ›¸(çµè¾¼æ¸ˆ): {list(relevant_geo_db.keys())}")
        except Exception as e:
            logger.error(f"åœ°åè¾æ›¸ã®æº–å‚™ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            geo_context_str = "{}" # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ç©ºã®è¾æ›¸

    # ãƒ†ã‚­ã‚¹ãƒˆãŒé•·ã™ãã‚‹å ´åˆã€å…ˆé ­500æ–‡å­—ã«åˆ‡ã‚Šè©°ã‚ã‚‹
    input_texts_jsonl = df_batch.apply(
        lambda row: json.dumps(
            {"id": row['id'], "text": str(row['ANALYSIS_TEXT_COLUMN'])[:500]},
            ensure_ascii=False
        ),
        axis=1
    ).tolist()

    prompt = PromptTemplate.from_template(
        """
        ã‚ãªãŸã¯é«˜ç²¾åº¦ãƒ‡ãƒ¼ã‚¿åˆ†æã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ã€Œã‚«ãƒ†ã‚´ãƒªå®šç¾©ã€ã€Œåœ°åè¾æ›¸ã€ã€Œåˆ†ææŒ‡é‡ã€ã«åŸºã¥ãã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºã—ã¾ã™ã€‚
        # åˆ†ææŒ‡é‡ (Analysis Scope): {analysis_prompt}
        # åœ°åè¾æ›¸ (JAPAN_GEOGRAPHY_DB): {geo_context}
        # ã‚«ãƒ†ã‚´ãƒªå®šç¾© (categories): {categories}
        # ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ (JSONL): {text_data_jsonl}
        # æŒ‡ç¤º:
        1. ã€Œãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿(JSONL)ã€ã®å„è¡Œã‚’å‡¦ç†ã™ã‚‹ã€‚
        2. ã€Œã‚«ãƒ†ã‚´ãƒªå®šç¾©ã€ã®ã‚­ãƒ¼åã‚’ã€å³æ ¼ã«ã€‘ä½¿ç”¨ã—ã€å…¨ã‚«ãƒ†ã‚´ãƒªã‚’æŠ½å‡ºã™ã‚‹ã€‚
        3. ã€"å¸‚åŒºç”ºæ‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰" ä»¥å¤–ã®ã‚«ãƒ†ã‚´ãƒªã€‘:
           - å€¤ã¯å¿…ãšã€ãƒªã‚¹ãƒˆå½¢å¼ã€‘ã§å‡ºåŠ›ï¼ˆè©²å½“ãªã‘ã‚Œã°ç©ºãƒªã‚¹ãƒˆ []ï¼‰ã€‚
        4. ã€"å¸‚åŒºç”ºæ‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰" (æœ€é‡è¦ãƒ»å˜ä¸€å›ç­”)ã€‘:
           - å€¤ã¯ã€å˜ä¸€ã®æ–‡å­—åˆ—ã€‘ã§å‡ºåŠ›ã™ã‚‹ (è©²å½“ãªã‘ã‚Œã°ç©ºæ–‡å­—åˆ— "")ã€‚ãƒªã‚¹ãƒˆå½¢å¼ã¯ã€å³ç¦ã€‘ã€‚
           - æŠ½å‡ºãƒ«ãƒ¼ãƒ«:
             a. ã€Œåœ°åè¾æ›¸ã€ã®ã€å€¤ã€‘(ä¾‹: "å‘‰å¸‚", "ä¸­åŒº") ã¾ãŸã¯ã€ã‚­ãƒ¼ã€‘(ä¾‹: "åºƒå³¶å¸‚") ã«ä¸€è‡´ã™ã‚‹ã€æœ€ã‚‚æ–‡è„ˆã«é–¢é€£æ€§ã®é«˜ã„ã‚‚ã®ã‚’ã€1ã¤ã ã‘ã€‘é¸ã¶ã€‚
             b. (ä¾‹: "åºƒå³¶å¸‚" ã¨ "ä¸­åŒº" ãŒä¸¡æ–¹è¨€åŠã•ã‚Œã¦ã„ã‚Œã°ã€ã‚ˆã‚Šè©³ç´°ãª "ä¸­åŒº" ã‚’å„ªå…ˆã™ã‚‹)
             c. "å®®å³¶" ã®ã‚ˆã†ãªãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯åã¯ã€ãã‚ŒãŒå±ã™ã‚‹ã€Œåœ°åè¾æ›¸ã€ã®å¸‚åŒºç”ºæ‘å (ä¾‹: "å»¿æ—¥å¸‚å¸‚") ã«ã€å¿…ãšå¤‰æ›ã€‘ã—ã¦å›ç­”ã™ã‚‹ã€‚
             d. "åºƒå³¶" ã®ã‚ˆã†ãªæ›–æ˜§ãªè¡¨ç¾ã¯ã€æ–‡è„ˆã‹ã‚‰ (a) ã®ã„ãšã‚Œã‹ã«ç‰¹å®šã§ãã‚‹å ´åˆã®ã¿ (ä¾‹: "åºƒå³¶å¸‚") æŠ½å‡ºã—ã€ç‰¹å®šã§ããªã‘ã‚Œã°ã€ç©ºæ–‡å­—åˆ— ""ã€‘ã¨ã™ã‚‹ã€‚
             e. éƒ½é“åºœçœŒå (ä¾‹: "åºƒå³¶çœŒ")ã€ãŠã‚ˆã³ã€Œè¦³å…‰åœ°ã€ã®ã‚ˆã†ãªåœ°åä»¥å¤–ã®å˜èªã¯ã€çµ¶å¯¾ã«æŠ½å‡ºã—ãªã„ã€‘ã€‚
             f. ã€Œåˆ†ææŒ‡é‡ã€ã¨ç„¡é–¢ä¿‚ãªåœ°åŸŸã®åœ°åï¼ˆä¾‹: æŒ‡é‡ãŒã€Œåºƒå³¶ã€ãªã®ã«ãƒ†ã‚­ã‚¹ãƒˆãŒã€Œæ»‹è³€çœŒã€ï¼‰ã¯ã€æŠ½å‡ºã—ãªã„ã€‘ã€‚
        5. ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆæƒ…å ±ã®æé€ ï¼‰ç¦æ­¢ã€‚
        6. å‡ºåŠ›ã¯ã€JSONLå½¢å¼ã®ã¿ã€‘ï¼ˆid ã¨ categories ã‚’å«ã‚€è¾æ›¸ï¼‰ã€‚
        # å›ç­” (JSONLå½¢å¼ã®ã¿):
        """
    )
    chain = prompt | llm | StrOutputParser()
    
    try:
        invoke_params = {
            "categories": json.dumps(categories_to_tag, ensure_ascii=False),
            "geo_context": geo_context_str,
            "text_data_jsonl": "\n".join(input_texts_jsonl),
            "analysis_prompt": analysis_prompt
        }
        response_str = chain.invoke(invoke_params)
        logger.debug(f"AI Tagging - Raw response received.")

        results = []
        expected_keys = list(categories_to_tag.keys())
        
        # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ ````jsonl ... ``` ã‚’é™¤å»
        match = re.search(r'```(?:jsonl|json)?\s*([\s\S]*?)\s*```', response_str, re.DOTALL)
        jsonl_content = match.group(1).strip() if match else response_str.strip()

        for line in jsonl_content.strip().split('\n'):
            cleaned_line = line.strip()
            if not cleaned_line: continue
            try:
                data = json.loads(cleaned_line)
                row_result = {"id": data.get("id")}
                # AIãŒ 'categories' ã§ãƒ©ãƒƒãƒ—ã™ã‚‹å ´åˆã¨ã€ã—ãªã„å ´åˆã®ä¸¡æ–¹ã«å¯¾å¿œ
                tag_source = data.get('categories', data)
                
                if not isinstance(tag_source, dict):
                    raise json.JSONDecodeError(f"tag_source is not a dict: {tag_source}", "", 0)

                for key in expected_keys:
                    # AIã®å›ç­”ã‚­ãƒ¼ãŒ ' ã‚«ãƒ†ã‚´ãƒª ' ã®ã‚ˆã†ã«ç©ºç™½ã‚’å«ã‚€å ´åˆã«å¯¾å¿œ
                    found_key = None
                    for resp_key in tag_source.keys():
                        if str(resp_key).strip() == key:
                            found_key = resp_key
                            break
                    
                    raw_value = tag_source.get(found_key) if found_key else None

                    # --- "å¸‚åŒºç”ºæ‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰" ã®å‡¦ç† (å˜ä¸€æ–‡å­—åˆ—) ---
                    if key == "å¸‚åŒºç”ºæ‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰":
                        processed_value = ""
                        if isinstance(raw_value, list) and raw_value:
                            # ãƒªã‚¹ãƒˆã§è¿”ã£ã¦ããŸå ´åˆã€æœ€åˆã®è¦ç´ ã‚’æ¡ç”¨
                            processed_value = str(raw_value[0]).strip()
                        elif raw_value is not None and str(raw_value).strip():
                            # æ–‡å­—åˆ—ã§è¿”ã£ã¦ããŸå ´åˆ
                            processed_value = str(raw_value).strip()
                        
                        # è©²å½“ãªã—ç­‰ã®è¡¨ç¾ã‚’ç©ºæ–‡å­—ã«çµ±ä¸€
                        if processed_value.lower() in ["è©²å½“ãªã—", "none", "null", "", "n/a"]:
                            row_result[key] = ""
                        else:
                            row_result[key] = processed_value
                    
                    # --- ãã®ä»–ã®ã‚«ãƒ†ã‚´ãƒªã®å‡¦ç† (ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šæ–‡å­—åˆ—) ---
                    else:
                        processed_values = []
                        if isinstance(raw_value, list):
                            processed_values = sorted(list(set(
                                str(val).strip() for val in raw_value if str(val).strip()
                            )))
                        elif raw_value is not None and str(raw_value).strip():
                            # å˜ä¸€æ–‡å­—åˆ—ã§è¿”ã£ã¦ããŸå ´åˆã‚‚ãƒªã‚¹ãƒˆã«æ ¼ç´
                            processed_values = [str(raw_value).strip()]
                        
                        # æ—¢å­˜ã‚³ãƒ¼ãƒ‰ (L304) ã«åˆã‚ã›ã€ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã®æ–‡å­—åˆ—ã¨ã—ã¦æ ¼ç´
                        row_result[key] = ", ".join(processed_values)
                
                results.append(row_result)
                
            except (json.JSONDecodeError, AttributeError) as json_e:
                logger.warning(f"AIã‚¿ã‚°ä»˜ã‘å›ç­”ãƒ‘ãƒ¼ã‚¹å¤±æ•—: {cleaned_line} - Error: {json_e}")
                id_match = re.search(r'"id":\s*(\d+)', cleaned_line)
                if id_match:
                    # ãƒ‘ãƒ¼ã‚¹å¤±æ•—æ™‚ã¯ã€IDã®ã¿ã®ç©ºã®è¡Œã‚’è¿½åŠ  (ãƒãƒ¼ã‚¸ãŒå¤±æ•—ã—ãªã„ã‚ˆã†ã«)
                    results.append({"id": int(id_match.group(1))})
                    
        return pd.DataFrame(results) if results else pd.DataFrame(columns=['id'] + list(expected_keys))

    except Exception as e:
        logger.error(f"AIã‚¿ã‚°ä»˜ã‘ãƒãƒƒãƒå‡¦ç†ä¸­ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        st.error(f"AIã‚¿ã‚°ä»˜ã‘å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        return pd.DataFrame()  # å¤±æ•—æ™‚ã¯ç©ºã®DF


# --- 7. (â˜…) Step A: UIæç”»é–¢æ•° ---

def update_progress_ui(
    progress_placeholder: st.delta_generator.DeltaGenerator,
    log_placeholder: st.delta_generator.DeltaGenerator,
    tip_placeholder: st.delta_generator.DeltaGenerator,  # (â˜…) Tipsç”¨ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€
    processed_rows: int,
    total_rows: int,
    message_prefix: str
):
    """
    (Step A) ã®é€²æ—ãƒãƒ¼ã¨ãƒ­ã‚°ã‚¨ãƒªã‚¢ã‚’æ›´æ–°ã™ã‚‹ (DRY)
    (â˜…) è¦ä»¶: AIèª­ã¿è¾¼ã¿æ™‚é–“ã®é€²æ—ã‚’0ï½100ï¼…ã§è¡¨ç¤º
    (â˜…) è¦ä»¶: AI Tipsã‚’ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³è¡¨ç¤º
    """
    try:
        # total_rows ãŒ 0 ã®å ´åˆ DivisionByZero ã‚’é˜²ã
        if total_rows == 0:
            progress_percent = 1.0
        else:
            progress_percent = min(processed_rows / total_rows, 1.0)
            
        progress_text = f"[{message_prefix}] å‡¦ç†ä¸­: {processed_rows}/{total_rows} ä»¶ ({progress_percent:.0%})"
        progress_placeholder.progress(progress_percent, text=progress_text)

        # ãƒ­ã‚°è¡¨ç¤º (æœ€æ–°50ä»¶)
        log_text_for_ui = "\n".join(st.session_state.log_messages[-50:])
        log_placeholder.text_area(
            "å®Ÿè¡Œãƒ­ã‚° (æœ€æ–°50ä»¶):",
            log_text_for_ui,
            height=200,
            key=f"log_update_{message_prefix}_{processed_rows}", # é‡è¤‡ã‚­ãƒ¼ã‚’é¿ã‘ã‚‹
            disabled=True
        )
        
        # (â˜…) --- AI Tips ã®ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³è¡¨ç¤º ---
        if 'tips_list' not in st.session_state or not st.session_state.tips_list:
             # ä¸‡ãŒä¸€TipsãŒç©ºã®å ´åˆã¯AI Tipsé–¢æ•°ã‚’å‘¼ã³å‡ºã™
             st.session_state.tips_list = get_analysis_tips_list_from_ai()
             st.session_state.current_tip_index = 0
             st.session_state.last_tip_time = time.time()

        now = time.time()
        # 60ç§’ã”ã¨ï¼ˆã¾ãŸã¯TIPSãŒ1ä»¶ã—ã‹ãªã„å ´åˆï¼‰ã«TIPSã‚’æ›´æ–°
        if (now - st.session_state.last_tip_time > 60) or (len(st.session_state.tips_list) == 1):
            if len(st.session_state.tips_list) > 1:
                st.session_state.current_tip_index = (st.session_state.current_tip_index + 1) % len(st.session_state.tips_list)
            st.session_state.last_tip_time = now
        
        # (â˜…) ãƒªã‚¹ãƒˆãŒç©ºã§ãªã„ã‹ãƒã‚§ãƒƒã‚¯
        if st.session_state.tips_list:
            current_tip = st.session_state.tips_list[st.session_state.current_tip_index]
            tip_placeholder.info(f"ğŸ’¡ ãƒ‡ãƒ¼ã‚¿åˆ†æTIPS: {current_tip}")
        # (â˜…) --- ã“ã“ã¾ã§ãŒå¤‰æ›´ç‚¹ ---

    except Exception as e:
        # UIã®æ›´æ–°ã‚¨ãƒ©ãƒ¼ã¯ãƒ­ã‚°ã«è­¦å‘Šã®ã¿æ®‹ã—ã€å‡¦ç†ã¯ç¶šè¡Œ
        logger.warning(f"UI update failed: {e}")

def render_step_a():
    """(Step A) ã‚¿ã‚°ä»˜ã‘å‡¦ç†ã®UIã‚’æç”»ã™ã‚‹"""
    st.title("ğŸ·ï¸ Step A: AIã‚¿ã‚°ä»˜ã‘ & ã‚­ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³")

    # Step A å›ºæœ‰ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã‚’åˆæœŸåŒ–
    if 'cancel_analysis' not in st.session_state:
        st.session_state.cancel_analysis = False
    if 'generated_categories' not in st.session_state:
        st.session_state.generated_categories = {}
    if 'selected_categories' not in st.session_state:
        st.session_state.selected_categories = set()
    if 'analysis_prompt_A' not in st.session_state:
        st.session_state.analysis_prompt_A = ""
    if 'selected_text_col' not in st.session_state:
        st.session_state.selected_text_col = {}
    if 'tagged_df_A' not in st.session_state:
        st.session_state.tagged_df_A = pd.DataFrame()

    st.header("Step 1: åˆ†æå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
    uploaded_files = st.file_uploader(
        "åˆ†æã—ãŸã„ Excel / CSV ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆè¤‡æ•°å¯ï¼‰",
        type=['csv', 'xlsx', 'xls'],
        accept_multiple_files=True,
        key="uploader_A"
    )

    if not uploaded_files:
        st.info("åˆ†æã‚’é–‹å§‹ã™ã‚‹ã«ã¯ã€Excelã¾ãŸã¯CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
        return

    # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å‡¦ç†
    valid_files_data = {}
    error_messages = []
    for f in uploaded_files:
        df, err = read_file(f)
        if err:
            error_messages.append(f"**{f.name}**: {err}")
        else:
            valid_files_data[f.name] = df
            
    if error_messages:
        st.error("ä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯èª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸ:\n" + "\n".join(error_messages))
    if not valid_files_data:
        st.warning("èª­ã¿è¾¼ã¿å¯èƒ½ãªãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return

    st.header("Step 2: åˆ†ææŒ‡é‡ã®å…¥åŠ›ã¨ã‚«ãƒ†ã‚´ãƒªç”Ÿæˆ")
    analysis_prompt = st.text_area(
        "AIãŒã‚¿ã‚°ä»˜ã‘ã¨ã‚­ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¡Œã†éš›ã®æŒ‡é‡ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆå¿…é ˆï¼‰:",
        value=st.session_state.analysis_prompt_A,
        height=100,
        placeholder="ä¾‹: åºƒå³¶çœŒã®è¦³å…‰ã«é–¢ã™ã‚‹Instagramã®æŠ•ç¨¿ã€‚ç„¡é–¢ä¿‚ãªåœ°åŸŸã®æŠ•ç¨¿ã‚„ã€å˜ãªã‚‹æŒ¨æ‹¶ãƒ»å®£ä¼ã¯é™¤å¤–ã—ãŸã„ã€‚",
        key="analysis_prompt_input_A"
    )
    st.session_state.analysis_prompt_A = analysis_prompt
    
    # (â˜…) --- ä¿®æ­£: ãƒœã‚¿ãƒ³ã‚’Step2ã«ç§»å‹• ---
    st.markdown(f"ï¼ˆ(â˜…) ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: `{MODEL_FLASH_LITE}`ï¼‰")
    if st.button("AIã«ã‚«ãƒ†ã‚´ãƒªå€™è£œã‚’ç”Ÿæˆã•ã›ã‚‹ (Step 2)", key="gen_cat_button", type="primary"):
        if not analysis_prompt.strip():
            st.warning("åˆ†ææŒ‡é‡ã¯å¿…é ˆã§ã™ã€‚AIãŒãƒ‡ãƒ¼ã‚¿ã‚’ç†è§£ã™ã‚‹ãŸã‚ã«ç›®çš„ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        elif not os.getenv("GOOGLE_API_KEY"):
            st.error("Google APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ï¼ˆ.envãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„ï¼‰")
        else:
            with st.spinner(f"AI ({MODEL_FLASH_LITE}) ãŒåˆ†ææŒ‡é‡ã‚’èª­ã¿è§£ãã€ã‚«ãƒ†ã‚´ãƒªã‚’è€ƒæ¡ˆä¸­..."):
                logger.info("AIã‚«ãƒ†ã‚´ãƒªç”Ÿæˆãƒœã‚¿ãƒ³ã‚¯ãƒªãƒƒã‚¯")
                st.session_state.generated_categories = {"å¸‚åŒºç”ºæ‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰": "åœ°åè¾æ›¸(JAPAN_GEOGRAPHY_DB)ã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸå¸‚åŒºç”ºæ‘å"}
                
                ai_categories = get_dynamic_categories(analysis_prompt)
                
                if ai_categories:
                    st.session_state.generated_categories.update(ai_categories)
                    logger.info(f"AIã‚«ãƒ†ã‚´ãƒªç”ŸæˆæˆåŠŸ: {list(ai_categories.keys())}")
                    st.success("AIã«ã‚ˆã‚‹ã‚«ãƒ†ã‚´ãƒªå€™è£œã®ç”ŸæˆãŒå®Œäº†ã—ã¾ã—ãŸã€‚Step 3 ã«é€²ã‚“ã§ãã ã•ã„ã€‚")
                else:
                    st.error("AIã«ã‚ˆã‚‹ã‚«ãƒ†ã‚´ãƒªç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚AIã®å¿œç­”ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    # (â˜…) --- ã“ã“ã¾ã§ãŒä¿®æ­£ç‚¹ ---

    if not analysis_prompt.strip():
        st.warning("åˆ†ææŒ‡é‡ã¯å¿…é ˆã§ã™ã€‚AIãŒãƒ‡ãƒ¼ã‚¿ã‚’ç†è§£ã™ã‚‹ãŸã‚ã«ç›®çš„ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        return

    # (â˜…) --- ä¿®æ­£: Step 3 ã¯ã‚«ãƒ†ã‚´ãƒªã®é¸æŠã®ã¿ã«å¤‰æ›´ ---
    st.header("Step 3: åˆ†æã‚«ãƒ†ã‚´ãƒªã®é¸æŠ")
    if not st.session_state.generated_categories:
        st.info("Step 2 ã§ã€ŒAIã«ã‚«ãƒ†ã‚´ãƒªå€™è£œã‚’ç”Ÿæˆã•ã›ã‚‹ã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")
        return
        
    st.markdown("ã‚¿ã‚°ä»˜ã‘ã—ãŸã„ã‚«ãƒ†ã‚´ãƒªã‚’ä»¥ä¸‹ã‹ã‚‰é¸æŠã—ã¦ãã ã•ã„ï¼ˆã€Œå¸‚åŒºç”ºæ‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€ã¯å¿…é ˆã§ã™ï¼‰")
    # (â˜…) ... (st.header("Step 4: ...") ã«åç§°å¤‰æ›´) ...
    # (â˜…) --- ä¿®æ­£: åç§°ã‚’Step 4, 5, 6, 7 ã«å¤‰æ›´ ---
    
    selected_cats = []
    cols = st.columns(3)
    categories_to_show = st.session_state.generated_categories.items()
    
    for i, (cat, desc) in enumerate(categories_to_show):
        with cols[i % 3]:
            is_checked = st.checkbox(
                cat,
                value=(cat == "å¸‚åŒºç”ºæ‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰" or cat in st.session_state.selected_categories),
                help=desc,
                key=f"cat_cb_{cat}",
                disabled=(cat == "å¸‚åŒºç”ºæ‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰")
            )
            if is_checked:
                selected_cats.append(cat)
    st.session_state.selected_categories = set(selected_cats)

    st.header("Step 4: åˆ†æå¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆåˆ—ã®æŒ‡å®š")
    selected_text_col_map = {}
    st.markdown("ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã«ã€ã‚¿ã‚°ä»˜ã‘å¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆãŒå«ã¾ã‚Œã‚‹åˆ—ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚")
    for f_name, df in valid_files_data.items():
        cols_list = list(df.columns)
        default_index = 0
        
        if st.session_state.selected_text_col.get(f_name) in cols_list:
            default_index = cols_list.index(st.session_state.selected_text_col.get(f_name))
        elif any(c in cols_list for c in ['text', 'body', 'content', 'æŠ•ç¨¿', 'æœ¬æ–‡']):
            try:
                default_index = next(i for i, c in enumerate(cols_list) if c in ['text', 'body', 'content', 'æŠ•ç¨¿', 'æœ¬æ–‡'])
            except StopIteration:
                default_index = 0
                
        selected_col = st.selectbox(f"**{f_name}** ã®ãƒ†ã‚­ã‚¹ãƒˆåˆ—:", cols_list, index=default_index, key=f"col_select_{f_name}")
        selected_text_col_map[f_name] = selected_col
    st.session_state.selected_text_col = selected_text_col_map

    st.header("Step 5: åˆ†æå®Ÿè¡Œ")
    st.markdown(f"ï¼ˆ(â˜…) ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: `{MODEL_FLASH_LITE}`ï¼‰")
    
    col_run, col_cancel = st.columns([1, 1])
    with col_cancel:
        if st.button("ã‚­ãƒ£ãƒ³ã‚»ãƒ«", key="cancel_button_A", use_container_width=True):
            st.session_state.cancel_analysis = True
            logger.warning("åˆ†æã‚­ãƒ£ãƒ³ã‚»ãƒ«ãƒœã‚¿ãƒ³ãŒæŠ¼ã•ã‚Œã¾ã—ãŸã€‚")
            st.warning("æ¬¡ã®ãƒãƒƒãƒå‡¦ç†å¾Œã«åˆ†æã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã—ã¾ã™...")
    
    with col_run:
        if st.button("åˆ†æå®Ÿè¡Œ (Step 5)", type="primary", key="run_analysis_A", use_container_width=True):
            st.session_state.cancel_analysis = False
            st.session_state.log_messages = []
            st.session_state.tagged_df_A = pd.DataFrame()
            
            # (â˜…) --- Tipsè¡¨ç¤ºã®åˆæœŸåŒ– ---
            tip_placeholder = st.empty()
            try:
                with st.spinner("åˆ†æTIPSã‚’AIã§ç”Ÿæˆä¸­..."):
                    if 'tips_list' not in st.session_state or not st.session_state.tips_list:
                        st.session_state.tips_list = get_analysis_tips_list_from_ai()
                
                if not st.session_state.tips_list: # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                    st.session_state.tips_list = ["ãƒ‡ãƒ¼ã‚¿åˆ†æTIPSã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚"]

                st.session_state.current_tip_index = random.randint(0, len(st.session_state.tips_list) - 1)
                st.session_state.last_tip_time = time.time()
                tip_placeholder.info(f"ğŸ’¡ ãƒ‡ãƒ¼ã‚¿åˆ†æTIPS: {st.session_state.tips_list[st.session_state.current_tip_index]}")
            except Exception as e:
                logger.error(f"TipsåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            # (â˜…) --- ã“ã“ã¾ã§ãŒå¤‰æ›´ç‚¹ ---

            try:
                with st.spinner(f"Step A: AIåˆ†æå‡¦ç†ä¸­ ({MODEL_FLASH_LITE})..."):
                    logger.info("Step A åˆ†æå®Ÿè¡Œãƒœã‚¿ãƒ³ã‚¯ãƒªãƒƒã‚¯")
                    progress_placeholder = st.progress(0.0, text="å‡¦ç†å¾…æ©Ÿä¸­...")
                    log_placeholder = st.empty()

                    # --- 1. ãƒ•ã‚¡ã‚¤ãƒ«çµåˆ ---
                    update_progress_ui(progress_placeholder, log_placeholder, tip_placeholder, 0, 100, "ãƒ•ã‚¡ã‚¤ãƒ«çµåˆ")
                    temp_dfs = []
                    for f_name, df in valid_files_data.items():
                        col_name = selected_text_col_map[f_name]
                        temp_df = df.rename(columns={col_name: 'ANALYSIS_TEXT_COLUMN'})
                        temp_dfs.append(temp_df)
                    
                    master_df = pd.concat(temp_dfs, ignore_index=True, sort=False)
                    master_df['id'] = master_df.index
                    if master_df.empty:
                        raise Exception("åˆ†æå¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

                    # --- 2. é‡è¤‡å‰Šé™¤ ---
                    initial_row_count = len(master_df)
                    master_df.drop_duplicates(subset=['ANALYSIS_TEXT_COLUMN'], keep='first', inplace=True)
                    deduped_row_count = len(master_df)
                    logger.info(f"é‡è¤‡å‰Šé™¤ å®Œäº†ã€‚ {initial_row_count}è¡Œ -> {deduped_row_count}è¡Œ")

                    # --- 3. (â˜…) AIé–¢é€£æ€§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° (ã‚­ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³) ---
                    total_filter_rows = len(master_df)
                    total_filter_batches = (total_filter_rows + FILTER_BATCH_SIZE - 1) // FILTER_BATCH_SIZE
                    all_filtered_results = []
                    
                    for i in range(0, total_filter_rows, FILTER_BATCH_SIZE):
                        if st.session_state.cancel_analysis:
                            raise Exception("åˆ†æãŒã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ")
                        
                        batch_df = master_df.iloc[i:i + FILTER_BATCH_SIZE]
                        current_batch_num = (i // FILTER_BATCH_SIZE) + 1
                        
                        update_progress_ui(
                            progress_placeholder, log_placeholder, tip_placeholder,
                            min(i + FILTER_BATCH_SIZE, total_filter_rows), total_filter_rows,
                            f"AIã‚­ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ (ãƒãƒƒãƒ {current_batch_num}/{total_filter_batches})"
                        )
                        
                        filtered_df = filter_relevant_data_by_ai(batch_df, analysis_prompt)
                        if filtered_df is not None and not filtered_df.empty:
                            all_filtered_results.append(filtered_df)
                        
                        time.sleep(FILTER_SLEEP_TIME)
                    
                    if not all_filtered_results:
                        raise Exception("AIãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")

                    filter_results_df = pd.concat(all_filtered_results, ignore_index=True)
                    relevant_ids = filter_results_df[filter_results_df['relevant'] == True]['id']
                    filtered_master_df = master_df[master_df['id'].isin(relevant_ids)].copy()
                    filtered_row_count = len(filtered_master_df)
                    logger.info(f"AIãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° å®Œäº†ã€‚ {deduped_row_count}è¡Œ -> {filtered_row_count}è¡Œ")

                    if filtered_master_df.empty:
                        st.warning("AIã‚­ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®çµæœã€åˆ†æå¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ãŒ0ä»¶ã«ãªã‚Šã¾ã—ãŸã€‚")
                        st.session_state.tagged_df_A = pd.DataFrame()
                        progress_placeholder.progress(1.0, text="å‡¦ç†å®Œäº† (å¯¾è±¡ãƒ‡ãƒ¼ã‚¿0ä»¶)")
                        return

                    # --- 4. (â˜…) AIã‚¿ã‚°ä»˜ã‘ ---
                    selected_category_definitions = {
                        cat: desc for cat, desc in st.session_state.generated_categories.items()
                        if cat in st.session_state.selected_categories
                    }
                    
                    master_df_for_tagging = filtered_master_df
                    total_rows = len(master_df_for_tagging)
                    all_tagged_results = []
                    total_batches = (total_rows + TAGGING_BATCH_SIZE - 1) // TAGGING_BATCH_SIZE
                    
                    for i in range(0, total_rows, TAGGING_BATCH_SIZE):
                        if st.session_state.cancel_analysis:
                            raise Exception("åˆ†æãŒã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ")
                        
                        batch_df = master_df_for_tagging.iloc[i:i + TAGGING_BATCH_SIZE]
                        current_batch_num = (i // TAGGING_BATCH_SIZE) + 1
                        
                        update_progress_ui(
                            progress_placeholder, log_placeholder, tip_placeholder,
                            min(i + TAGGING_BATCH_SIZE, total_rows), total_rows,
                            f"AIã‚¿ã‚°ä»˜ã‘ (ãƒãƒƒãƒ {current_batch_num}/{total_batches})"
                        )

                        tagged_df = perform_ai_tagging(batch_df, selected_category_definitions, analysis_prompt)
                        if tagged_df is not None and not tagged_df.empty:
                            all_tagged_results.append(tagged_df)
                        
                        time.sleep(TAGGING_SLEEP_TIME)

                    if not all_tagged_results:
                        raise Exception("AIã‚¿ã‚°ä»˜ã‘å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")

                    # --- 5. æœ€çµ‚ãƒãƒ¼ã‚¸ ---
                    logger.info("å…¨AIã‚¿ã‚°ä»˜ã‘çµæœçµåˆ...");
                    tagged_results_df = pd.concat(all_tagged_results, ignore_index=True)

                    logger.info("æœ€çµ‚ãƒãƒ¼ã‚¸å‡¦ç†é–‹å§‹...");
                    final_df = pd.merge(master_df_for_tagging, tagged_results_df, on='id', how='right')
                    
                    final_cols = list(master_df_for_tagging.columns) + [col for col in tagged_results_df.columns if col not in master_df_for_tagging.columns]
                    final_df = final_df[final_cols]

                    st.session_state.tagged_df_A = final_df
                    logger.info("Step A åˆ†æå‡¦ç† æ­£å¸¸çµ‚äº†");
                    st.success("AIã«ã‚ˆã‚‹åˆ†æå‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚");
                    progress_placeholder.progress(1.0, text="å‡¦ç†å®Œäº†")
                    
                    update_progress_ui(
                        progress_placeholder, log_placeholder, tip_placeholder, 
                        total_rows, total_rows, "å‡¦ç†å®Œäº†"
                    )
                    
                    tip_placeholder.empty() # å‡¦ç†å®Œäº†å¾Œã€Tipsã‚’æ¶ˆã™

            except Exception as e:
                logger.error(f"Step A åˆ†æå®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
                st.error(f"åˆ†æå®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                if 'progress_placeholder' in locals():
                    progress_placeholder.progress(1.0, text="ã‚¨ãƒ©ãƒ¼ã«ã‚ˆã‚Šå‡¦ç†ä¸­æ–­")
                if 'tip_placeholder' in locals():
                    tip_placeholder.empty() # ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚Tipsã‚’æ¶ˆã™

    # (â˜…) è¦ä»¶â‘£: ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆãƒªãƒ³ã‚¯ã‚’è¡¨ç¤º
    if not st.session_state.tagged_df_A.empty:
        st.header("Step 6: åˆ†æçµæœã®ç¢ºèªã¨ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ")
        st.dataframe(st.session_state.tagged_df_A.head(50))

        @st.cache_data
        def convert_df_to_csv(df: pd.DataFrame) -> bytes:
            """DataFrameã‚’UTF-8-SIGã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã®CSV (bytes) ã«å¤‰æ›ã™ã‚‹"""
            return df.to_csv(encoding="utf-8-sig", index=False).encode("utf-8-sig")

        csv_data = convert_df_to_csv(st.session_state.tagged_df_A)
        st.download_button(
            label="åˆ†æçµæœCSV (Curated_Data.csv) ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            data=csv_data,
            file_name="Curated_Data.csv",
            mime="text/csv",
        )
        st.info("ã“ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã€Step B ã§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦åˆ†æã‚’ç¶šã‘ã¦ãã ã•ã„ã€‚")

import networkx as nx # (â˜…) Step B (å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯) ã§å¿…è¦
from itertools import combinations # (â˜…) Step B (å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯) ã§å¿…è¦

def find_col(df: pd.DataFrame, patterns: List[str]) -> Optional[str]:
    """DataFrameã‹ã‚‰ã€è¤‡æ•°ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã«æœ€åˆã«ä¸€è‡´ã™ã‚‹åˆ—å(str)ã‚’1ã¤è¿”ã™"""
    cols = df.columns
    for pattern in patterns:
        try:
            # 1. å®Œå…¨ä¸€è‡´ (å¤§æ–‡å­—å°æ–‡å­—ç„¡è¦–)
            for col in cols:
                if col.lower() == pattern.lower():
                    return col
            # 2. éƒ¨åˆ†ä¸€è‡´ (å¤§æ–‡å­—å°æ–‡å­—ç„¡è¦–)
            for col in cols:
                if re.search(pattern, col, re.IGNORECASE):
                    return col
        except re.error:
            continue # (e.g. invalid regex pattern)
    return None

def find_cols(df: pd.DataFrame, patterns: List[str]) -> List[str]:
    """DataFrameã‹ã‚‰ã€è¤‡æ•°ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ä¸€è‡´ã™ã‚‹åˆ—å(list)ã‚’ã™ã¹ã¦è¿”ã™"""
    cols = df.columns
    found_cols = set()
    for pattern in patterns:
        try:
            # 1. ï½¥ï½¥ï½¥ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ (hard-coded rule from old function)
            for col in cols:
                 if col.endswith('ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰'):
                     found_cols.add(col)
            # 2. éƒ¨åˆ†ä¸€è‡´ (å¤§æ–‡å­—å°æ–‡å­—ç„¡è¦–)
            for col in cols:
                if re.search(pattern, col, re.IGNORECASE):
                    found_cols.add(col)
        except re.error:
            continue
    return sorted(list(found_cols))

def find_engagement_cols(df: pd.DataFrame, patterns: List[str]) -> List[str]:
    """DataFrameã‹ã‚‰ã€ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ä¸€è‡´ã™ã‚‹ã€Œæ•°å€¤ã€åˆ—å(list)ã‚’ã™ã¹ã¦è¿”ã™"""
    numeric_cols = df.select_dtypes(include=np.number).columns
    found_cols = set()
    for pattern in patterns:
        try:
            for col in numeric_cols: # (â˜…) Only search numeric cols
                if re.search(pattern, col, re.IGNORECASE):
                    found_cols.add(col)
        except re.error:
            continue
    return sorted(list(found_cols))
# (â˜…) --- END NEW HELPER FUNCTIONS ---


def suggest_analysis_techniques_py(df: pd.DataFrame) -> List[Dict[str, Any]]:
    """
    (Step B) ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’åˆ†æã—ã€Pythonã§å®Ÿè¡Œå¯èƒ½ãªåŸºæœ¬çš„ãªåˆ†ææ‰‹æ³•ã‚’ææ¡ˆã™ã‚‹ã€‚
    (â˜…) ä¿®æ­£: 2024/11/10 - re.search ã‚’ä½¿ç”¨ã—ã€åˆ—åã‚’æŸ”è»Ÿã«æ¤œç´¢ã™ã‚‹ã‚ˆã†å …ç‰¢åŒ–
    """
    suggestions = []
    if df is None or df.empty:
        logger.error("suggest_analysis_techniques_py: DFãŒç©ºã§ã™ã€‚")
        return suggestions
        
    try:
        # (â˜…) --- 1. æŸ”è»Ÿãªåˆ—åã®ç‰¹å®š ---
        all_cols = list(df.columns)
        
        # (â˜…) ä¸»è¦ãªåˆ—ã‚’è¦‹ã¤ã‘ã‚‹
        text_col = find_col(df, ['ANALYSIS_TEXT_COLUMN', 'text', 'content', 'æœ¬æ–‡'])
        topic_col = find_col(df, ['è©±é¡Œã‚«ãƒ†ã‚´ãƒª', 'topic', 'category'])
        location_col = find_col(df, ['å¸‚åŒºç”ºæ‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰', 'location', 'city', 'åœ°åŸŸ'])
        tour_spot_col = find_col(df, ['è¦³å…‰åœ°ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰', 'tourist_spot', 'spot'])
        hashtag_col = find_col(df, ['hash', 'ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°'])
        sentiment_col = find_col(df, ['sent', 'ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆ'])
        date_col = find_col(df, ['date', 'time', 'æ—¥ä»˜', 'æ—¥æ™‚'])
        
        # (â˜…) è¤‡æ•°ã®å¯èƒ½æ€§ãŒã‚ã‚‹åˆ—
        engagement_cols = find_engagement_cols(df, ['eng', 'like', 'ã„ã„ã­', 'ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ'])
        # (â˜…) `...ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰` ã§çµ‚ã‚ã‚‹åˆ— + `topic_col` ã‚„ `location_col` ãªã©
        flag_cols = find_cols(df, ['key', 'keyword', 'ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰'])
        flag_cols = sorted(list(set(flag_cols + [c for c in [topic_col, location_col, tour_spot_col, hashtag_col] if c])))

        other_categorical = [
            col for col in df.select_dtypes(include='object').columns
            if col not in flag_cols and col != text_col and col != date_col
        ]
        
        logger.info(f"ææ¡ˆåˆ†æ(PY) - Text:{text_col}, Topic:{topic_col}, Location:{location_col}")
        logger.info(f"ææ¡ˆåˆ†æ(PY) - FlagCols(All):{flag_cols}")
        logger.info(f"ææ¡ˆåˆ†æ(PY) - Engagement:{engagement_cols}, Sentiment:{sentiment_col}, Date:{date_col}")

        potential_suggestions = []

        # (â˜…) --- 2. ææ¡ˆãƒ­ã‚¸ãƒƒã‚¯ (å …ç‰¢åŒ–ç‰ˆ) ---

        # --- 1. å…¨ä½“ãƒ¡ãƒˆãƒªã‚¯ã‚¹ ---
        # (â˜…) ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆåˆ—ã¨ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆåˆ—ã‚’ `suitable_cols` ã«æ¸¡ã™
        overall_metric_cols = [c for c in [sentiment_col] + engagement_cols if c]
        potential_suggestions.append({
            "priority": 1, "name": "å…¨ä½“ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹",
            "description": "æŠ•ç¨¿æ•°ã€ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆã€ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆå‚¾å‘ãªã©ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã®æ¦‚è¦ã‚’è¨ˆç®—ã—ã¾ã™ã€‚",
            "reason": "ãƒ‡ãƒ¼ã‚¿å…¨ä½“ã®çŠ¶æ³æŠŠæ¡ã«å¿…é ˆã§ã™ã€‚",
            "suitable_cols": overall_metric_cols, # (â˜…) 
            "type": "python"
        })

        # --- 3. å˜ç´”é›†è¨ˆï¼ˆé »åº¦åˆ†æï¼‰---
        # (â˜…) è¦‹ã¤ã‹ã£ãŸã™ã¹ã¦ã®ã€Œãƒ•ãƒ©ã‚°åˆ—ã€ã«å¯¾ã—ã¦ææ¡ˆ
        if flag_cols:
            for col in flag_cols:
                potential_suggestions.append({
                    "priority": 1, 
                    "name": f"å˜ç´”é›†è¨ˆ: {col}", # (â˜…) ä¾‹: "å˜ç´”é›†è¨ˆ: å¸‚åŒºç”ºæ‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰"
                    "description": f"ã€Œ{col}ã€åˆ—ã®å‡ºç¾é »åº¦ï¼ˆTOP50ï¼‰ã‚’åˆ†æã—ã¾ã™ã€‚",
                    "reason": f"StepAã§ç”Ÿæˆã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ—({col})ã®åŸºæœ¬æŒ‡æ¨™ã§ã™ã€‚",
                    "suitable_cols": [col], # (â˜…) 1åˆ—ã®ã¿
                    "type": "python"
                })

        # --- 2. ã‚¯ãƒ­ã‚¹é›†è¨ˆ ---
        if len(flag_cols) >= 2:
            potential_suggestions.append({
                "priority": 2, "name": "ã‚¯ãƒ­ã‚¹é›†è¨ˆï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é–“ï¼‰",
                "description": "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é–“ã®çµ„ã¿åˆã‚ã›ã§å¤šãå‡ºç¾ã™ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¢ã‚Šã¾ã™ã€‚",
                "reason": f"è¤‡æ•°ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ—({len(flag_cols)}å€‹)ã‚ã‚Šã€é–¢é€£æ€§ã®ç™ºè¦‹ã«ã€‚",
                "suitable_cols": flag_cols, 
                "type": "python"
            })
        if flag_cols and other_categorical:
             potential_suggestions.append({
                "priority": 2, "name": "ã‚¯ãƒ­ã‚¹é›†è¨ˆï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰Ã—å±æ€§ï¼‰",
                "description": f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰({flag_cols[0]}ãªã©)ã¨ä»–ã®å±æ€§({', '.join(other_categorical)})ã®é–¢ä¿‚æ€§ã‚’åˆ†æã—ã¾ã™ã€‚",
                "reason": f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ—ã¨ä»–ã‚«ãƒ†ã‚´ãƒªåˆ—({len(other_categorical)}å€‹)ã‚ã‚Šã€‚",
                "suitable_cols": flag_cols + other_categorical, 
                "type": "python"
            })
            
        # (â˜…) ä¿®æ­£: `topic_col` ã¨ `tour_spot_col` ãŒä¸¡æ–¹è¦‹ã¤ã‹ã£ãŸå ´åˆ
        if topic_col and tour_spot_col:
            potential_suggestions.append({
                "priority": 2, "name": f"{topic_col}åˆ¥ {tour_spot_col} TOP10",
                "description": f"ã€Œ{topic_col}ã€ã¨ã€Œ{tour_spot_col}ã€ã‚’ã‚¯ãƒ­ã‚¹é›†è¨ˆã—ã€ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®äººæ°—è¦³å…‰åœ°ã‚’åˆ†æã—ã¾ã™ã€‚",
                "reason": "ã‚«ãƒ†ã‚´ãƒªã¨è¦³å…‰åœ°ã®é–¢é€£æ€§ã‚’åˆ†æã—ã¾ã™ã€‚",
                "suitable_cols": [topic_col, tour_spot_col],
                "type": "python"
            })

        # --- 3. æ™‚ç³»åˆ—åˆ†æ ---
        if date_col and flag_cols:
            potential_suggestions.append({
                "priority": 3, "name": "æ™‚ç³»åˆ—ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æ",
                "description": f"ç‰¹å®šã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å‡ºç¾æ•°ãŒæ™‚é–“ï¼ˆ{date_col}ãªã©ï¼‰ã¨ã¨ã‚‚ã«ã©ã†å¤‰åŒ–ã—ãŸã‹ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’åˆ†æã—ã¾ã™ã€‚",
                "reason": f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ—ã¨æ—¥æ™‚åˆ—({date_col})ã‚ã‚Šã€‚",
                "suitable_cols": {"datetime": [date_col], "keywords": flag_cols}, # (â˜…) 
                "type": "python"
            })
            
        # --- 3. å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ ---
        if text_col:
            potential_suggestions.append({
                "priority": 3, "name": "å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯",
                "description": "æŠ•ç¨¿ãƒ†ã‚­ã‚¹ãƒˆå†…ã®å˜èªã®å‡ºç¾ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æã—ã€é–¢é€£æ€§ã®é«˜ã„å˜èªã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚",
                "reason": "ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‹ã‚‰éš ã‚ŒãŸãƒˆãƒ”ãƒƒã‚¯ã‚„é–¢é€£æ€§ã‚’ç™ºè¦‹ã—ã¾ã™ã€‚",
                "suitable_cols": [text_col],
                "type": "python"
            })
            
        # --- 4. ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚° ---
        if text_col:
            potential_suggestions.append({
                "priority": 4, "name": "ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°ï¼ˆé »å‡ºå˜èªï¼‰",
                "description": "åŸæ–‡ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰é »å‡ºã™ã‚‹å˜èªã‚’æŠ½å‡ºã—ã€ã©ã®ã‚ˆã†ãªè¨€è‘‰ãŒå¤šãä½¿ã‚ã‚Œã¦ã„ã‚‹ã‹å…¨ä½“åƒã‚’æŠŠæ¡ã—ã¾ã™ã€‚",
                "reason": "åŸæ–‡ãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚Šã€ã‚¿ã‚°ä»˜ã‘ä»¥å¤–ã®ã‚¤ãƒ³ã‚µã‚¤ãƒˆç™ºè¦‹ã«ã€‚",
                "suitable_cols": [text_col],
                "type": "python"
            })

        # --- 4. è©±é¡Œã‚«ãƒ†ã‚´ãƒªåˆ¥ ã‚µãƒãƒª (Python + AI) ---
        if topic_col and text_col:
            potential_suggestions.append({
                "priority": 4, "name": "è©±é¡Œã‚«ãƒ†ã‚´ãƒªåˆ¥ æŠ•ç¨¿æ•°ã¨ã‚µãƒãƒª",
                "description": "æŒ‡å®šã•ã‚ŒãŸè©±é¡Œã‚«ãƒ†ã‚´ãƒªï¼ˆã‚°ãƒ«ãƒ¡ã€è‡ªç„¶ãªã©ï¼‰ã”ã¨ã«æŠ•ç¨¿æ•°ã‚’é›†è¨ˆã—ã€AIãŒæŠ•ç¨¿å†…å®¹ã®ã‚µãƒãƒªã‚’ç”Ÿæˆã—ã¾ã™ã€‚",
                "reason": "ã‚«ãƒ†ã‚´ãƒªã”ã¨ã®ä¸»è¦ãªè©±é¡Œã‚’æŠŠæ¡ã—ã¾ã™ã€‚",
                "suitable_cols": [topic_col, text_col], # (â˜…)
                "type": "python"
            })

        # --- 4. è©±é¡Œã‚«ãƒ†ã‚´ãƒªåˆ¥ ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆTOP5 (Python + AI) ---
        if topic_col and text_col and engagement_cols:
            potential_suggestions.append({
                "priority": 4, "name": "è©±é¡Œã‚«ãƒ†ã‚´ãƒªåˆ¥ ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆTOP5ã¨æ¦‚è¦",
                "description": f"æŒ‡å®šã•ã‚ŒãŸè©±é¡Œã‚«ãƒ†ã‚´ãƒªã”ã¨ã«ã€ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆï¼ˆ{engagement_cols[0]}ï¼‰ãŒé«˜ã„TOP5æŠ•ç¨¿ã‚’æŠ½å‡ºã—ã€AIãŒãã®æ¦‚è¦ã‚’ç”Ÿæˆã—ã¾ã™ã€‚",
                "reason": "ã‚«ãƒ†ã‚´ãƒªã”ã¨ã«ã€Œãƒã‚ºã£ãŸã€æŠ•ç¨¿ã®å†…å®¹ã‚’æŠŠæ¡ã—ã¾ã™ã€‚",
                "suitable_cols": {'topic': [topic_col], 'text': [text_col], 'engagement': engagement_cols},
                "type": "python"
            })

        suggestions = sorted(potential_suggestions, key=lambda x: x['priority'])
        
        # (â˜…) é‡è¤‡ã™ã‚‹ææ¡ˆã‚’å‰Šé™¤ (ä¾‹: "å˜ç´”é›†è¨ˆ: è©±é¡Œã‚«ãƒ†ã‚´ãƒª" ã¨ "è©±é¡Œã‚«ãƒ†ã‚´ãƒªåˆ¥ ...")
        final_suggestions = []
        seen_names = set()
        for s in suggestions:
             if s['name'] not in seen_names:
                 final_suggestions.append(s)
                 seen_names.add(s['name'])
                 
        logger.info(f"Pythonãƒ™ãƒ¼ã‚¹ææ¡ˆ(ã‚½ãƒ¼ãƒˆå¾Œ): {[s['name'] for s in final_suggestions]}")
        return final_suggestions

    except Exception as e:
        logger.error(f"Pythonåˆ†ææ‰‹æ³•ææ¡ˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        st.warning(f"åˆ†ææ‰‹æ³•ææ¡ˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
    return suggestions

def suggest_analysis_techniques_ai(
    user_prompt: str,
    df: pd.DataFrame,
    existing_suggestions: List[Dict[str, Any]]
) -> List[Dict[str, Any]]:
    """
    (Step B) ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è‡ªç”±è¨˜è¿°ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«åŸºã¥ãã€AIãŒè¿½åŠ ã®åˆ†ææ‰‹æ³•ã‚’ææ¡ˆã™ã‚‹ã€‚
    """
    logger.info("AIãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ™ãƒ¼ã‚¹ã®åˆ†æææ¡ˆ (Flash Lite) ã‚’é–‹å§‹...")
    
    llm = get_llm(model_name=MODEL_FLASH_LITE, temperature=0.1)
    if llm is None:
        logger.error("suggest_analysis_techniques_ai: LLM (Flash Lite) ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")
        return []

    try:
        col_info = []
        for col in df.columns:
            col_info.append(f"- {col} (å‹: {df[col].dtype}, ä¾‹: {df[col].dropna().iloc[0] if not df[col].dropna().empty else 'N/A'})")
        column_info_str = "\n".join(col_info[:15])
        
        existing_names = [s['name'] for s in existing_suggestions]
        
        prompt = PromptTemplate.from_template(
            """
            ã‚ãªãŸã¯ãƒ‡ãƒ¼ã‚¿åˆ†æã®å°‚é–€å®¶ã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã€Œåˆ†ææŒ‡ç¤ºã€ã¨ã€Œãƒ‡ãƒ¼ã‚¿æ§‹é€ ã€ã‚’èª­ã¿ã€å®Ÿè¡Œå¯èƒ½ãªã€Œåˆ†æã‚¿ã‚¹ã‚¯ã€ã‚’JSONãƒªã‚¹ãƒˆå½¢å¼ã§ææ¡ˆã—ã¦ãã ã•ã„ã€‚
            
            # ãƒ‡ãƒ¼ã‚¿æ§‹é€  (åˆ©ç”¨å¯èƒ½ãªåˆ—å):
            {column_info}
            
            # æ—¢ã«ææ¡ˆæ¸ˆã¿ã®ã‚¿ã‚¹ã‚¯ (ã“ã‚Œã‚‰ã¯ææ¡ˆã—ãªã„ã§ãã ã•ã„):
            {existing_tasks}
            
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®åˆ†ææŒ‡ç¤º:
            {user_prompt}
            
            # æŒ‡ç¤º:
            1. ã€Œãƒ¦ãƒ¼ã‚¶ãƒ¼ã®åˆ†ææŒ‡ç¤ºã€ã‚’è§£é‡ˆã—ã€å…·ä½“çš„ãªåˆ†æã‚¿ã‚¹ã‚¯ï¼ˆä¾‹ï¼šã€Œåºƒå³¶å¸‚ã¨è¦³å…‰åœ°ã®ç›¸é–¢åˆ†æã€ï¼‰ã«åˆ†è§£ã™ã‚‹ã€‚
            2. å„ã‚¿ã‚¹ã‚¯ã‚’ä»¥ä¸‹ã®JSONå½¢å¼ã§å®šç¾©ã™ã‚‹ã€‚
            3. `name`ã¯ã‚¿ã‚¹ã‚¯åã€`description`ã¯AIï¼ˆã‚ãªãŸè‡ªèº«ï¼‰ãŒã“ã®å¾Œå®Ÿè¡Œã™ã‚‹ã‚¿ã‚¹ã‚¯ã®å…·ä½“çš„ãªæŒ‡ç¤ºï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼‰ã¨ã™ã‚‹ã€‚
            4. `priority`ã¯ 5 å›ºå®šã€`type`ã¯ "ai" å›ºå®šã¨ã™ã‚‹ã€‚
            5. æŒ‡ç¤ºãŒç©ºã€ã¾ãŸã¯è§£é‡ˆä¸èƒ½ãªå ´åˆã¯ã€ç©ºãƒªã‚¹ãƒˆ [] ã‚’è¿”ã™ã€‚
            
            # å›ç­” (JSONãƒªã‚¹ãƒˆå½¢å¼ã®ã¿):
            [
              {{
                "priority": 5,
                "name": "ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡ç¤ºã«åŸºã¥ãã‚¿ã‚¹ã‚¯å1ï¼‰",
                "description": "ï¼ˆã“ã®ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã™ã‚‹ãŸã‚ã®AIã¸ã®å…·ä½“çš„ãªæŒ‡ç¤ºãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ1ï¼‰",
                "reason": "ãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡ç¤ºã«åŸºã¥ã",
                "suitable_cols": [],
                "type": "ai"
              }}
            ]
            """
        )
        chain = prompt | llm | StrOutputParser()
        response_str = chain.invoke({
            "column_info": column_info_str,
            "user_prompt": user_prompt,
            "existing_tasks": ", ".join(existing_names)
        })

        logger.info(f"AIè¿½åŠ ææ¡ˆ(ç”Ÿ): {response_str}")
        match = re.search(r'\[.*\]', response_str, re.DOTALL)
        if not match:
            logger.warning("AIãŒJSONãƒªã‚¹ãƒˆå½¢å¼ã§å¿œç­”ã—ã¾ã›ã‚“ã§ã—ãŸã€‚")
            return []
            
        json_str = match.group(0)
        ai_suggestions = json.loads(json_str)
        
        for s in ai_suggestions:
            s['type'] = 'ai'
            if 'priority' not in s: s['priority'] = 5
            
        logger.info(f"AIè¿½åŠ ææ¡ˆ(ãƒ‘ãƒ¼ã‚¹æ¸ˆ): {len(ai_suggestions)}ä»¶")
        return ai_suggestions

    except Exception as e:
        logger.error(f"AIè¿½åŠ ææ¡ˆã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        st.warning(f"AIè¿½åŠ ææ¡ˆã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return []

import networkx as nx # (â˜…) Step B (å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯) ã§å¿…è¦
from itertools import combinations # (â˜…) Step B (å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯) ã§å¿…è¦
import math # (â˜…) ã‚°ãƒ©ãƒ•ã®ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆè¨ˆç®—ç”¨


# --- 8.0. (â˜…) ã‚°ãƒ©ãƒ•ç”Ÿæˆãƒ˜ãƒ«ãƒ‘ãƒ¼ (ã‚°ãƒ©ãƒ•ã‚µã‚¤ã‚ºä¿®æ­£) ---

def generate_graph_image(
    df: pd.DataFrame,
    plot_type: str,
    x_col: Optional[str] = None,
    y_col: Optional[str] = None,
    title: str = "åˆ†æã‚°ãƒ©ãƒ•"
) -> Optional[str]:
    """
    (â˜…) DataFrameã‹ã‚‰matplotlibã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆã—ã€Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸç”»åƒæ–‡å­—åˆ—ã‚’è¿”ã™ã€‚
    (â˜…) 1. 2. ã‚°ãƒ©ãƒ•ã‚µã‚¤ã‚ºä¿®æ­£
    """
    logger.info(f"ã‚°ãƒ©ãƒ•ç”Ÿæˆé–‹å§‹: {title} (ã‚¿ã‚¤ãƒ—: {plot_type})")
    if df is None or df.empty:
        logger.warning("ã‚°ãƒ©ãƒ•ç”Ÿæˆã‚¹ã‚­ãƒƒãƒ—: DataFrameãŒç©ºã§ã™ã€‚")
        return None

    # (â˜…) --- ä¿®æ­£: ãƒ—ãƒ­ãƒƒãƒˆã‚¿ã‚¤ãƒ—ã«å¿œã˜ã¦Figureã‚µã‚¤ã‚ºã‚’å¤‰æ›´ ---
    if plot_type == 'network':
        plt.figure(figsize=(12, 12)) # (â˜…) 2. å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯: æ­£æ–¹å½¢ (12x12)
    elif plot_type == 'timeseries':
        plt.figure(figsize=(15, 7)) # (â˜…) 1. æ™‚ç³»åˆ—: æ¨ªé•· (15x7)
    else:
        plt.figure(figsize=(10, 7)) # (â˜…) ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ (æ£’ã‚°ãƒ©ãƒ•ãªã©)
    
    plt.rcParams['font.size'] = 12
    # (â˜…) --- ã“ã“ã¾ã§ãŒä¿®æ­£ç‚¹ ---
    
    try:
        if plot_type == 'bar' and x_col and y_col:
            df_plot = df.nlargest(20, y_col).sort_values(by=y_col, ascending=True)
            if df_plot.empty:
                raise ValueError("ã‚°ãƒ©ãƒ•æç”»å¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
                
            bars = plt.barh(df_plot[x_col], df_plot[y_col], color='#7280C1')
            plt.xlabel('ä»¶æ•°')
            plt.ylabel(x_col)
            plt.grid(axis='x', linestyle='--', alpha=0.6)
            
            for bar in bars:
                plt.text(
                    bar.get_width() + (df_plot[y_col].max() * 0.01),
                    bar.get_y() + bar.get_height() / 2,
                    f' {bar.get_width():.0f}',
                    va='center',
                    ha='left'
                )
        
        elif plot_type == 'timeseries' and x_col and y_col:
            try:
                df[x_col] = pd.to_datetime(df[x_col])
                df_pivot = df.pivot(index=x_col, columns='keyword', values=y_col).fillna(0)
                
                if len(df_pivot.columns) > 6:
                    top_5_keywords = df_pivot.sum().nlargest(5).index
                    df_pivot['ãã®ä»–'] = df_pivot.drop(columns=top_5_keywords).sum(axis=1)
                    df_pivot = df_pivot[list(top_5_keywords) + ['ãã®ä»–']]
                
                df_pivot.plot(kind='line', ax=plt.gca(), linewidth=2.5)
                plt.xlabel('æ—¥ä»˜')
                plt.ylabel('ä»¶æ•°')
                plt.legend(title='ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰', bbox_to_anchor=(1.05, 1), loc='upper left')
                plt.grid(axis='y', linestyle='--', alpha=0.6)
                
            except Exception as e:
                logger.error(f"æ™‚ç³»åˆ—ãƒ”ãƒœãƒƒãƒˆ/ãƒ—ãƒ­ãƒƒãƒˆã‚¨ãƒ©ãƒ¼: {e}")
                plt.plot(df[x_col], df[y_col])
                plt.xlabel(x_col)
                plt.ylabel(y_col)

        elif plot_type == 'network':
            df_plot = df.nlargest(100, 'weight')
            if df_plot.empty:
                raise ValueError("ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æç”»å¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

            G = nx.from_pandas_edgelist(df_plot, 'source', 'target', ['weight'])
            
            try:
                partition = community.best_partition(G)
                num_communities = len(set(partition.values()))
                colors = plt.cm.get_cmap('tab20', num_communities)
                node_colors = [colors(partition.get(node)) for node in G.nodes()]
            except Exception:
                node_colors = '#7280C1'
            
            # (â˜…) --- ä¿®æ­£: 2. å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆèª¿æ•´ ---
            # (â˜…) kå€¤ã‚’èª¿æ•´ (ãƒãƒ¼ãƒ‰ã‚’åºƒã’ã‚‹)
            k_val = 2.5 / math.sqrt(len(G.nodes())) # (â˜…) 1.5 -> 2.5 ã«å¤‰æ›´
            pos = nx.spring_layout(G, k=max(k_val, 0.5), iterations=50, seed=42)
            
            node_sizes = []
            try:
                for node in G.nodes():
                    total_weight = sum(data['weight'] for _, _, data in G.edges(node, data=True))
                    node_sizes.append(total_weight * 20)
            except Exception:
                node_sizes = 500
            
            edge_weights = [d['weight'] / df_plot['weight'].max() * 8 for u, v, d in G.edges(data=True)]

            nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color=node_colors, alpha=0.9)
            # (â˜…) 2. ã‚¨ãƒƒã‚¸ã®alphaã‚’èª¿æ•´ (ç´°ã)
            nx.draw_networkx_edges(G, pos, width=edge_weights, alpha=0.1, edge_color='grey')
            nx.draw_networkx_labels(G, pos, font_size=10, font_family='IPAGothic')
            # (â˜…) --- ã“ã“ã¾ã§ãŒä¿®æ­£ç‚¹ ---
            
            plt.axis('off')

        else:
            logger.warning(f"æœªå¯¾å¿œã®ãƒ—ãƒ­ãƒƒãƒˆã‚¿ã‚¤ãƒ—: {plot_type}")
            return None

        plt.title(title, fontsize=16, pad=20)
        plt.tight_layout()

        buf = BytesIO()
        plt.savefig(buf, format='png', dpi=96) # (â˜…) 150 -> 96 (ãƒˆãƒ¼ã‚¯ãƒ³æ•°å‰Šæ¸›)
        buf.seek(0)
        
        image_base64 = base64.b64encode(buf.getvalue()).decode('utf-8')
        logger.info(f"ã‚°ãƒ©ãƒ•ç”ŸæˆæˆåŠŸ: {title}")
        return image_base64

    except Exception as e:
        logger.error(f"ã‚°ãƒ©ãƒ•ç”Ÿæˆ ({title}) å¤±æ•—: {e}", exc_info=True)
        return None
    finally:
        plt.clf()
        plt.close('all')


# --- 8. (â˜…) Step B: åˆ†æææ¡ˆé–¢é€£ ---

def suggest_analysis_techniques_py(df: pd.DataFrame) -> List[Dict[str, Any]]:
    """
    (Step B) ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’åˆ†æã—ã€Pythonã§å®Ÿè¡Œå¯èƒ½ãªåŸºæœ¬çš„ãªåˆ†ææ‰‹æ³•ã‚’ææ¡ˆã™ã‚‹ã€‚
    (â˜…) 3. å˜ç´”é›†è¨ˆã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’ä¿®æ­£
    """
    suggestions = []
    if df is None or df.empty:
        logger.error("suggest_analysis_techniques_py: DFãŒç©ºã§ã™ã€‚")
        return suggestions
        
    try:
        numeric_cols = df.select_dtypes(include=np.number).columns.tolist()
        object_cols = df.select_dtypes(include='object').columns.tolist()
        datetime_cols = []

        for col in object_cols:
             if df[col].isnull().sum() / len(df) > 0.5: continue
             sample = df[col].dropna().head(50)
             if sample.empty: continue
             try:
                 pd.to_datetime(sample, errors='raise')
                 temp_dt = pd.to_datetime(df[col], errors='coerce').dropna()
                 if not temp_dt.empty and (temp_dt.dt.year.nunique() > 1 or temp_dt.dt.month.nunique() > 1 or temp_dt.dt.day.nunique() > 1 or col.lower() in ['date', 'time', 'timestamp', 'æ—¥ä»˜', 'æ—¥æ™‚']):
                     datetime_cols.append(col)
                     logger.info(f"åˆ— '{col}' ã‚’æ—¥æ™‚åˆ—ã¨ã—ã¦èªè­˜ã—ã¾ã—ãŸã€‚")
             except (ValueError, TypeError, OverflowError, pd.errors.ParserError):
                 pass

        numeric_cols = [col for col in numeric_cols if col != 'id']
        categorical_cols = [col for col in object_cols if col != 'ANALYSIS_TEXT_COLUMN' and col not in datetime_cols]
        # (â˜…) 3. _ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ ã§çµ‚ã‚ã‚‹åˆ—ã‚’ flag_cols ã¨ã™ã‚‹
        flag_cols = [col for col in categorical_cols if col.endswith('ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰')]
        other_categorical = [col for col in categorical_cols if not col.endswith('ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰')]
        
        logger.info(f"ææ¡ˆåˆ†æ(PY) - æ•°å€¤:{numeric_cols}, ã‚«ãƒ†ã‚´ãƒª(ãƒ•ãƒ©ã‚°):{flag_cols}, ã‚«ãƒ†ã‚´ãƒª(ä»–):{other_categorical}, æ—¥æ™‚:{datetime_cols}")

        potential_suggestions = []

        # (â˜…) --- 1. å…¨ä½“ãƒ¡ãƒˆãƒªã‚¯ã‚¹ ---
        potential_suggestions.append({
            "priority": 1, "name": "å…¨ä½“ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹",
            "description": "æŠ•ç¨¿æ•°ã€ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆã€ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆå‚¾å‘ãªã©ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã®æ¦‚è¦ã‚’è¨ˆç®—ã—ã¾ã™ã€‚",
            "reason": "ãƒ‡ãƒ¼ã‚¿å…¨ä½“ã®çŠ¶æ³æŠŠæ¡ã«å¿…é ˆã§ã™ã€‚",
            "suitable_cols": [col for col in df.columns if 'ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆ' in col or 'ã„ã„ã­' in col or 'ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ' in col],
            "type": "python"
        })

        # (â˜…) --- 3. å˜ç´”é›†è¨ˆï¼ˆé »åº¦åˆ†æï¼‰---
        # (â˜…) ä¿®æ­£: StepAã§ç”Ÿæˆã—ãŸå…¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ—ã‚’å€‹åˆ¥ã«ææ¡ˆ
        if flag_cols:
            for col in flag_cols:
                potential_suggestions.append({
                    "priority": 1, 
                    "name": f"å˜ç´”é›†è¨ˆ: {col}", # (â˜…) ä¾‹: "å˜ç´”é›†è¨ˆ: å¸‚åŒºç”ºæ‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰"
                    "description": f"ã€Œ{col}ã€åˆ—ã®å‡ºç¾é »åº¦ï¼ˆTOP50ï¼‰ã‚’åˆ†æã—ã¾ã™ã€‚",
                    "reason": f"StepAã§ç”Ÿæˆã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ—({col})ã®åŸºæœ¬æŒ‡æ¨™ã§ã™ã€‚",
                    "suitable_cols": [col], # (â˜…) 1åˆ—ã®ã¿
                    "type": "python"
                })

        # å„ªå…ˆåº¦2: ã‚¯ãƒ­ã‚¹é›†è¨ˆ
        if len(flag_cols) >= 2:
            potential_suggestions.append({
                "priority": 2, "name": "ã‚¯ãƒ­ã‚¹é›†è¨ˆï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é–“ï¼‰",
                "description": "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é–“ã®çµ„ã¿åˆã‚ã›ã§å¤šãå‡ºç¾ã™ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¢ã‚Šã¾ã™ã€‚",
                "reason": f"è¤‡æ•°ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ—({len(flag_cols)}å€‹)ã‚ã‚Šã€é–¢é€£æ€§ã®ç™ºè¦‹ã«ã€‚",
                "suitable_cols": flag_cols, # (â˜…) ç·¨é›†ç”¨ã«å…¨ãƒ•ãƒ©ã‚°åˆ—ã‚’æ¸¡ã™
                "type": "python"
            })
        if flag_cols and other_categorical:
             potential_suggestions.append({
                "priority": 2, "name": "ã‚¯ãƒ­ã‚¹é›†è¨ˆï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰Ã—å±æ€§ï¼‰",
                "description": f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰({flag_cols[0]}ãªã©)ã¨ä»–ã®å±æ€§({', '.join(other_categorical)})ã®é–¢ä¿‚æ€§ã‚’åˆ†æã—ã¾ã™ã€‚",
                "reason": f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ—ã¨ä»–ã‚«ãƒ†ã‚´ãƒªåˆ—({len(other_categorical)}å€‹)ã‚ã‚Šã€‚",
                "suitable_cols": flag_cols + other_categorical, # (â˜…) ç·¨é›†ç”¨ã«å…¨åˆ—ã‚’æ¸¡ã™
                "type": "python"
            })
            
        if 'è©±é¡Œã‚«ãƒ†ã‚´ãƒª' in df.columns and 'è¦³å…‰åœ°ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰' in df.columns:
            potential_suggestions.append({
                "priority": 2, "name": "è©±é¡Œã‚«ãƒ†ã‚´ãƒªåˆ¥ è¦³å…‰åœ°TOP10",
                "description": "ã€Œè©±é¡Œã‚«ãƒ†ã‚´ãƒªã€ã¨ã€Œè¦³å…‰åœ°ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€ã‚’ã‚¯ãƒ­ã‚¹é›†è¨ˆã—ã€ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®äººæ°—è¦³å…‰åœ°ã‚’åˆ†æã—ã¾ã™ã€‚",
                "reason": "ã‚«ãƒ†ã‚´ãƒªã¨è¦³å…‰åœ°ã®é–¢é€£æ€§ã‚’åˆ†æã—ã¾ã™ã€‚",
                "suitable_cols": ['è©±é¡Œã‚«ãƒ†ã‚´ãƒª', 'è¦³å…‰åœ°ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰'],
                "type": "python"
            })

        # å„ªå…ˆåº¦3: æ™‚ç³»åˆ—åˆ†æ
        if datetime_cols and flag_cols:
            potential_suggestions.append({
                "priority": 3, "name": "æ™‚ç³»åˆ—ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æ",
                "description": f"ç‰¹å®šã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å‡ºç¾æ•°ãŒæ™‚é–“ï¼ˆ{datetime_cols[0]}ãªã©ï¼‰ã¨ã¨ã‚‚ã«ã©ã†å¤‰åŒ–ã—ãŸã‹ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’åˆ†æã—ã¾ã™ã€‚",
                "reason": f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ—ã¨æ—¥æ™‚åˆ—({len(datetime_cols)}å€‹)ã‚ã‚Šã€‚",
                "suitable_cols": {"datetime": datetime_cols, "keywords": flag_cols}, # (â˜…) ç·¨é›†ç”¨ã«å…¨å€™è£œã‚’æ¸¡ã™
                "type": "python"
            })
            
        # (â˜…) --- 3. å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ ---
        if 'ANALYSIS_TEXT_COLUMN' in df.columns:
            potential_suggestions.append({
                "priority": 3, "name": "å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯",
                "description": "æŠ•ç¨¿ãƒ†ã‚­ã‚¹ãƒˆå†…ã®å˜èªã®å‡ºç¾ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æã—ã€é–¢é€£æ€§ã®é«˜ã„å˜èªã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚",
                "reason": "ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‹ã‚‰éš ã‚ŒãŸãƒˆãƒ”ãƒƒã‚¯ã‚„é–¢é€£æ€§ã‚’ç™ºè¦‹ã—ã¾ã™ã€‚",
                "suitable_cols": ['ANALYSIS_TEXT_COLUMN'],
                "type": "python"
            })
            
        # å„ªå…ˆåº¦4: ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°
        if 'ANALYSIS_TEXT_COLUMN' in df.columns:
            potential_suggestions.append({
                "priority": 4, "name": "ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°ï¼ˆé »å‡ºå˜èªï¼‰",
                "description": "åŸæ–‡ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰é »å‡ºã™ã‚‹å˜èªã‚’æŠ½å‡ºã—ã€ã©ã®ã‚ˆã†ãªè¨€è‘‰ãŒå¤šãä½¿ã‚ã‚Œã¦ã„ã‚‹ã‹å…¨ä½“åƒã‚’æŠŠæ¡ã—ã¾ã™ã€‚",
                "reason": "åŸæ–‡ãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚Šã€ã‚¿ã‚°ä»˜ã‘ä»¥å¤–ã®ã‚¤ãƒ³ã‚µã‚¤ãƒˆç™ºè¦‹ã«ã€‚",
                "suitable_cols": ['ANALYSIS_TEXT_COLUMN'],
                "type": "python"
            })

        # (â˜…) --- 4. è©±é¡Œã‚«ãƒ†ã‚´ãƒªåˆ¥ ã‚µãƒãƒª (Python + AI) ---
        if 'è©±é¡Œã‚«ãƒ†ã‚´ãƒª' in df.columns and 'ANALYSIS_TEXT_COLUMN' in df.columns:
            potential_suggestions.append({
                "priority": 4, "name": "è©±é¡Œã‚«ãƒ†ã‚´ãƒªåˆ¥ æŠ•ç¨¿æ•°ã¨ã‚µãƒãƒª",
                "description": "æŒ‡å®šã•ã‚ŒãŸè©±é¡Œã‚«ãƒ†ã‚´ãƒªï¼ˆã‚°ãƒ«ãƒ¡ã€è‡ªç„¶ãªã©ï¼‰ã”ã¨ã«æŠ•ç¨¿æ•°ã‚’é›†è¨ˆã—ã€AIãŒæŠ•ç¨¿å†…å®¹ã®ã‚µãƒãƒªã‚’ç”Ÿæˆã—ã¾ã™ã€‚",
                "reason": "ã‚«ãƒ†ã‚´ãƒªã”ã¨ã®ä¸»è¦ãªè©±é¡Œã‚’æŠŠæ¡ã—ã¾ã™ã€‚",
                "suitable_cols": ['è©±é¡Œã‚«ãƒ†ã‚´ãƒª', 'ANALYSIS_TEXT_COLUMN'],
                "type": "python"
            })

        # (â˜…) --- 4. è©±é¡Œã‚«ãƒ†ã‚´ãƒªåˆ¥ ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆTOP5 (Python + AI) ---
        engagement_cols = [col for col in numeric_cols if any(c in col.lower() for c in ['ã„ã„ã­', 'like', 'ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ', 'engagement'])]
        if 'è©±é¡Œã‚«ãƒ†ã‚´ãƒª' in df.columns and 'ANALYSIS_TEXT_COLUMN' in df.columns and engagement_cols:
            potential_suggestions.append({
                "priority": 4, "name": "è©±é¡Œã‚«ãƒ†ã‚´ãƒªåˆ¥ ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆTOP5ã¨æ¦‚è¦",
                "description": f"æŒ‡å®šã•ã‚ŒãŸè©±é¡Œã‚«ãƒ†ã‚´ãƒªã”ã¨ã«ã€ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆï¼ˆ{engagement_cols[0]}ï¼‰ãŒé«˜ã„TOP5æŠ•ç¨¿ã‚’æŠ½å‡ºã—ã€AIãŒãã®æ¦‚è¦ã‚’ç”Ÿæˆã—ã¾ã™ã€‚",
                "reason": "ã‚«ãƒ†ã‚´ãƒªã”ã¨ã«ã€Œãƒã‚ºã£ãŸã€æŠ•ç¨¿ã®å†…å®¹ã‚’æŠŠæ¡ã—ã¾ã™ã€‚",
                # (â˜…) ç·¨é›†ç”¨ã«å…¨å€™è£œã‚’æ¸¡ã™
                "suitable_cols": {'topic': ['è©±é¡Œã‚«ãƒ†ã‚´ãƒª'], 'text': ['ANALYSIS_TEXT_COLUMN'], 'engagement': engagement_cols},
                "type": "python"
            })

        suggestions = sorted(potential_suggestions, key=lambda x: x['priority'])
        logger.info(f"Pythonãƒ™ãƒ¼ã‚¹ææ¡ˆ(ã‚½ãƒ¼ãƒˆå¾Œ): {[s['name'] for s in suggestions]}")
        return suggestions

    except Exception as e:
        logger.error(f"Pythonåˆ†ææ‰‹æ³•ææ¡ˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        st.warning(f"åˆ†ææ‰‹æ³•ææ¡ˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
    return suggestions

def suggest_analysis_techniques_ai(
    user_prompt: str,
    df: pd.DataFrame,
    existing_suggestions: List[Dict[str, Any]]
) -> List[Dict[str, Any]]:
    """
    (Step B) ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è‡ªç”±è¨˜è¿°ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«åŸºã¥ãã€AIãŒè¿½åŠ ã®åˆ†ææ‰‹æ³•ã‚’ææ¡ˆã™ã‚‹ã€‚
    """
    logger.info("AIãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ™ãƒ¼ã‚¹ã®åˆ†æææ¡ˆ (Flash Lite) ã‚’é–‹å§‹...")
    
    llm = get_llm(model_name=MODEL_FLASH_LITE, temperature=0.1)
    if llm is None:
        logger.error("suggest_analysis_techniques_ai: LLM (Flash Lite) ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")
        return []

    try:
        col_info = []
        for col in df.columns:
            col_info.append(f"- {col} (å‹: {df[col].dtype}, ä¾‹: {df[col].dropna().iloc[0] if not df[col].dropna().empty else 'N/A'})")
        column_info_str = "\n".join(col_info[:15])
        
        existing_names = [s['name'] for s in existing_suggestions]
        
        prompt = PromptTemplate.from_template(
            """
            ã‚ãªãŸã¯ãƒ‡ãƒ¼ã‚¿åˆ†æã®å°‚é–€å®¶ã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã€Œåˆ†ææŒ‡ç¤ºã€ã¨ã€Œãƒ‡ãƒ¼ã‚¿æ§‹é€ ã€ã‚’èª­ã¿ã€å®Ÿè¡Œå¯èƒ½ãªã€Œåˆ†æã‚¿ã‚¹ã‚¯ã€ã‚’JSONãƒªã‚¹ãƒˆå½¢å¼ã§ææ¡ˆã—ã¦ãã ã•ã„ã€‚
            
            # ãƒ‡ãƒ¼ã‚¿æ§‹é€  (åˆ©ç”¨å¯èƒ½ãªåˆ—å):
            {column_info}
            
            # æ—¢ã«ææ¡ˆæ¸ˆã¿ã®ã‚¿ã‚¹ã‚¯ (ã“ã‚Œã‚‰ã¯ææ¡ˆã—ãªã„ã§ãã ã•ã„):
            {existing_tasks}
            
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®åˆ†ææŒ‡ç¤º:
            {user_prompt}
            
            # æŒ‡ç¤º:
            1. ã€Œãƒ¦ãƒ¼ã‚¶ãƒ¼ã®åˆ†ææŒ‡ç¤ºã€ã‚’è§£é‡ˆã—ã€å…·ä½“çš„ãªåˆ†æã‚¿ã‚¹ã‚¯ï¼ˆä¾‹ï¼šã€Œåºƒå³¶å¸‚ã¨è¦³å…‰åœ°ã®ç›¸é–¢åˆ†æã€ï¼‰ã«åˆ†è§£ã™ã‚‹ã€‚
            2. å„ã‚¿ã‚¹ã‚¯ã‚’ä»¥ä¸‹ã®JSONå½¢å¼ã§å®šç¾©ã™ã‚‹ã€‚
            3. `name`ã¯ã‚¿ã‚¹ã‚¯åã€`description`ã¯AIï¼ˆã‚ãªãŸè‡ªèº«ï¼‰ãŒã“ã®å¾Œå®Ÿè¡Œã™ã‚‹ã‚¿ã‚¹ã‚¯ã®å…·ä½“çš„ãªæŒ‡ç¤ºï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼‰ã¨ã™ã‚‹ã€‚
            4. `priority`ã¯ 5 å›ºå®šã€`type`ã¯ "ai" å›ºå®šã¨ã™ã‚‹ã€‚
            5. æŒ‡ç¤ºãŒç©ºã€ã¾ãŸã¯è§£é‡ˆä¸èƒ½ãªå ´åˆã¯ã€ç©ºãƒªã‚¹ãƒˆ [] ã‚’è¿”ã™ã€‚
            
            # å›ç­” (JSONãƒªã‚¹ãƒˆå½¢å¼ã®ã¿):
            [
              {{
                "priority": 5,
                "name": "ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡ç¤ºã«åŸºã¥ãã‚¿ã‚¹ã‚¯å1ï¼‰",
                "description": "ï¼ˆã“ã®ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã™ã‚‹ãŸã‚ã®AIã¸ã®å…·ä½“çš„ãªæŒ‡ç¤ºãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ1ï¼‰",
                "reason": "ãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡ç¤ºã«åŸºã¥ã",
                "suitable_cols": [],
                "type": "ai"
              }}
            ]
            """
        )
        chain = prompt | llm | StrOutputParser()
        response_str = chain.invoke({
            "column_info": column_info_str,
            "user_prompt": user_prompt,
            "existing_tasks": ", ".join(existing_names)
        })

        logger.info(f"AIè¿½åŠ ææ¡ˆ(ç”Ÿ): {response_str}")
        match = re.search(r'\[.*\]', response_str, re.DOTALL)
        if not match:
            logger.warning("AIãŒJSONãƒªã‚¹ãƒˆå½¢å¼ã§å¿œç­”ã—ã¾ã›ã‚“ã§ã—ãŸã€‚")
            return []
            
        json_str = match.group(0)
        ai_suggestions = json.loads(json_str)
        
        for s in ai_suggestions:
            s['type'] = 'ai'
            if 'priority' not in s: s['priority'] = 5
            
        logger.info(f"AIè¿½åŠ ææ¡ˆ(ãƒ‘ãƒ¼ã‚¹æ¸ˆ): {len(ai_suggestions)}ä»¶")
        return ai_suggestions

    except Exception as e:
        logger.error(f"AIè¿½åŠ ææ¡ˆã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        st.warning(f"AIè¿½åŠ ææ¡ˆã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return []

# --- 8.1. (â˜…) Step B: Pythonåˆ†æãƒ˜ãƒ«ãƒ‘ãƒ¼ (ä¿®æ­£åæ˜ ) ---

def run_simple_count(df: pd.DataFrame, suggestion: Dict[str, Any]) -> Dict[str, Any]:
    """(Step B) å˜ç´”é›†è¨ˆï¼ˆé »åº¦åˆ†æï¼‰ã‚’å®Ÿè¡Œã—ã€DataFrameã¨ã‚°ãƒ©ãƒ•(Base64)ã‚’è¿”ã™"""
    results = {"data": pd.DataFrame(), "image_base64": None, "summary": ""}
    
    flag_cols = suggestion.get('suitable_cols', [])
    if not flag_cols:
        msg = "é›†è¨ˆå¯¾è±¡ã®åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
        logger.warning(f"run_simple_count: {msg}")
        results["summary"] = msg
        return results
    
    # (â˜…) 3. ææ¡ˆãƒ­ã‚¸ãƒƒã‚¯ã®å¤‰æ›´ã«ã‚ˆã‚Šã€suitable_cols[0] ã¯åˆ†æå¯¾è±¡ã®åˆ—å (e.g., 'è©±é¡Œã‚«ãƒ†ã‚´ãƒª') ã«ãªã£ã¦ã„ã‚‹
    col_to_analyze = flag_cols[0]
    
    if col_to_analyze not in df.columns:
        msg = f"åˆ— '{col_to_analyze}' ãŒDFã«å­˜åœ¨ã—ã¾ã›ã‚“ã€‚"
        logger.warning(f"run_simple_count: {msg}")
        results["summary"] = msg
        return results
        
    try:
        s = df[col_to_analyze].astype(str).str.split(', ').explode()
        s = s[s.str.strip().isin(['', 'nan', 'None', 'N/A', 'è©²å½“ãªã—']) == False]
        s = s.str.strip()
        
        if s.empty:
            msg = "é›†è¨ˆå¯¾è±¡ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
            logger.info(f"run_simple_count: {msg}")
            results["summary"] = msg
            return results
            
        counts = s.value_counts().head(50)
        counts_df = counts.reset_index()
        counts_df.columns = [col_to_analyze, 'count']
        
        results["data"] = counts_df
        results["summary"] = f"'{col_to_analyze}' ã®å˜ç´”é›†è¨ˆï¼ˆé »åº¦åˆ†æï¼‰ã‚’å®Ÿè¡Œã€‚ä¸Šä½ã¯ {counts_df.iloc[0,0]} ({counts_df.iloc[0,1]}ä»¶), {counts_df.iloc[1,0]} ({counts_df.iloc[1,1]}ä»¶) ã§ã—ãŸã€‚"
        
        results["image_base64"] = generate_graph_image(
            df=counts_df,
            plot_type='bar',
            x_col=col_to_analyze,
            y_col='count',
            title=f"ã€Œ{col_to_analyze}ã€ é »å‡ºTOP20"
        )
        return results
            
    except Exception as e:
        logger.error(f"run_simple_count error: {e}", exc_info=True)
        results["summary"] = f"ã‚¨ãƒ©ãƒ¼: {e}"
    return results

def run_crosstab(df: pd.DataFrame, suggestion: Dict[str, Any]) -> Dict[str, Any]:
    """(Step B) ã‚¯ãƒ­ã‚¹é›†è¨ˆã‚’å®Ÿè¡Œã—ã€DataFrameã‚’è¿”ã™"""
    results = {"data": pd.DataFrame(), "image_base64": None, "summary": ""}
    
    # (â˜…) UIã§ç·¨é›†ã•ã‚ŒãŸåˆ—ã‚’å–å¾—
    cols = suggestion.get('suitable_cols', [])
    if len(cols) < 2:
        msg = "ã‚¯ãƒ­ã‚¹é›†è¨ˆã«ã¯2åˆ—ä»¥ä¸Šå¿…è¦ã§ã™ã€‚"
        logger.warning(f"run_crosstab: {msg}")
        results["summary"] = msg
        return results

    # (â˜…) ç·¨é›†ãƒ­ã‚¸ãƒƒã‚¯ (UIå´ã§2åˆ—ãŒé¸æŠã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’æœŸå¾…)
    col1, col2 = cols[0], cols[1]

    if col1 not in df.columns or col2 not in df.columns:
        msg = f"é¸æŠã•ã‚ŒãŸåˆ— ({col1}, {col2}) ãŒDFã«å­˜åœ¨ã—ã¾ã›ã‚“ã€‚"
        logger.warning(f"run_crosstab: {msg}")
        results["summary"] = msg
        return results
    
    try:
        df_exploded_1 = df.assign(**{col1: df[col1].astype(str).str.split(', ')}).explode(col1)
        df_exploded_2 = df_exploded_1.assign(**{col2: df_exploded_1[col2].astype(str).str.split(', ')}).explode(col2)

        df_exploded_2[col1] = df_exploded_2[col1].str.strip()
        df_exploded_2[col2] = df_exploded_2[col2].str.strip()
        df_exploded_2 = df_exploded_2.replace('', np.nan).replace('nan', np.nan).replace('None', np.nan).dropna(subset=[col1, col2])
        
        crosstab_df = pd.crosstab(df_exploded_2[col1], df_exploded_2[col2])
        
        crosstab_long = crosstab_df.stack().reset_index()
        crosstab_long.columns = [col1, col2, 'count']
        crosstab_long = crosstab_long[crosstab_long['count'] > 0].sort_values(by='count', ascending=False)
        
        results["data"] = crosstab_long.head(100)
        results["summary"] = f"'{col1}' ã¨ '{col2}' ã®ã‚¯ãƒ­ã‚¹é›†è¨ˆã‚’å®Ÿè¡Œã€‚æœ€å¼·ã®çµ„ã¿åˆã‚ã›ã¯ {crosstab_long.iloc[0,0]} x {crosstab_long.iloc[0,1]} ({crosstab_long.iloc[0,2]}ä»¶) ã§ã—ãŸã€‚"
        logger.info("run_crosstab: ã‚°ãƒ©ãƒ•ç”Ÿæˆã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã—ãŸã€‚")
        
        return results
        
    except Exception as e:
        logger.error(f"run_crosstab error: {e}", exc_info=True)
        results["summary"] = f"ã‚¨ãƒ©ãƒ¼: {e}"
    return results

def run_timeseries(df: pd.DataFrame, suggestion: Dict[str, Any]) -> Dict[str, Any]:
    """(Step B) æ™‚ç³»åˆ—åˆ†æã‚’å®Ÿè¡Œã—ã€DataFrameã¨ã‚°ãƒ©ãƒ•(Base64)ã‚’è¿”ã™"""
    results = {"data": pd.DataFrame(), "image_base64": None, "summary": ""}
    
    # (â˜…) UIã§ç·¨é›†ã•ã‚ŒãŸåˆ—ã‚’å–å¾—
    cols_dict = suggestion.get('suitable_cols', {})
    if not isinstance(cols_dict, dict) or 'datetime' not in cols_dict or 'keywords' not in cols_dict:
        msg = "åˆ—æƒ…å ±ï¼ˆdatetime, keywordsï¼‰ãŒä¸ååˆ†ã§ã™ã€‚"
        logger.warning(f"run_timeseries: {msg}")
        results["summary"] = msg
        return results
        
    # (â˜…) ç·¨é›†ãƒ­ã‚¸ãƒƒã‚¯ (UIå´ã§1åˆ—ãšã¤é¸æŠã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’æœŸå¾…)
    dt_col = cols_dict['datetime'][0]
    kw_col = cols_dict['keywords'][0]

    if dt_col not in df.columns:
        msg = f"æ—¥æ™‚åˆ— '{dt_col}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
        logger.warning(f"run_timeseries: {msg}"); results["summary"] = msg; return results
    if kw_col not in df.columns:
        msg = f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ— '{kw_col}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
        logger.warning(f"run_timeseries: {msg}"); results["summary"] = msg; return results

    try:
        df_copy = df[[dt_col, kw_col]].copy()
        df_copy[dt_col] = pd.to_datetime(df_copy[dt_col], errors='coerce')
        df_copy = df_copy.dropna(subset=[dt_col])
        
        df_exploded = df_copy.assign(**{kw_col: df_copy[kw_col].astype(str).str.split(', ')}).explode(kw_col)
        df_exploded[kw_col] = df_exploded[kw_col].str.strip()
        df_exploded = df_exploded[df_exploded[kw_col].isin(['', 'nan', 'None', 'N/A', 'è©²å½“ãªã—']) == False]
        
        if df_exploded.empty:
            msg = "æœ‰åŠ¹ãªæ—¥æ™‚/ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚"
            logger.info(f"run_timeseries: {msg}"); results["summary"] = msg; return results

        time_df = df_exploded.groupby([pd.Grouper(key=dt_col, freq='D'), kw_col]).size().rename("count").reset_index()
        
        time_df.columns = ['date', 'keyword', 'count']
        
        top_keywords = df_exploded[kw_col].value_counts().head(50).index
        time_df_filtered = time_df[time_df['keyword'].isin(top_keywords)]
        
        time_df_for_graph = time_df_filtered.copy()
        
        time_df_for_json = time_df_filtered.sort_values(by=['keyword', 'date'])
        time_df_for_json['date'] = time_df_for_json['date'].dt.strftime('%Y-%m-%d')
        
        results["data"] = time_df_for_json
        
        results["image_base64"] = generate_graph_image(
            df=time_df_for_graph,
            plot_type='timeseries',
            x_col='date',
            y_col='count',
            title=f"ã€Œ{kw_col}ã€åˆ¥ æ™‚ç³»åˆ—ãƒˆãƒ¬ãƒ³ãƒ‰ (TOP5)"
        )
        results["summary"] = f"'{dt_col}' ã¨ '{kw_col}' ã§æ™‚ç³»åˆ—åˆ†æã‚’å®Ÿè¡Œã€‚TOP5ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ãƒˆãƒ¬ãƒ³ãƒ‰ã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆã—ã¾ã—ãŸã€‚"
        return results
            
    except Exception as e:
        logger.error(f"run_timeseries error: {e}", exc_info=True)
        results["summary"] = f"ã‚¨ãƒ©ãƒ¼: {e}"
    return results

def run_text_mining(df: pd.DataFrame, suggestion: Dict[str, Any]) -> Dict[str, Any]:
    """(Step B) ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°ï¼ˆé »å‡ºå˜èªï¼‰ã‚’å®Ÿè¡Œã—ã€DataFrameã¨ã‚°ãƒ©ãƒ•(Base64)ã‚’è¿”ã™"""
    results = {"data": pd.DataFrame(), "image_base64": None, "summary": ""}
    
    # (â˜…) UIã§ç·¨é›†ã•ã‚ŒãŸåˆ—ã‚’å–å¾—
    text_col = suggestion.get('suitable_cols', ['ANALYSIS_TEXT_COLUMN'])[0]
    
    if text_col not in df.columns or df[text_col].empty:
        msg = f"ãƒ†ã‚­ã‚¹ãƒˆåˆ— '{text_col}' ãŒãªã„ã‹ã€ç©ºã§ã™ã€‚"
        logger.warning(f"run_text_mining: {msg}"); results["summary"] = msg; return results

    nlp = load_spacy_model()
    if nlp is None:
        st.error("spaCyæ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
        results["summary"] = "spaCyæ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸã€‚"
        return results
            
    try:
        texts = df[text_col].dropna().astype(str)
        if texts.empty:
            results["summary"] = "ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ã€‚"
            return results
            
        words = []
        target_pos = {'NOUN', 'PROPN', 'ADJ'}
        stop_words = {
            'ã®', 'ã«', 'ã¯', 'ã‚’', 'ãŒ', 'ã§', 'ã¦', 'ã§ã™', 'ã¾ã™', 'ã“ã¨', 'ã‚‚ã®', 'ãã‚Œ', 'ã‚ã‚Œ',
            'ã“ã‚Œ', 'ãŸã‚', 'ã„ã‚‹', 'ã™ã‚‹', 'ã‚ã‚‹', 'ãªã„', 'ã„ã†', 'ã‚ˆã†', 'ãã†', 'ãªã©', 'ã•ã‚“',
            'çš„', 'äºº', 'è‡ªåˆ†', 'ç§', 'åƒ•', 'ä½•', 'ãã®', 'ã“ã®', 'ã‚ã®'
        }
        
        total_texts = len(texts)
        if 'progress_text' not in st.session_state:
             st.session_state.progress_text = ""
        st.session_state.progress_text = "ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚° (spaCy) å‡¦ç†ä¸­... 0%"

        for i, doc in enumerate(nlp.pipe(texts, disable=["parser", "ner"], batch_size=50)):
            for token in doc:
                if (token.pos_ in target_pos) and (not token.is_stop) and (token.lemma_ not in stop_words) and (len(token.lemma_) > 1):
                    words.append(token.lemma_)
            
            if (i + 1) % 100 == 0:
                percent = (i + 1) / total_texts
                st.session_state.progress_text = f"ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚° (spaCy) å‡¦ç†ä¸­... {percent:.0%}"

        if not words:
            msg = "æŠ½å‡ºå¯èƒ½ãªæœ‰åŠ¹ãªå˜èªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
            logger.warning(f"run_text_mining: {msg}"); results["summary"] = msg; return results

        word_counts = pd.Series(words).value_counts().head(100)
        word_counts_df = word_counts.reset_index()
        word_counts_df.columns = ['word', 'count']
        
        st.session_state.progress_text = "ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚° (spaCy) å®Œäº†ã€‚"
        
        results["data"] = word_counts_df
        results["summary"] = f"'{text_col}' ã«å¯¾ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œã€‚é »å‡ºå˜èªã¯ '{word_counts_df.iloc[0,0]}' ({word_counts_df.iloc[0,1]}ä»¶) ã§ã—ãŸã€‚"
        
        results["image_base64"] = generate_graph_image(
            df=word_counts_df,
            plot_type='bar',
            x_col='word',
            y_col='count',
            title=f"ã€Œ{text_col}ã€ é »å‡ºå˜èª TOP20"
        )
        return results
        
    except Exception as e:
        logger.error(f"run_text_mining error: {e}", exc_info=True)
        results["summary"] = f"ã‚¨ãƒ©ãƒ¼: {e}"
    return results

def run_overall_metrics(df: pd.DataFrame, suggestion: Dict[str, Any]) -> Dict[str, Any]:
    """(Step B) ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—ã™ã‚‹"""
    # (å¤‰æ›´ãªã—)
    logger.info("run_overall_metrics å®Ÿè¡Œ...")
    metrics = {}
    try:
        metrics["total_posts"] = len(df)
        engagement_cols = [col for col in df.columns if any(c in col.lower() for c in ['ã„ã„ã­', 'like', 'ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ', 'engagement', 'retweet', 'ãƒªãƒ„ã‚¤ãƒ¼ãƒˆ'])]
        total_engagement = 0
        if engagement_cols:
            for col in engagement_cols:
                if pd.api.types.is_numeric_dtype(df[col]):
                    total_engagement += df[col].sum()
            metrics["total_engagement"] = int(total_engagement)
        else:
            metrics["total_engagement"] = "N/A"
        sentiment_col = None
        if 'ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆ' in df.columns:
            sentiment_col = 'ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆ'
        elif len(df.columns) > 9 and ('ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆ' in str(df.columns[9]) or 'sentiment' in str(df.columns[9]).lower()):
            sentiment_col = df.columns[9]
        if sentiment_col:
            pos_count = int(df[df[sentiment_col].astype(str).str.contains('ãƒã‚¸ãƒ†ã‚£ãƒ–|Positive', case=False, na=False)].shape[0])
            neg_count = int(df[df[sentiment_col].astype(str).str.contains('ãƒã‚¬ãƒ†ã‚£ãƒ–|Negative', case=False, na=False)].shape[0])
            metrics["positive_posts"] = pos_count
            metrics["negative_posts"] = neg_count
            if (pos_count + neg_count) > 0:
                tendency = ((pos_count - neg_count) / (pos_count + neg_count)) * 100
                metrics["sentiment_tendency_percent"] = int(np.floor(tendency))
            else:
                metrics["sentiment_tendency_percent"] = 0
        else:
            logger.warning("åˆ— 'ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆ' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            metrics["positive_posts"] = "N/A"
            metrics["negative_posts"] = "N/A"
            metrics["sentiment_tendency_percent"] = "N/A"
        summary = f"å…¨ä½“ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—ã€‚ç·æŠ•ç¨¿æ•°: {metrics['total_posts']}ä»¶, ç·ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ: {metrics['total_engagement']}ã€‚"
        return {"data": metrics, "image_base64": None, "summary": summary}
    except Exception as e:
        logger.error(f"run_overall_metrics error: {e}", exc_info=True)
        return {"data": {"error": str(e)}, "image_base64": None, "summary": f"ã‚¨ãƒ©ãƒ¼: {e}"}

def run_cooccurrence_network(df: pd.DataFrame, suggestion: Dict[str, Any]) -> Dict[str, Any]:
    """(Step B) å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æ§‹ç¯‰ã—ã€ã‚¨ãƒƒã‚¸ãƒªã‚¹ãƒˆã®DataFrameã¨ã‚°ãƒ©ãƒ•(Base64)ã‚’è¿”ã™"""
    # (â˜…) UIã§ç·¨é›†ã•ã‚ŒãŸåˆ—ã‚’å–å¾—
    results = {"data": pd.DataFrame(), "image_base64": None, "summary": ""}
    text_col = suggestion.get('suitable_cols', ['ANALYSIS_TEXT_COLUMN'])[0]
    if text_col not in df.columns or df[text_col].empty:
        msg = f"ãƒ†ã‚­ã‚¹ãƒˆåˆ— '{text_col}' ãŒãªã„ã‹ã€ç©ºã§ã™ã€‚"
        logger.warning(f"run_cooccurrence_network: {msg}"); results["summary"] = msg; return results

    nlp = load_spacy_model()
    if nlp is None:
        st.error("spaCyæ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
        results["summary"] = "spaCyæ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸã€‚"
        return results

    try:
        texts = df[text_col].dropna().astype(str)
        if texts.empty:
            results["summary"] = "ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ã€‚"
            return results
        target_pos = {'NOUN', 'PROPN', 'ADJ'}
        stop_words = {
            'ã®', 'ã«', 'ã¯', 'ã‚’', 'ãŒ', 'ã§', 'ã¦', 'ã§ã™', 'ã¾ã™', 'ã“ã¨', 'ã‚‚ã®', 'ãã‚Œ', 'ã‚ã‚Œ',
            'ã“ã‚Œ', 'ãŸã‚', 'ã„ã‚‹', 'ã™ã‚‹', 'ã‚ã‚‹', 'ãªã„', 'ã„ã†', 'ã‚ˆã†', 'ãã†', 'ãªã©', 'ã•ã‚“',
            'çš„', 'äºº', 'è‡ªåˆ†', 'ç§', 'åƒ•', 'ä½•', 'ãã®', 'ã“ã®', 'ã‚ã®'
        }
        st.session_state.progress_text = "å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ (spaCy) å‡¦ç†ä¸­... 0%"
        total_texts = len(texts)
        doc_words_list = []
        for i, doc in enumerate(nlp.pipe(texts, disable=["parser", "ner"], batch_size=50)):
            words_in_text = set()
            for token in doc:
                if (token.pos_ in target_pos) and (not token.is_stop) and (token.lemma_ not in stop_words) and (len(token.lemma_) > 1):
                    words_in_text.add(token.lemma_)
            doc_words_list.append(list(words_in_text))
            if (i + 1) % 100 == 0:
                percent = (i + 1) / total_texts
                st.session_state.progress_text = f"å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ (spaCy) å‡¦ç†ä¸­... {percent:.0%}"

        all_words = [word for sublist in doc_words_list for word in sublist]
        if not all_words:
            msg = "æŠ½å‡ºå¯èƒ½ãªå˜èªãŒã‚ã‚Šã¾ã›ã‚“ã€‚"
            logger.warning(f"run_cooccurrence_network: {msg}"); results["summary"] = msg; return results
        top_n_words_limit = 100
        top_n_words_set = set(pd.Series(all_words).value_counts().head(top_n_words_limit).index)
        G = nx.Graph()
        for words_in_text_set in doc_words_list:
            filtered_words = [word for word in words_in_text_set if word in top_n_words_set]
            for word1, word2 in combinations(sorted(list(filtered_words)), 2):
                if G.has_edge(word1, word2):
                    G[word1][word2]['weight'] += 1
                else:
                    G.add_edge(word1, word2, weight=1)
        if G.number_of_nodes() == 0 or G.number_of_edges() == 0:
            msg = "æœ‰åŠ¹ãªã‚¨ãƒƒã‚¸ãŒæ§‹ç¯‰ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚"
            logger.info(f"run_cooccurrence_network: {msg}"); results["summary"] = msg; return results
        edge_list = []
        for u, v, data in G.edges(data=True):
            edge_list.append({"source": u, "target": v, "weight": data['weight']})
        edges_df = pd.DataFrame(edge_list)
        edges_df_sorted = edges_df.sort_values(by="weight", ascending=False).head(500)
        results["data"] = edges_df_sorted
        st.session_state.progress_text = "å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ ã‚°ãƒ©ãƒ•æç”»ä¸­..."
        results["image_base64"] = generate_graph_image(
            df=edges_df_sorted,
            plot_type='network',
            title="å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ (ä¸Šä½100ã‚¨ãƒƒã‚¸)"
        )
        results["summary"] = f"'{text_col}' ã«å¯¾ã™ã‚‹å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆ†æã‚’å®Ÿè¡Œã€‚{len(G.nodes())}ãƒãƒ¼ãƒ‰, {len(G.edges())}ã‚¨ãƒƒã‚¸ã‚’æ¤œå‡ºã€‚ã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆã—ã¾ã—ãŸã€‚"
        return results
    except Exception as e:
        logger.error(f"run_cooccurrence_network error: {e}", exc_info=True)
        results["summary"] = f"ã‚¨ãƒ©ãƒ¼: {e}"
        return results

def run_topic_category_summary(df: pd.DataFrame, suggestion: Dict[str, Any]) -> Dict[str, Any]:
    """(Step B) è©±é¡Œã‚«ãƒ†ã‚´ãƒªåˆ¥ã«æŠ•ç¨¿æ•°ã€ã‚µãƒãƒª(AI)ã€ä¸Šä½ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’åˆ†æã™ã‚‹"""
    logger.info("run_topic_category_summary å®Ÿè¡Œ...")
    results = {"data": pd.DataFrame(), "image_base64": None, "summary": ""}
    
    # (â˜…) UIã§ç·¨é›†ã•ã‚ŒãŸåˆ—ã‚’å–å¾—
    topic_col = suggestion.get('suitable_cols', ['è©±é¡Œã‚«ãƒ†ã‚´ãƒª'])[0]
    
    target_categories = ['ã‚°ãƒ«ãƒ¡', 'è‡ªç„¶', 'æ­´å²ãƒ»æ–‡åŒ–', 'ã‚¢ãƒ¼ãƒˆ', 'ã‚¤ãƒ™ãƒ³ãƒˆ', 'å®¿æ³Šãƒ»æ¸©æ³‰']
    if topic_col not in df.columns:
        msg = f"åˆ— '{topic_col}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚StepAã§ã€Œè©±é¡Œã‚«ãƒ†ã‚´ãƒªã€ãŒç”Ÿæˆã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
        logger.warning(f"run_topic_category_summary: {msg}")
        return {"data": pd.DataFrame([{"error": msg}]), "image_base64": None, "summary": msg}
    
    # (â˜…) --- 4. TOPã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ãƒ­ã‚¸ãƒƒã‚¯ä¿®æ­£ (åœ°åŸŸåã‚’é™¤å¤–) ---
    flag_cols = [col for col in df.columns if col.endswith('ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰')]
    cols_to_use = [col for col in flag_cols if col != 'å¸‚åŒºç”ºæ‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰']
    logger.info(f"TOPã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é›†è¨ˆå¯¾è±¡ (åœ°åŸŸåé™¤å¤–): {cols_to_use}")
    # (â˜…) --- ã“ã“ã¾ã§ãŒä¿®æ­£ç‚¹ ---
    
    results_list = []
    
    total_cats = len(target_categories)
    if 'progress_text' not in st.session_state:
            st.session_state.progress_text = ""
            
    for i, category in enumerate(target_categories):
        st.session_state.progress_text = f"è©±é¡Œã‚«ãƒ†ã‚´ãƒªåˆ†æä¸­ ({i+1}/{total_cats}): {category}"
        
        df_filtered = df[df[topic_col].astype(str).str.contains(category, na=False)]
        post_count = len(df_filtered)
        
        if post_count == 0:
            results_list.append({
                "category": category,
                "post_count": 0,
                "summary_ai": "N/A (æŠ•ç¨¿ãªã—)",
                "top_keywords": []
            })
            continue
        
        text_samples = df_filtered['ANALYSIS_TEXT_COLUMN'].dropna().sample(n=min(10, post_count), random_state=1).tolist()
        text_samples_str = "\n".join([f"- {text[:200]}..." for text in text_samples])
        
        ai_suggestion = {
            "description": f"ã€Œ{category}ã€ã‚«ãƒ†ã‚´ãƒªã«é–¢ã™ã‚‹ä»¥ä¸‹ã®æŠ•ç¨¿ã‚µãƒ³ãƒ—ãƒ«ã‚’èª­ã¿ã€ä¸»è¦ãªè©±é¡Œã‚’1ï½2æ–‡ã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚\nã‚µãƒ³ãƒ—ãƒ«:\n{text_samples_str}"
        }
        summary_ai = run_ai_summary_batch(df_filtered, ai_suggestion)
        
        # (â˜…) --- 4. ä¿®æ­£: ä¸Šä½ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ (Python) ---
        top_keywords = []
        if cols_to_use and not df_filtered.empty:
            all_keywords_series = []
            for kw_col in cols_to_use:
                if kw_col in df_filtered.columns:
                    s = df_filtered[kw_col].astype(str).str.split(', ').explode()
                    s = s[s.str.strip().isin(['', 'nan', 'None', 'N/A', 'è©²å½“ãªã—']) == False]
                    s = s.str.strip()
                    if not s.empty:
                        all_keywords_series.append(s)
            if all_keywords_series:
                combined_s = pd.concat(all_keywords_series)
                top_keywords = combined_s.value_counts().head(5).index.tolist()
        
        results_list.append({
            "category": category,
            "post_count": post_count,
            "summary_ai": summary_ai,
            "top_keywords": top_keywords
        })
        
        time.sleep(max(TAGGING_SLEEP_TIME / 2, 1.0))

    st.session_state.progress_text = "è©±é¡Œã‚«ãƒ†ã‚´ãƒªåˆ†æ å®Œäº†ã€‚"
    results_df = pd.DataFrame(results_list)
    
    image_base64 = generate_graph_image(
        df=results_df,
        plot_type='bar',
        x_col='category',
        y_col='post_count',
        title=f"ã€Œ{topic_col}ã€åˆ¥ æŠ•ç¨¿æ•°"
    )
    
    summary = f"ã€Œ{topic_col}ã€åˆ¥ã®åˆ†æã‚’å®Ÿè¡Œã€‚æŠ•ç¨¿æ•°ã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆã—ã¾ã—ãŸã€‚"
    return {"data": results_df, "image_base64": image_base64, "summary": summary}

def run_topic_engagement_top5(df: pd.DataFrame, suggestion: Dict[str, Any]) -> Dict[str, Any]:
    """(Step B) è©±é¡Œã‚«ãƒ†ã‚´ãƒªåˆ¥ã«ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆTOP5æŠ•ç¨¿ã¨æ¦‚è¦(AI)ã‚’åˆ†æã™ã‚‹"""
    logger.info("run_topic_engagement_top5 å®Ÿè¡Œ...")
    results = {"data": pd.DataFrame(), "image_base64": None, "summary": ""}

    # (â˜…) UIã§ç·¨é›†ã•ã‚ŒãŸåˆ—ã‚’å–å¾—
    cols_dict = suggestion.get('suitable_cols', {})
    if not isinstance(cols_dict, dict) or 'topic' not in cols_dict or 'text' not in cols_dict or 'engagement' not in cols_dict:
        msg = "åˆ—æƒ…å ±ï¼ˆtopic, text, engagementï¼‰ãŒä¸ååˆ†ã§ã™ã€‚"
        logger.warning(f"run_topic_engagement_top5: {msg}")
        return {"data": pd.DataFrame([{"error": msg}]), "image_base64": None, "summary": msg}

    topic_col = cols_dict['topic'][0]
    text_col = cols_dict['text'][0]
    engagement_col = cols_dict['engagement'][0]
    target_categories = ['ã‚°ãƒ«ãƒ¡', 'è‡ªç„¶', 'æ­´å²ãƒ»æ–‡åŒ–', 'ã‚¢ãƒ¼ãƒˆ', 'ã‚¤ãƒ™ãƒ³ãƒˆ', 'å®¿æ³Šãƒ»æ¸©æ³‰'] # (â˜…) ã“ã‚Œã¯å›ºå®š

    if topic_col not in df.columns:
        msg = f"è©±é¡Œã‚«ãƒ†ã‚´ãƒªåˆ— '{topic_col}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
        return {"data": pd.DataFrame([{"error": msg}]), "image_base64": None, "summary": msg}
    if engagement_col not in df.columns or not pd.api.types.is_numeric_dtype(df[engagement_col]):
        msg = f"ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆåˆ— '{engagement_col}' ãŒæ•°å€¤åˆ—ã¨ã—ã¦å­˜åœ¨ã—ã¾ã›ã‚“ã€‚"
        return {"data": pd.DataFrame([{"error": msg}]), "image_base64": None, "summary": msg}
    if text_col not in df.columns:
        msg = f"ãƒ†ã‚­ã‚¹ãƒˆåˆ— '{text_col}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
        return {"data": pd.DataFrame([{"error": msg}]), "image_base64": None, "summary": msg}


    # (â˜…) --- 5. ãƒ¡ãƒ‡ã‚£ã‚¢ãƒªãƒ³ã‚¯åˆ—ã‚’ç‰¹å®š ---
    link_col_candidates = ['link', 'url', 'media_url', 'æŠ•ç¨¿URL', 'URL', 'Link', 'Url']
    df_cols_lower = {col.lower(): col for col in df.columns}
    found_link_col = None
    for cand in link_col_candidates:
        if cand in df_cols_lower:
            found_link_col = df_cols_lower[cand]
            break
    if found_link_col:
        logger.info(f"ãƒ¡ãƒ‡ã‚£ã‚¢ãƒªãƒ³ã‚¯åˆ—: '{found_link_col}' ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
    else:
        logger.warning(f"ãƒ¡ãƒ‡ã‚£ã‚¢ãƒªãƒ³ã‚¯åˆ— ({link_col_candidates}) ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
    # (â˜…) --- ã“ã“ã¾ã§ãŒä¿®æ­£ç‚¹ ---

    results_list = []
    
    total_cats = len(target_categories)
    if 'progress_text' not in st.session_state:
            st.session_state.progress_text = ""

    for i, category in enumerate(target_categories):
        st.session_state.progress_text = f"ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆTOP5åˆ†æä¸­ ({i+1}/{total_cats}): {category}"
        
        df_filtered = df[df[topic_col].astype(str).str.contains(category, na=False)]
        post_count = len(df_filtered)
        
        if post_count == 0:
            continue
            
        df_top5 = df_filtered.nlargest(5, engagement_col, keep='first')
        top5_posts_data = []
        
        if df_top5.empty:
                results_list.append({
                "category": category,
                "post_count": post_count,
                "top_posts": []
            })
                continue

        for _, row in df_top5.iterrows():
            post_text = str(row[text_col])
            engagement_value = row[engagement_col]
            
            ai_suggestion = {
                "description": f"ä»¥ä¸‹ã®æŠ•ç¨¿ãƒ†ã‚­ã‚¹ãƒˆã‚’èª­ã¿ã€å†…å®¹ã‚’1æ–‡ã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚\nãƒ†ã‚­ã‚¹ãƒˆ: {post_text[:500]}..."
            }
            summary_ai = run_ai_summary_batch(df_filtered, ai_suggestion)
            
            # (â˜…) --- 5. ãƒ¡ãƒ‡ã‚£ã‚¢ãƒªãƒ³ã‚¯ã‚’å–å¾— ---
            link_value = None
            if found_link_col and found_link_col in row and pd.notna(row[found_link_col]):
                link_value = str(row[found_link_col])
            
            top5_posts_data.append({
                "engagement": int(engagement_value),
                "summary_ai": summary_ai,
                "original_text_snippet": post_text[:100],
                "media_link": link_value # (â˜…) è¿½åŠ 
            })
            
            time.sleep(max(TAGGING_SLEEP_TIME / 2, 1.0))

        results_list.append({
            "category": category,
            "post_count": post_count,
            "top_posts": top5_posts_data
        })

    st.session_state.progress_text = "ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆTOP5åˆ†æ å®Œäº†ã€‚"
    results_df = pd.DataFrame(results_list)
    
    summary = f"ã€Œ{topic_col}ã€åˆ¥ã®é«˜ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆæŠ•ç¨¿TOP5ã‚’æŠ½å‡ºã—ã¾ã—ãŸã€‚"
    return {"data": results_df, "image_base64": None, "summary": summary}


# --- 8.2. (â˜…) Step B: AIåˆ†æãƒ˜ãƒ«ãƒ‘ãƒ¼ (å¤‰æ›´ãªã—) ---
# ... (run_ai_summary_batch ã¯å¤‰æ›´ãªã—) ...


# --- 8.3. (â˜…) Step B: åˆ†æå®Ÿè¡Œãƒ«ãƒ¼ã‚¿ãƒ¼ (å¤‰æ›´ãªã—) ---
def execute_analysis(
    analysis_name: str,
    df: pd.DataFrame,
    suggestion: Dict[str, Any]
) -> Dict[str, Any]:
    """
    (Step B) åˆ†æåã«åŸºã¥ãã€é©åˆ‡ãªPythonã¾ãŸã¯AIã®å®Ÿè¡Œé–¢æ•°ã‚’å‘¼ã³å‡ºã™ãƒ«ãƒ¼ã‚¿ãƒ¼
    """
    try:
        analysis_type = suggestion.get('type', 'python')
        
        if analysis_type == 'python':
            if analysis_name == "å…¨ä½“ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹":
                return run_overall_metrics(df, suggestion)
            # (â˜…) 3. "å˜ç´”é›†è¨ˆ: {col}" ã®ã‚ˆã†ãªå‹•çš„ãªåå‰ã«å¯¾å¿œ
            elif analysis_name.startswith("å˜ç´”é›†è¨ˆ:"):
                return run_simple_count(df, suggestion)
            elif analysis_name in ["ã‚¯ãƒ­ã‚¹é›†è¨ˆï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é–“ï¼‰", "ã‚¯ãƒ­ã‚¹é›†è¨ˆï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰Ã—å±æ€§ï¼‰", "è©±é¡Œã‚«ãƒ†ã‚´ãƒªåˆ¥ è¦³å…‰åœ°TOP10"]:
                return run_crosstab(df, suggestion)
            elif analysis_name == "æ™‚ç³»åˆ—ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æ":
                return run_timeseries(df, suggestion)
            elif analysis_name == "ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°ï¼ˆé »å‡ºå˜èªï¼‰":
                return run_text_mining(df, suggestion)
            elif analysis_name == "å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯":
                return run_cooccurrence_network(df, suggestion)
            elif analysis_name == "è©±é¡Œã‚«ãƒ†ã‚´ãƒªåˆ¥ æŠ•ç¨¿æ•°ã¨ã‚µãƒãƒª":
                return run_topic_category_summary(df, suggestion)
            elif analysis_name == "è©±é¡Œã‚«ãƒ†ã‚´ãƒªåˆ¥ ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆTOP5ã¨æ¦‚è¦":
                return run_topic_engagement_top5(df, suggestion)
            else:
                logger.warning(f"Pythonåˆ†æ '{analysis_name}' ã®å®Ÿè¡Œãƒ­ã‚¸ãƒƒã‚¯ãŒå®šç¾©ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚AIåˆ†æã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™ã€‚")
                suggestion['description'] = f"ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«ã‚’ä½¿ã„ã€'{analysis_name}' ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚"
                ai_result_str = run_ai_summary_batch(df, suggestion)
                return {"data": ai_result_str, "image_base64": None, "summary": ai_result_str[:100] + "..."}
        
        elif analysis_type == 'ai':
            ai_result_str = run_ai_summary_batch(df, suggestion)
            return {"data": ai_result_str, "image_base64": None, "summary": ai_result_str[:100] + "..."}
            
        else:
            err_msg = f"ä¸æ˜ãªåˆ†æã‚¿ã‚¤ãƒ— ('{analysis_type}') ã§ã™: {analysis_name}"
            return {"data": err_msg, "image_base64": None, "summary": err_msg}
            
    except Exception as e:
        logger.error(f"execute_analysis ('{analysis_name}') å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        err_msg = f"åˆ†æ '{analysis_name}' ã®å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"
        return {"data": err_msg, "image_base64": None, "summary": err_msg}

# --- 8.4. (â˜…) Step B: JSONå‡ºåŠ›ãƒ˜ãƒ«ãƒ‘ãƒ¼ (å¤‰æ›´ãªã—) ---
# ... (convert_results_to_json_string ã¯å¤‰æ›´ãªã—) ...


# --- 8.5. (â˜…) Step B: UIæç”»é–¢æ•° (â˜… æ–°ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼) ---

def render_step_b():
    """(Step B) åˆ†ææ‰‹æ³•ã®ææ¡ˆãƒ»å®Ÿè¡Œãƒ»ãƒ‡ãƒ¼ã‚¿å‡ºåŠ›UIã‚’æç”»ã™ã‚‹"""
    st.title("ğŸ“Š Step B: åˆ†æã®å®Ÿè¡Œã¨ãƒ‡ãƒ¼ã‚¿å‡ºåŠ›")

    # (â˜…) --- ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã®åˆæœŸåŒ– ---
    if 'df_flagged_B' not in st.session_state:
        st.session_state.df_flagged_B = pd.DataFrame()
    if 'suggestions_B' not in st.session_state:
        # (â˜…) ææ¡ˆã‚’ã€Œè¾æ›¸ã€ã§æŒã¤ (task_name -> details)
        st.session_state.suggestions_B = {}
    if 'step_b_results' not in st.session_state:
        # (â˜…) ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ç”¨ã®çµæœ (task_name -> result_data)
        st.session_state.step_b_results = {}
    if 'step_b_json_output' not in st.session_state:
        st.session_state.step_b_json_output = None
    if 'progress_text' not in st.session_state:
         st.session_state.progress_text = ""
    if 'tips_list' not in st.session_state:
        st.session_state.tips_list = []
    if 'current_tip_index' not in st.session_state:
        st.session_state.current_tip_index = 0
    if 'last_tip_time' not in st.session_state:
        st.session_state.last_tip_time = time.time()
    # (â˜…) --- ã“ã“ã¾ã§ãŒä¿®æ­£ç‚¹ ---

    # --- 1. ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ ---
    st.header("Step 1: ã‚­ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ¸ˆã¿CSVã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
    st.info(f"Step A ã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã—ãŸ CSV (Curated_Data.csv) ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
    uploaded_flagged_file = st.file_uploader(
        "ãƒ•ãƒ©ã‚°ä»˜ã‘æ¸ˆã¿CSVãƒ•ã‚¡ã‚¤ãƒ«",
        type=['csv'],
        key="step_b_uploader"
    )

    if uploaded_flagged_file:
        try:
            df, err = read_file(uploaded_flagged_file)
            if err:
                st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {err}")
                return
            st.session_state.df_flagged_B = df
            
            # (â˜…) æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸã‚‰ã€å¤ã„ææ¡ˆã¨çµæœã‚’ã‚¯ãƒªã‚¢
            st.session_state.suggestions_B = {}
            st.session_state.step_b_results = {}
            st.session_state.step_b_json_output = None
            
            st.success(f"ãƒ•ã‚¡ã‚¤ãƒ«ã€Œ{uploaded_flagged_file.name}ã€èª­è¾¼å®Œäº† ({len(df)}è¡Œ)")
            with st.expander("ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ (å…ˆé ­5è¡Œ)"):
                st.dataframe(df.head())
        except Exception as e:
            logger.error(f"Step B ãƒ•ã‚¡ã‚¤ãƒ«èª­è¾¼ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            return
    else:
        st.warning("åˆ†æã‚’ç¶šã‘ã‚‹ã«ã¯ã€Step A ã§ç”Ÿæˆã—ãŸCSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
        return

    df_B = st.session_state.df_flagged_B
    
    # (â˜…) --- DFã®åˆ—æƒ…å ±ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ (Selectboxç”¨) ---
    all_cols = list(df_B.columns)
    text_cols = ['ANALYSIS_TEXT_COLUMN'] + [col for col in all_cols if 'text' in col.lower() or 'content' in col.lower()]
    keyword_cols = [col for col in all_cols if col.endswith('ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰')]
    date_cols = [col for col in all_cols if col in df_B.select_dtypes(include='datetime64').columns or 'date' in col.lower()]
    numeric_cols = [col for col in all_cols if pd.api.types.is_numeric_dtype(df_B[col])]
    engagement_cols = [col for col in numeric_cols if any(c in col.lower() for c in ['ã„ã„ã­', 'like', 'ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ', 'engagement'])]

    # --- 2. åˆ†ææ‰‹æ³•ã®ææ¡ˆã¨ä¸€æ‹¬å®Ÿè¡Œ --- (â˜…) ã‚¿ã‚¤ãƒˆãƒ«å¤‰æ›´
    st.header("Step 2: åˆ†ææ‰‹æ³•ã®ææ¡ˆã¨ä¸€æ‹¬å®Ÿè¡Œ")
    st.markdown(f"ï¼ˆ(â˜…) AIææ¡ˆãƒ¢ãƒ‡ãƒ«: `{MODEL_FLASH_LITE}`ï¼‰")
    
    analysis_prompt_B = st.text_area(
        "ï¼ˆä»»æ„ï¼‰AIã«è¿½åŠ ã§æŒ‡ç¤ºã—ãŸã„åˆ†æã‚¿ã‚¹ã‚¯ã‚’å…¥åŠ›:",
        placeholder="ä¾‹: åºƒå³¶å¸‚ã¨è¦³å…‰åœ°ã®ç›¸é–¢é–¢ä¿‚ã‚’æ·±æ˜ã‚Šã—ãŸã„ã€‚\nä¾‹: ãƒã‚¸ãƒ†ã‚£ãƒ–ãªæ„è¦‹ã¨ãƒã‚¬ãƒ†ã‚£ãƒ–ãªæ„è¦‹ã®å…·ä½“ä¾‹ã‚’3ã¤ãšã¤æŠ½å‡ºã—ã¦ã€‚",
        key="step_b_prompt"
    )

    if st.button("ğŸ’¡ åˆ†ææ‰‹æ³•ã‚’ææ¡ˆã•ã›ã‚‹ (Step 2)", key="suggest_button_B", type="primary"):
        if not st.session_state.tips_list:
            with st.spinner("åˆ†æTIPSã‚’AIã§ç”Ÿæˆä¸­..."):
                st.session_state.tips_list = get_analysis_tips_list_from_ai()
                if st.session_state.tips_list:
                    st.session_state.current_tip_index = random.randint(0, len(st.session_state.tips_list) - 1)
                    st.session_state.last_tip_time = time.time()
            
        with st.spinner(f"ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã¨æŒ‡ç¤ºå†…å®¹ã‚’åˆ†æã—ã€æ‰‹æ³•ã‚’ææ¡ˆä¸­ ({MODEL_FLASH_LITE})..."):
            st.session_state.step_b_results = {} # (â˜…) ææ¡ˆã®ãŸã³ã«çµæœã‚’ãƒªã‚»ãƒƒãƒˆ
            st.session_state.step_b_json_output = None # (â˜…) å‡ºåŠ›ã‚‚ãƒªã‚»ãƒƒãƒˆ
            
            # (â˜…) å …ç‰¢åŒ–ã•ã‚ŒãŸé–¢æ•° (ä¸Šè¨˜ 1. ã§ä¿®æ­£) ãŒå‘¼ã°ã‚Œã‚‹
            base_suggestions = suggest_analysis_techniques_py(df_B)
            ai_suggestions = []
            if analysis_prompt_B.strip():
                ai_suggestions = suggest_analysis_techniques_ai(
                    analysis_prompt_B, df_B, base_suggestions
                )
            base_names = {s['name'] for s in base_suggestions}
            filtered_ai_suggestions = [s for s in ai_suggestions if s['name'] not in base_names]
            all_suggestions = sorted(base_suggestions + filtered_ai_suggestions, key=lambda x: x['priority'])
            
            # (â˜…) --- BEGIN FIX (Loop Prevention) ---
            if not all_suggestions:
                # (â˜…) ææ¡ˆãŒ0ä»¶ã ã£ãŸå ´åˆã®å‡¦ç†
                st.session_state.suggestions_B = {}
                st.warning(
                    "åˆ†ææ‰‹æ³•ã®ææ¡ˆãŒ 0ä»¶ ã§ã—ãŸã€‚\n"
                    "ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸCSVãƒ•ã‚¡ã‚¤ãƒ«ã«ã€åˆ†æå¯èƒ½ãªåˆ—ï¼ˆ`...ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰`ã§çµ‚ã‚ã‚‹åˆ—ã‚„ã€`ANALYSIS_TEXT_COLUMN`ãªã©ï¼‰ãŒ"
                    "æ­£ã—ãå«ã¾ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
                )
                # (â˜…) st.rerun() ã‚’ *ã—ãªã„* ã§ã€ã“ã®ã¾ã¾ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ç¶šè¡Œã•ã›ã‚‹
            else:
                # (â˜…) ææ¡ˆãŒ1ä»¶ä»¥ä¸Šã‚ã£ãŸå ´åˆã®å‡¦ç†
                st.session_state.suggestions_B = {s['name']: s for s in all_suggestions}
                st.success(f"åˆ†ææ‰‹æ³•ã®ææ¡ˆãŒå®Œäº†ã—ã¾ã—ãŸ ({len(all_suggestions)}ä»¶)ã€‚Step 2.5 ã§ä¸€æ‹¬å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
                st.rerun()

    # (â˜…) --- NEW Step 2.5: Bulk Execution ---
    # (â˜…) ææ¡ˆ (suggestions_B) ãŒå­˜åœ¨ã™ã‚‹å ´åˆã«ã®ã¿è¡¨ç¤º
    if st.session_state.suggestions_B:
        st.markdown("---")
        st.info("ä»¥ä¸‹ã®ãƒœã‚¿ãƒ³ã§ã€ææ¡ˆã•ã‚ŒãŸã™ã¹ã¦ã®åˆ†æã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ä¸€æ‹¬å®Ÿè¡Œã§ãã¾ã™ã€‚")
        
        if st.button("ğŸƒ å…¨åˆ†æã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ä¸€æ‹¬å®Ÿè¡Œ (Step 2.5)", type="primary", use_container_width=True):
            st.session_state.progress_text = "ä¸€æ‹¬å®Ÿè¡Œã‚’é–‹å§‹ã—ã¾ã™..."
            total_tasks = len(st.session_state.suggestions_B)
            progress_bar = st.progress(0.0, text="ä¸€æ‹¬å®Ÿè¡Œ å¾…æ©Ÿä¸­...")
            
            # (â˜…) Tipsè¡¨ç¤º
            tip_placeholder_b_bulk = st.empty()
            if st.session_state.tips_list:
                try:
                    current_tip = st.session_state.tips_list[st.session_state.current_tip_index]
                    tip_placeholder_b_bulk.info(f"ğŸ’¡ ãƒ‡ãƒ¼ã‚¿åˆ†æTIPS: {current_tip}")
                except IndexError:
                    st.session_state.current_tip_index = 0
            
            with st.spinner(f"å…¨ {total_tasks} ä»¶ã®åˆ†æã‚’ä¸€æ‹¬å®Ÿè¡Œä¸­..."):
                for i, (task_name, suggestion_details) in enumerate(st.session_state.suggestions_B.items()):
                    st.session_state.progress_text = f"({i+1}/{total_tasks}) ã€Œ{task_name}ã€ã‚’å®Ÿè¡Œä¸­..."
                    progress_bar.progress((i+1)/total_tasks, text=f"å®Ÿè¡Œä¸­: {task_name}")
                    
                    try:
                        result_data = execute_analysis(task_name, df_B, suggestion_details)
                        st.session_state.step_b_results[task_name] = result_data
                    except Exception as e:
                        logger.error(f"ä¸€æ‹¬å®Ÿè¡Œã‚¨ãƒ©ãƒ¼ ({task_name}): {e}", exc_info=True)
                        st.session_state.step_b_results[task_name] = {
                            "data": f"ä¸€æ‹¬å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}",
                            "image_base64": None,
                            "summary": f"ã‚¨ãƒ©ãƒ¼: {e}"
                        }
                
                st.session_state.progress_text = "å…¨åˆ†æã®ä¸€æ‹¬å®Ÿè¡ŒãŒå®Œäº†ã—ã¾ã—ãŸã€‚"
                progress_bar.progress(1.0, text="ä¸€æ‹¬å®Ÿè¡Œ å®Œäº†")
                tip_placeholder_b_bulk.empty()
                st.success("å…¨åˆ†æã®ä¸€æ‹¬å®Ÿè¡ŒãŒå®Œäº†ã—ã¾ã—ãŸã€‚Step 3 ã§çµæœã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
                st.rerun() # (â˜…) ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’æ›´æ–°ã™ã‚‹ãŸã‚ã«ãƒªãƒ©ãƒ³


    # --- 3. åˆ†æã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä¿®æ­£ ---
    if not st.session_state.suggestions_B:
        st.info("Step 2 ã§ã€Œåˆ†ææ‰‹æ³•ã‚’ææ¡ˆã•ã›ã‚‹ã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")
        return

    st.header("Step 3: åˆ†æã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä¿®æ­£")
    st.info("å„åˆ†æé …ç›®ã®ã€Œâ–¼ã€ã‚’é–‹ãã€ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆåˆ†æå¯¾è±¡ã®åˆ—ãªã©ï¼‰ã‚’ä¿®æ­£ã—ã¦ã€å€‹åˆ¥ã«ã€Œå†å®Ÿè¡Œ/æ›´æ–°ã€ã‚‚å¯èƒ½ã§ã™ã€‚")
    
    # (â˜…) Tipsè¡¨ç¤º
    tip_placeholder = st.empty()
    if st.session_state.tips_list:
        try:
            current_tip = st.session_state.tips_list[st.session_state.current_tip_index]
            tip_placeholder.info(f"ğŸ’¡ ãƒ‡ãƒ¼ã‚¿åˆ†æTIPS: {current_tip}")
        except IndexError:
            st.session_state.current_tip_index = 0

    # (â˜…) UIé€²æ—è¡¨ç¤º
    progress_text_placeholder = st.empty()
    if st.session_state.progress_text:
         progress_text_placeholder.info(st.session_state.progress_text)
         
    # (â˜…) ææ¡ˆã•ã‚ŒãŸã‚¿ã‚¹ã‚¯ã‚’ãƒ«ãƒ¼ãƒ—å‡¦ç†
    for task_name, suggestion_details_from_session in st.session_state.suggestions_B.items():
        
        # (â˜…) çŠ¶æ…‹ãŒãƒªã‚»ãƒƒãƒˆã•ã‚Œãªã„ã‚ˆã†ã€ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®è¾æ›¸ã‚’ç›´æ¥æ“ä½œã›ãšã€
        # (â˜…) æç”»ãƒ«ãƒ¼ãƒ—ç”¨ã®ãƒ­ãƒ¼ã‚«ãƒ«ã‚³ãƒ”ãƒ¼ã‚’ä½œæˆã™ã‚‹
        suggestion_details = suggestion_details_from_session.copy()
        
        st.markdown("---")
        
        # (â˜…) å„ã‚¿ã‚¹ã‚¯ã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚¨ãƒªã‚¢ (Expander ã® *å¤–*)
        # (â˜…) å®Ÿè¡ŒçµæœãŒã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ã‚ã‚Œã°è¡¨ç¤º
        if task_name in st.session_state.step_b_results:
            result = st.session_state.step_b_results[task_name]
            st.subheader(f"âœ… ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼: {task_name}")
            
            # (â˜…) [object Object] å¯¾ç­–: TOP5ã®è¾æ›¸ãƒªã‚¹ãƒˆã‚’æ­£ã—ãè¡¨ç¤º
            if task_name == "è©±é¡Œã‚«ãƒ†ã‚´ãƒªåˆ¥ ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆTOP5ã¨æ¦‚è¦" and isinstance(result['data'], pd.DataFrame):
                st.dataframe(result['data'])
                for _, row in result['data'].iterrows():
                    st.markdown(f"**ã‚«ãƒ†ã‚´ãƒª: {row['category']}** (æŠ•ç¨¿æ•°: {row['post_count']})")
                    if row['top_posts']:
                         for post in row['top_posts']:
                             st.markdown(f"  - **EG: {post['engagement']}** - {post['summary_ai']}")
                             if post['media_link']:
                                 st.markdown(f"    [Link]({post['media_link']})")
                    st.markdown("---")
            
            # (â˜…) ãã®ä»–ã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
            elif result['image_base64']:
                st.image(base64.b64decode(result['image_base64']))
            
            if isinstance(result['data'], pd.DataFrame) and task_name != "è©±é¡Œã‚«ãƒ†ã‚´ãƒªåˆ¥ ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆTOP5ã¨æ¦‚è¦":
                st.dataframe(result['data'].head(10))
            elif isinstance(result['data'], dict):
                st.json(result['data'])
            elif isinstance(result['data'], str):
                st.markdown(result['data'])
                
            st.caption(f"ã‚µãƒãƒª: {result.get('summary', 'N/A')}")
        else:
            st.subheader(f"â¬œï¸ æœªå®Ÿè¡Œ: {task_name}")

        
        # (â˜…) å„ã‚¿ã‚¹ã‚¯ã®ç·¨é›†ãƒ»å®Ÿè¡Œã‚¨ãƒªã‚¢
        with st.expander(f"ã€Œ{task_name}ã€ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä¿®æ­£ãƒ»å†å®Ÿè¡Œ"):
            
            st.markdown(f"**èª¬æ˜:** {suggestion_details['description']}")
            st.markdown("##### (ã‚ªãƒ—ã‚·ãƒ§ãƒ³) ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å¤‰æ›´")
            
            # (â˜…) ã‚¿ã‚¹ã‚¯ã”ã¨ã«ç·¨é›†UIã‚’å‹•çš„ã«ç”Ÿæˆ
            try:
                # (â˜…) 3. å˜ç´”é›†è¨ˆ
                if task_name.startswith("å˜ç´”é›†è¨ˆ:"):
                    default_col = suggestion_details['suitable_cols'][0]
                    new_col = st.selectbox(f"é›†è¨ˆå¯¾è±¡ã®åˆ— ({task_name})", options=keyword_cols, index=keyword_cols.index(default_col) if default_col in keyword_cols else 0, key=f"sel_{task_name}")
                    suggestion_details['suitable_cols'] = [new_col] # (â˜…) ãƒ­ãƒ¼ã‚«ãƒ«ã‚³ãƒ”ãƒ¼ã‚’æ›´æ–°
                
                # (â˜…) ã‚¯ãƒ­ã‚¹é›†è¨ˆ
                elif task_name.startswith("ã‚¯ãƒ­ã‚¹é›†è¨ˆ"):
                    default_col1 = suggestion_details['suitable_cols'][0]
                    default_col2 = suggestion_details['suitable_cols'][1]
                    c1, c2 = st.columns(2)
                    new_col1 = c1.selectbox(f"åˆ— 1 ({task_name})", options=all_cols, index=all_cols.index(default_col1) if default_col1 in all_cols else 0, key=f"sel_{task_name}_1")
                    new_col2 = c2.selectbox(f"åˆ— 2 ({task_name})", options=all_cols, index=all_cols.index(default_col2) if default_col2 in all_cols else 1, key=f"sel_{task_name}_2")
                    suggestion_details['suitable_cols'] = [new_col1, new_col2] # (â˜…) ãƒ­ãƒ¼ã‚«ãƒ«ã‚³ãƒ”ãƒ¼ã‚’æ›´æ–°

                # (â˜…) æ™‚ç³»åˆ—
                elif task_name == "æ™‚ç³»åˆ—ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æ":
                    default_dt = suggestion_details['suitable_cols']['datetime'][0]
                    default_kw = suggestion_details['suitable_cols']['keywords'][0]
                    c1, c2 = st.columns(2)
                    new_dt = c1.selectbox(f"æ—¥æ™‚åˆ— ({task_name})", options=date_cols, index=date_cols.index(default_dt) if default_dt in date_cols else 0, key=f"sel_{task_name}_dt")
                    new_kw = c2.selectbox(f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ— ({task_name})", options=keyword_cols, index=keyword_cols.index(default_kw) if default_kw in keyword_cols else 0, key=f"sel_{task_name}_kw")
                    suggestion_details['suitable_cols'] = {"datetime": [new_dt], "keywords": [new_kw]} # (â˜…) ãƒ­ãƒ¼ã‚«ãƒ«ã‚³ãƒ”ãƒ¼ã‚’æ›´æ–°
                
                # (â˜…) ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚° / å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
                elif task_name in ["ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°ï¼ˆé »å‡ºå˜èªï¼‰", "å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯"]:
                    default_col = suggestion_details['suitable_cols'][0]
                    new_col = st.selectbox(f"ãƒ†ã‚­ã‚¹ãƒˆåˆ— ({task_name})", options=text_cols, index=text_cols.index(default_col) if default_col in text_cols else 0, key=f"sel_{task_name}_txt")
                    suggestion_details['suitable_cols'] = [new_col] # (â˜…) ãƒ­ãƒ¼ã‚«ãƒ«ã‚³ãƒ”ãƒ¼ã‚’æ›´æ–°
                
                # (â˜…) 5. ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆTOP5
                elif task_name == "è©±é¡Œã‚«ãƒ†ã‚´ãƒªåˆ¥ ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆTOP5ã¨æ¦‚è¦":
                    defaults = suggestion_details['suitable_cols']
                    c1, c2 = st.columns(2)
                    new_topic = c1.selectbox(f"è©±é¡Œã‚«ãƒ†ã‚´ãƒªåˆ— ({task_name})", options=keyword_cols, index=keyword_cols.index(defaults['topic'][0]) if defaults['topic'][0] in keyword_cols else 0, key=f"sel_{task_name}_top")
                    new_eng = c2.selectbox(f"ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆåˆ— ({task_name})", options=engagement_cols, index=engagement_cols.index(defaults['engagement'][0]) if defaults['engagement'][0] in engagement_cols else 0, key=f"sel_{task_name}_eng")
                    # (â˜…) text_col ã¯å¤‰æ›´ã—ãªã„
                    suggestion_details['suitable_cols'] = {'topic': [new_topic], 'text': defaults['text'], 'engagement': [new_eng]} # (â˜…) ãƒ­ãƒ¼ã‚«ãƒ«ã‚³ãƒ”ãƒ¼ã‚’æ›´æ–°

            except Exception as e:
                st.error(f"ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿UIã®æç”»ã«å¤±æ•—: {e}")
                logger.error(f"ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿UIæç”»ã‚¨ãƒ©ãƒ¼ ({task_name}): {e}", exc_info=True)


            # (â˜…) å€‹åˆ¥å®Ÿè¡Œãƒœã‚¿ãƒ³
            if st.button(f"ã€Œ{task_name}ã€ã‚’å†å®Ÿè¡Œ/æ›´æ–°", key=f"run_{task_name}"): # (â˜…) æ–‡è¨€å¤‰æ›´
                st.session_state.progress_text = f"ã€Œ{task_name}ã€ã‚’å€‹åˆ¥ã«å®Ÿè¡Œä¸­..."
                with st.spinner(f"ã€Œ{task_name}ã€ã‚’å®Ÿè¡Œä¸­..."):
                    try:
                        # (â˜…) ç·¨é›†ã•ã‚ŒãŸãƒ­ãƒ¼ã‚«ãƒ«ã‚³ãƒ”ãƒ¼ (suggestion_details) ã‚’æ¸¡ã™
                        result_data = execute_analysis(task_name, df_B, suggestion_details)
                        # (â˜…) å®Ÿè¡Œçµæœã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã«ä¿å­˜
                        st.session_state.step_b_results[task_name] = result_data
                        
                        # (â˜…) !!! é‡è¦: ä¿®æ­£ã—ãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã«æ›¸ãæˆ»ã™ !!!
                        st.session_state.suggestions_B[task_name] = suggestion_details
                        
                        st.session_state.progress_text = f"ã€Œ{task_name}ã€ã®å®Ÿè¡ŒãŒå®Œäº†ã—ã¾ã—ãŸã€‚"
                        st.rerun() # (â˜…) UIã‚’å³æ™‚æ›´æ–°ã—ã¦ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’è¡¨ç¤º
                    except Exception as e:
                         st.error(f"åˆ†æå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
                         logger.error(f"å€‹åˆ¥å®Ÿè¡Œã‚¨ãƒ©ãƒ¼ ({task_name}): {e}", exc_info=True)
                         st.session_state.progress_text = f"ã€Œ{task_name}ã€ã®å®Ÿè¡Œã«å¤±æ•—ã—ã¾ã—ãŸã€‚"


    # (â˜…) --- 4. (NEW) æœ€çµ‚ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ ---
    st.markdown("---")
    st.header("Step 4: æœ€çµ‚JSONã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ")
    
    # (â˜…) å®Ÿè¡ŒçŠ¶æ³ã®ã‚µãƒãƒªãƒ¼
    total_suggestions = len(st.session_state.suggestions_B)
    total_results = len(st.session_state.step_b_results)
    
    if total_results < total_suggestions:
        st.warning(f"ã¾ã ã™ã¹ã¦ã®åˆ†æãŒå®Ÿè¡Œã•ã‚Œã¦ã„ã¾ã›ã‚“ ({total_results} / {total_suggestions} ä»¶)ã€‚(Step 2.5 ã®ä¸€æ‹¬å®Ÿè¡Œã‚’æ¨å¥¨ã—ã¾ã™)")
    else:
        st.success(f"ã™ã¹ã¦ã®åˆ†æ ({total_results} / {total_suggestions} ä»¶) ãŒå®Ÿè¡Œã•ã‚Œã¾ã—ãŸã€‚JSONã‚’ç”Ÿæˆã§ãã¾ã™ã€‚")

    
    if st.button("StepCç”¨ JSONã‚’ç”Ÿæˆãƒ»ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ (Step 4)", type="primary", use_container_width=True):
        if total_results == 0:
            st.error("åˆ†æãŒ1ã¤ã‚‚å®Ÿè¡Œã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚Step 3 ã§å„åˆ†æã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        else:
            with st.spinner("æœ€çµ‚JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆä¸­..."):
                try:
                    # (â˜…) ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã•ã‚ŒãŸçµæœ (`step_b_results`) ã‚’JSONLã«å¤‰æ›
                    json_output_string = convert_results_to_json_string(st.session_state.step_b_results)
                    st.session_state.step_b_json_output = json_output_string
                    st.success("StepCç”¨ã®JSONãƒ‡ãƒ¼ã‚¿ãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸï¼")
                except Exception as e:
                    logger.error(f"Step B æœ€çµ‚JSONå‡ºåŠ›å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
                    st.error(f"åˆ†æçµæœã®JSONå¤‰æ›ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

    # (â˜…) --- 5. (NEW) ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚»ã‚¯ã‚·ãƒ§ãƒ³ ---
    if st.session_state.step_b_json_output:
        st.info(f"ä»¥ä¸‹ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ã¯ã€Step 3 ã§ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ»å®Ÿè¡Œã•ã‚ŒãŸ {len(st.session_state.step_b_results)} ä»¶ã®åˆ†æçµæœãŒã™ã¹ã¦å«ã¾ã‚Œã¦ã„ã¾ã™ã€‚")
        
        st.download_button(
            label="åˆ†æãƒ‡ãƒ¼ã‚¿ (analysis_data.json) ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            data=st.session_state.step_b_json_output,
            file_name="analysis_data.json",
            mime="application/json",
            type="primary",
            use_container_width=True
        )
        
        st.markdown("---")
        st.subheader("å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿ (JSONL) ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")
        
        preview_summaries = []
        try:
            for line in st.session_state.step_b_json_output.splitlines():
                line_data = json.loads(line)
                task_name = line_data.get("analysis_task")
                summary = line_data.get("summary", line_data.get("analysis_summaries", "No summary."))
                img_note = line_data.get("image_note", "No image.")
                
                if task_name == "OverallSummary":
                    preview_summaries.append(f"--- Overall Summary ---")
                    if isinstance(summary, dict):
                        for k, v in summary.items():
                             preview_summaries.append(f"  - {k}: {str(v)[:100]}...")
                    continue
                
                preview_summaries.append(f"[{task_name}] (Image: {img_note})")
                
        except Exception as e:
            preview_summaries = ["JSONãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®ç”Ÿæˆã«å¤±æ•—", str(e)]

        st.text_area(
            "JSONL (ã‚µãƒãƒªãƒ¼ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼):",
            value="\n".join(preview_summaries),
            height=300,
            key="json_preview_B_summary",
            disabled=True
        )
        st.success("ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€Step C (AIãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ) ã«é€²ã‚“ã§ãã ã•ã„ã€‚")
# --- 9. (â˜…) Step C: AIãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ (Proãƒ¢ãƒ‡ãƒ«) ---
# (è¦ä»¶: Step Cã¯ gemini-2.5-pro ã‚’ä½¿ç”¨)

def generate_step_c_prompt(jsonl_data_string: str) -> str:
    """
    (Step C) ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸ Step B ã® JSONL ãƒ‡ãƒ¼ã‚¿ã‚’ã€Œä¸‹èª­ã¿ã€ã—ã€
    gemini-2.5-pro ã¸ã®é«˜å“è³ªãªæŒ‡ç¤ºãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è‡ªå‹•ç”Ÿæˆã™ã‚‹ã€‚
    (â˜…) ã“ã®ã€Œä¸‹èª­ã¿ã€è‡ªä½“ã¯é«˜é€Ÿãª flash-lite ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹
    (â˜…) ã”è¦æœ›ã«åŸºã¥ãã€å“è³ªå‘ä¸Šã®ãŸã‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆãƒ­ã‚¸ãƒƒã‚¯ã‚’å¤§å¹…ã«å¼·åŒ–
    """
    logger.info("Step C ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè‡ªå‹•ç”Ÿæˆ (Flash Lite) å®Ÿè¡Œ...")
    
    llm = get_llm(model_name=MODEL_FLASH_LITE, temperature=0.1)
    if llm is None:
        logger.error("generate_step_c_prompt: LLM (Flash Lite) ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")
        return "# AIãƒ¢ãƒ‡ãƒ«(Flash Lite)ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"

    # (â˜…) JSONLã‹ã‚‰åˆ†æã‚¿ã‚¹ã‚¯åï¼ˆ"analysis_task"ï¼‰ã‚’æŠ½å‡º
    task_names = []
    summary_data = {}
    try:
        for line in jsonl_data_string.splitlines():
            try:
                task_data = json.loads(line)
                task_name = task_data.get("analysis_task")
                if task_name == "OverallSummary":
                    summary_data = task_data.get("data", {})
                    # analysis_summaries ã‹ã‚‰ã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆã‚’å–å¾— (ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯)
                    if not task_names and "analysis_summaries" in task_data:
                         task_names = list(task_data.get("analysis_summaries", {}).keys())
                elif task_name:
                    task_names.append(task_name)
            except json.JSONDecodeError:
                continue
        
        task_names_str = ", ".join(list(set(task_names))) # é‡è¤‡å‰Šé™¤
        if not task_names_str:
            task_names_str = "ï¼ˆJSONLå†…ã®å„åˆ†æã‚¿ã‚¹ã‚¯ï¼‰"
            
        summary_context = json.dumps(summary_data, ensure_ascii=False, indent=2)
        if len(summary_context) > 2000:
             summary_context = summary_context[:2000] + "\n... (ã‚µãƒãƒªçœç•¥)"
        
    except Exception as e:
        logger.error(f"Step B JSONLã®ãƒ‘ãƒ¼ã‚¹ (ä¸‹èª­ã¿) ã«å¤±æ•—: {e}")
        task_names_str = "ï¼ˆJSONLå†…ã®å„åˆ†æã‚¿ã‚¹ã‚¯ï¼‰"
        summary_context = "{}"

    # (â˜…) --- gemini-2.5-pro ã¸ã®ã€Œè¶…ã€è©³ç´°ãªæŒ‡ç¤ºãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ ---
    # (â˜…) ã”è¦æœ›ã«åŸºã¥ãã€1ä¸‡æ–‡å­—ã‚’è¶…ãˆã‚‹è©³ç´°ãªãƒ¬ãƒãƒ¼ãƒˆã‚’å‡ºåŠ›ã•ã›ã‚‹ãŸã‚ã€
    # (â˜…) å‡ºåŠ›JSONå½¢å¼ã€å½¹å‰²ã€å„ã‚¿ã‚¹ã‚¯ã®å‡¦ç†æ–¹æ³•ã‚’å³å¯†ã«å®šç¾©ã—ã¾ã™ã€‚
    
    prompt_template = """
ã‚ãªãŸã¯ã€é«˜åãªãƒ‡ãƒ¼ã‚¿ã‚¢ãƒŠãƒªã‚¹ãƒˆå…¼çµŒå–¶ã‚³ãƒ³ã‚µãƒ«ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆä¾‹ï¼šè¦³å…‰å”ä¼šã€äº‹æ¥­ä¼šç¤¾ï¼‰ã«æå‡ºã™ã‚‹ãŸã‚ã®ã€é«˜å“è³ªãªã€Œãƒ‡ãƒ¼ã‚¿åˆ†æãƒ¬ãƒãƒ¼ãƒˆã€ã‚’ä½œæˆã™ã‚‹ä»»å‹™ã‚’è² ã£ã¦ã„ã¾ã™ã€‚

ã“ã‚Œã‹ã‚‰ã€éƒ¨ä¸‹ãŒPythonã¨AIï¼ˆFlash Liteï¼‰ã§ä¸€æ¬¡åˆ†æã—ãŸçµæœï¼ˆã‚°ãƒ©ãƒ•ç”»åƒBase64ã€ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«ã€ã‚µãƒãƒªã‚’å«ã‚€JSONLï¼‰ãŒæä¾›ã•ã‚Œã¾ã™ã€‚
ã‚ãªãŸã®ä»•äº‹ã¯ã€ã“ã®ä¸€æ¬¡åˆ†æçµæœã‚’ã€å°‚é–€å®¶ã®è¦–ç‚¹ã€‘ã§è§£é‡ˆã—ç›´ã—ã€ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®æ„æ€æ±ºå®šã«è³‡ã™ã‚‹ã€Œã‚¤ãƒ³ã‚µã‚¤ãƒˆã€ã¨ã€Œæˆ¦ç•¥çš„æè¨€ã€ã‚’å«ã‚€ã€è©³ç´°ã‹ã¤æ§‹é€ åŒ–ã•ã‚ŒãŸãƒ¬ãƒãƒ¼ãƒˆï¼ˆJSONå½¢å¼ï¼‰ã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ã§ã™ã€‚

# 0. å…¨ä½“ã‚µãƒãƒªãƒ¼ï¼ˆå‚è€ƒï¼‰
åˆ†æå¯¾è±¡ã¨ãªã£ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å…¨ä½“åƒã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ã€‚
{summary_context}

# 1. å®Ÿè¡Œã‚¿ã‚¹ã‚¯
æä¾›ã•ã‚Œã‚‹ã€Œåˆ†æãƒ‡ãƒ¼ã‚¿ï¼ˆJSONLï¼‰ã€ã®å„è¡Œï¼ˆå„åˆ†æã‚¿ã‚¹ã‚¯ï¼‰ã‚’ã€æ¼ã‚Œãªãã€‘å‡¦ç†ã—ã€ä»¥ä¸‹ã®ã€Œå‡ºåŠ›JSONå½¢å¼ã€ã«å¾“ã£ã¦ã€ã‚¹ãƒ©ã‚¤ãƒ‰æ§‹æˆæ¡ˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

# 2. å‡ºåŠ›JSONå½¢å¼ï¼ˆå³å®ˆï¼‰
å‡ºåŠ›ã¯ã€ä»¥ä¸‹ã®ã‚­ãƒ¼ã‚’æŒã¤è¾æ›¸ã®ã€ãƒªã‚¹ãƒˆï¼ˆé…åˆ—ï¼‰ã€‘å½¢å¼ `[ {{...}}, {{...}} ]` ã¨ã—ã¾ã™ã€‚
JSONä»¥å¤–ã®ãƒ†ã‚­ã‚¹ãƒˆï¼ˆä¾‹ï¼šã€Œæ‰¿çŸ¥ã—ã¾ã—ãŸã€ï¼‰ã¯ã€çµ¶å¯¾ã«ã€‘å«ã‚ãªã„ã§ãã ã•ã„ã€‚

[
  {{
    "slide_title": "ï¼ˆã‚¹ãƒ©ã‚¤ãƒ‰ã®ã‚¿ã‚¤ãƒˆãƒ«ï¼‰",
    "slide_layout": "ï¼ˆ"title_and_content" ã¾ãŸã¯ "text_and_image" ã¾ãŸã¯ "title_only"ï¼‰",
    "slide_content": [
      "ï¼ˆã“ã®ã‚¹ãƒ©ã‚¤ãƒ‰ã§ä¼ãˆã‚‹ã¹ãã€ã‚¤ãƒ³ã‚µã‚¤ãƒˆã€‘ã‚„ã€è€ƒå¯Ÿã€‘ã‚’ç®‡æ¡æ›¸ãã§è©³ç´°ã«è¨˜è¿°ï¼‰",
      "ï¼ˆãƒ‡ãƒ¼ã‚¿ã‚’å˜ç´”ã«ç¾…åˆ—ã™ã‚‹ã®ã§ã¯ãªãã€å°‚é–€å®¶ã¨ã—ã¦ã®è§£é‡ˆã‚’åŠ ãˆã‚‹ã“ã¨ï¼‰",
      "ï¼ˆå¿…è¦ã§ã‚ã‚Œã°1ã‚¹ãƒ©ã‚¤ãƒ‰ã‚ãŸã‚Š1000æ–‡å­—ä»¥ä¸Šã®è©³ç´°ãªè¨˜è¿°ã‚’è¡Œã†ã“ã¨ï¼‰"
    ],
    "image_base64": "ï¼ˆ"analysis_task" ã« "image_base64" ãŒå­˜åœ¨ã™ã‚Œã°ã€ãã®Base64æ–‡å­—åˆ—ã‚’ã“ã“ã«ã‚³ãƒ”ãƒ¼ã™ã‚‹ã€‚å­˜åœ¨ã—ãªã‘ã‚Œã° null ã¨ã™ã‚‹ï¼‰"
  }}
]

# 3. å¿…é ˆã‚¹ãƒ©ã‚¤ãƒ‰æ§‹æˆï¼ˆã“ã®é †ç•ªã§æ§‹æˆã™ã‚‹ã“ã¨ï¼‰

### A. å°å…¥ã‚¹ãƒ©ã‚¤ãƒ‰ (3ã€œ4æš)
1.  **è¡¨ç´™ (layout: "title_only")**: ã€ŒSNSãƒ‡ãƒ¼ã‚¿åˆ†æãƒ¬ãƒãƒ¼ãƒˆï¼ˆä»®ï¼‰ã€ã®ã‚ˆã†ãªã‚¿ã‚¤ãƒˆãƒ«ã€‚
2.  **ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ãƒ»ã‚µãƒãƒªãƒ¼ (layout: "title_and_content")**:
    * `analysis_task: "OverallSummary"` ã®ãƒ‡ãƒ¼ã‚¿ï¼ˆç‰¹ã« `total_posts` ã‚„ `total_engagement`ï¼‰ã‚’å‚ç…§ã—ã€åˆ†æã®æœ€ã‚‚é‡è¦ãªç™ºè¦‹ï¼ˆKGI/KPIï¼‰ã‚’3ã€œ5ç‚¹ã®ç®‡æ¡æ›¸ãã§è¦ç´„ã—ã¾ã™ã€‚
    * ã€æ¥µã‚ã¦é‡è¦ã€‘ã“ã“ã§ã®ã‚¤ãƒ³ã‚µã‚¤ãƒˆã¯ã€å˜ãªã‚‹æ•°å­—ã®å ±å‘Šã§ã¯ãªãã€ã€Œä½•ã‚’æ„å‘³ã™ã‚‹ã®ã‹ã€ã‚’è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚
3.  **åˆ†ææ¦‚è¦ (layout: "title_and_content")**:
    * åˆ†æã®ç›®çš„ï¼ˆä¾‹ï¼šã€ŒSNSæŠ•ç¨¿ã‹ã‚‰è¦³å…‰å®¢ã®å‹•å‘ã¨é–¢å¿ƒäº‹ã‚’ç‰¹å®šã™ã‚‹ã€ï¼‰ã¨ã€åˆ†æå¯¾è±¡ãƒ‡ãƒ¼ã‚¿ï¼ˆä¾‹ï¼šã€ŒInstagramæŠ•ç¨¿ XXXä»¶ã€ï¼‰ã‚’è¨˜è¿°ã—ã¾ã™ã€‚
4.  **ã‚¢ã‚¸ã‚§ãƒ³ãƒ€ (layout: "title_and_content")**:
    * ã“ã®å¾Œã®ãƒ¬ãƒãƒ¼ãƒˆã®ç›®æ¬¡ï¼ˆä¾‹ï¼šã€Œ1. å…¨ä½“å‚¾å‘ã€ã€Œ2. ä¸»è¦ã‚«ãƒ†ã‚´ãƒªåˆ†æã€ã€Œ3. æˆ¦ç•¥çš„æè¨€ã€ï¼‰ã‚’è¨˜è¿°ã—ã¾ã™ã€‚

### B. è©³ç´°åˆ†æã‚¹ãƒ©ã‚¤ãƒ‰ (JSONLã®å„ã‚¿ã‚¹ã‚¯ã”ã¨ã«1æšä»¥ä¸Š)
`analysis_task` ãŒ "OverallSummary" ä»¥å¤–ã®å„ã‚¿ã‚¹ã‚¯ï¼ˆä¾‹ï¼š {task_names}ï¼‰ã«ã¤ã„ã¦ã€ä»¥ä¸‹ã®æŒ‡ç¤ºã«å¾“ã£ã¦ã‚¹ãƒ©ã‚¤ãƒ‰ã‚’ç”Ÿæˆã—ã¾ã™ã€‚

5.  **ã‚¿ã‚¹ã‚¯å: `run_simple_count` / `run_text_mining` / `run_topic_category_summary` (ã‚°ãƒ©ãƒ•ãŒã‚ã‚‹ã‚¿ã‚¹ã‚¯)**
    * `slide_layout`: "text_and_image"
    * `slide_title`: JSONLã® `analysis_task` åï¼ˆä¾‹ï¼šã€Œé »å‡ºå˜èªåˆ†æã€ï¼‰ã‚’ã€åˆ†ã‹ã‚Šã‚„ã™ã„æ—¥æœ¬èªã‚¿ã‚¤ãƒˆãƒ«ï¼ˆä¾‹ï¼šã€Œä¸»è¦ãªé »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®åˆ†æã€ï¼‰ã«ç¿»è¨³ãƒ»ä¿®æ­£ã—ã¦è¨˜è¼‰ã—ã¾ã™ã€‚
    * `image_base64`: JSONLã® `image_base64` ã‚’ã€å¿…ãšã‚³ãƒ”ãƒ¼ã€‘ã—ã¾ã™ã€‚
    * `slide_content`:
        * ã€æœ€é‡è¦ã€‘ã‚°ãƒ©ãƒ•ï¼ˆ`image_base64`ï¼‰ã¨ãƒ‡ãƒ¼ã‚¿ï¼ˆ`data`ï¼‰ã‚’è©³ç´°ã«åˆ†æã—ã€ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒç†è§£ã™ã¹ãã€ã‚¤ãƒ³ã‚µã‚¤ãƒˆï¼ˆç™ºè¦‹ï¼‰ã€‘ã‚’3ã€œ5ç‚¹ä»¥ä¸Šã®è©³ç´°ãªç®‡æ¡æ›¸ãã§è¨˜è¿°ã—ã¾ã™ã€‚
        * ï¼ˆæ‚ªã„ä¾‹ï¼šã€Œã‚°ãƒ©ãƒ•ã®é€šã‚Šã€AãŒ1ä½ã§ã—ãŸã€‚ã€ï¼‰
        * ï¼ˆè‰¯ã„ä¾‹ï¼šã€Œåˆ†æã®çµæœã€AãŒåœ§å€’çš„å¤šæ•°ã‚’å ã‚ã¦ãŠã‚Šã€ã“ã‚Œã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®é–¢å¿ƒãŒAã«é›†ä¸­ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¤ºå”†ã—ã¦ã„ã¾ã™ã€‚ç‰¹ã«2ä½ã®Bã¨ã®å·®ã¯...ã€ï¼‰
        * `data`ï¼ˆJSONé…åˆ—ï¼‰ã‹ã‚‰ã€ä¸Šä½3ä½ã¾ã§ã®å…·ä½“çš„ãªæ•°å€¤ã‚„é …ç›®åã‚’å¼•ç”¨ã—ã€è€ƒå¯Ÿã«åšã¿ã‚’æŒãŸã›ã¦ãã ã•ã„ã€‚

6.  **ã‚¿ã‚¹ã‚¯å: `run_cooccurrence_network` (å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯)**
    * `slide_layout`: "text_and_image"
    * `slide_title`: ã€Œå…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆ†æï¼šè©±é¡Œã®é–¢é€£æ€§ã€
    * `image_base64`: JSONLã® `image_base64` ã‚’ã€å¿…ãšã‚³ãƒ”ãƒ¼ã€‘ã—ã¾ã™ã€‚
    * `slide_content`:
        * ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³ï¼ˆ`image_base64`ï¼‰ã‚’è§£é‡ˆã—ã€ã©ã®å˜èªãŒä¸­å¿ƒã«ã‚ã‚‹ã‹ï¼ˆä¸­å¿ƒæ€§ï¼‰ã€ã©ã®å˜èªåŒå£«ãŒå¼·ãçµã³ã¤ã„ã¦ã„ã‚‹ã‹ï¼ˆã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ï¼‰ã‚’æŒ‡æ‘˜ã—ã¾ã™ã€‚
        * ï¼ˆä¾‹ï¼šã€Œã€Aã€ã¨ã€Bã€ã€ã¾ãŸã€Cã€ã¨ã€Dã€ã®ã‚°ãƒ«ãƒ¼ãƒ—ãŒå½¢æˆã•ã‚Œã¦ãŠã‚Šã€ã“ã‚Œã¯...ã¨ã„ã†2ã¤ã®ç•°ãªã‚‹æ–‡è„ˆã§èªã‚‰ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¤ºã—ã¾ã™ã€‚ã€ï¼‰
        * `data`ï¼ˆã‚¨ãƒƒã‚¸ãƒªã‚¹ãƒˆï¼‰ã‹ã‚‰ã€`weight`ï¼ˆé‡ã¿ï¼‰ãŒç‰¹ã«é«˜ã„çµ„ã¿åˆã‚ã›ã‚’å…·ä½“çš„ã«å¼•ç”¨ã—ã¦ãã ã•ã„ã€‚

7.  **ã‚¿ã‚¹ã‚¯å: `run_crosstab` / `run_topic_engagement_top5` (ã‚°ãƒ©ãƒ•ãŒãªã„ã‚¿ã‚¹ã‚¯)**
    * `slide_layout`: "title_and_content"
    * `slide_title`: ã‚¿ã‚¹ã‚¯åã‚’åˆ†ã‹ã‚Šã‚„ã™ã„æ—¥æœ¬èªã‚¿ã‚¤ãƒˆãƒ«ï¼ˆä¾‹ï¼šã€Œã‚«ãƒ†ã‚´ãƒªåˆ¥ ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆTOP5ã€ï¼‰ã«ä¿®æ­£ã—ã¦è¨˜è¼‰ã—ã¾ã™ã€‚
    * `image_base64`: null
    * `slide_content`:
        * `data`ï¼ˆJSONé…åˆ—ï¼‰ã®åˆ†æçµæœã‹ã‚‰ã€ç‰¹ç­†ã™ã¹ããƒ‘ã‚¿ãƒ¼ãƒ³ã€ç›¸é–¢ã€ã¾ãŸã¯å…·ä½“ä¾‹ï¼ˆä¾‹ï¼šã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆãŒæœ€ã‚‚é«˜ã‹ã£ãŸæŠ•ç¨¿ã®AIè¦ç´„ï¼‰ã‚’å¼•ç”¨ã—ã€è©³ç´°ãªã‚¤ãƒ³ã‚µã‚¤ãƒˆã‚’è¨˜è¿°ã—ã¾ã™ã€‚

8.  **ã‚¿ã‚¹ã‚¯å: `run_ai_summary_batch` (AIã«ã‚ˆã‚‹è‡ªç”±åˆ†æ)**
    * `slide_layout`: "title_and_content"
    * `slide_title`: AIãŒå®Ÿè¡Œã—ãŸã‚¿ã‚¹ã‚¯åï¼ˆä¾‹ï¼šã€Œãƒã‚¸ãƒ†ã‚£ãƒ–ãªæ„è¦‹ã®å…·ä½“ä¾‹ã€ï¼‰
    * `image_base64`: null
    * `slide_content`:
        * AIã®å›ç­”ï¼ˆ`data`ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®æ–‡å­—åˆ—ï¼‰ã‚’ã€ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå‘ã‘ã«åˆ†ã‹ã‚Šã‚„ã™ãç®‡æ¡æ›¸ãã§å†æ§‹æˆã—ã€æç¤ºã—ã¾ã™ã€‚

### C. çµè«–ã‚¹ãƒ©ã‚¤ãƒ‰ (2æš)
9.  **çµè«– (layout: "title_and_content")**:
    * å…¨ã¦ã®è©³ç´°åˆ†æï¼ˆã‚¹ãƒ©ã‚¤ãƒ‰Bç¾¤ï¼‰ã‹ã‚‰å°ãå‡ºã•ã‚Œã‚‹ã€å…¨ä½“çš„ãªçµè«–ã€‘ã‚’ã€å¼·å›ºãªæ ¹æ‹ ï¼ˆåˆ†æãƒ‡ãƒ¼ã‚¿ã¸ã®è¨€åŠï¼‰ã¨ã¨ã‚‚ã«è¦ç´„ã—ã¾ã™ã€‚
    * ï¼ˆä¾‹ï¼šã€ŒSNSåˆ†æã®çµæœã€å¼·ã¿ã¯ã€Xã€ã§ã‚ã‚Šã€èª²é¡Œã¯ã€Yã€ã§ã‚ã‚‹ã“ã¨ãŒæ˜ã‚‰ã‹ã«ãªã£ãŸã€‚ã€ï¼‰

10. **æˆ¦ç•¥çš„æè¨€ (layout: "title_and_content")**:
    * ã€æœ€é‡è¦ã€‘ã‚ãªãŸãŒçµŒå–¶ã‚³ãƒ³ã‚µãƒ«ã‚¿ãƒ³ãƒˆã¨ã—ã¦ã€ã“ã®åˆ†æçµæœã«åŸºã¥ãã€ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒæ¬¡ã«å–ã‚‹ã¹ãã€å…·ä½“çš„ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆæ–½ç­–ï¼‰ã€‘ã‚’3ã€œ5ç‚¹ã€ææ¡ˆã—ã¦ãã ã•ã„ã€‚
    * ï¼ˆä¾‹ï¼šã€Œ1. å¼·ã¿ã§ã‚ã‚‹ã€Xã€ã‚’ã•ã‚‰ã«ä¼¸ã°ã™ãŸã‚ã€...ãªã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ã‚’æ¨å¥¨ã™ã‚‹ã€‚ã€ï¼‰
    * ï¼ˆä¾‹ï¼šã€Œ2. èª²é¡Œã§ã‚ã‚‹ã€Yã€ã‚’å…‹æœã™ã‚‹ãŸã‚ã€...ãªã‚¿ãƒ¼ã‚²ãƒƒãƒˆå±¤ã¸ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’å¼·åŒ–ã™ã‚‹ã€‚ã€ï¼‰

# 4. æœ€çµ‚ç¢ºèª
* **æƒ…å ±é‡**: ãƒ¬ãƒãƒ¼ãƒˆå…¨ä½“ã§1ä¸‡æ–‡å­—ã‚’è¶…ãˆã‚‹ã‚ˆã†ãªã€è©³ç´°ã§å……å®Ÿã—ãŸå†…å®¹ã‚’æœŸå¾…ã—ã¾ã™ã€‚`slide_content` ã®å„é …ç›®ã¯ã€å˜èªã§ã¯ãªãã€å®Œå…¨ãªã€Œæ–‡ç« ã€ã¾ãŸã¯ã€Œè©³ç´°ãªç®‡æ¡æ›¸ãã€ã«ã—ã¦ãã ã•ã„ã€‚
* **å³æ ¼ãªå½¢å¼**: å‡ºåŠ›ã¯ `[` ã§å§‹ã¾ã‚Š `]` ã§çµ‚ã‚ã‚‹ã€JSONé…åˆ—å½¢å¼ã®ã¿ã¨ã—ã¾ã™ã€‚

ã“ã®æŒ‡ç¤ºã«å¾“ã„ã€æœ€é«˜ã®åˆ†æãƒ¬ãƒãƒ¼ãƒˆï¼ˆJSONï¼‰ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
"""

    prompt = PromptTemplate.from_template(prompt_template)
    
    try:
        # (â˜…) Flash Lite ã« Pro ã¸ã®æŒ‡ç¤ºæ›¸ã‚’ç”Ÿæˆã•ã›ã‚‹
        generated_prompt = prompt.invoke({
            "summary_context": summary_context,
            "task_names": task_names_str
        })
        
        logger.info("Step C ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè‡ªå‹•ç”Ÿæˆ å®Œäº†ã€‚")
        return generated_prompt

    except Exception as e:
        logger.error(f"generate_step_c_prompt error: {e}", exc_info=True)
        # (â˜…) --- ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®æœ€å°é™ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ ---
        return f"# (â˜…) ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè‡ªå‹•ç”Ÿæˆå¤±æ•—: {e}\n\n# æŒ‡ç¤º:\nã‚ãªãŸã¯å„ªç§€ãªçµŒå–¶ã‚³ãƒ³ã‚µãƒ«ã‚¿ãƒ³ãƒˆã§ã™ã€‚æä¾›ã•ã‚Œã‚‹ã€Œåˆ†æãƒ‡ãƒ¼ã‚¿ï¼ˆJSONLï¼‰ã€ã‚’èª­ã¿ã€ä»¥ä¸‹ã®JSONå½¢å¼ã§PowerPointç”¨ã®åˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚\n\n[ {{ \"slide_title\": \"...\", \"slide_layout\": \"title_and_content\", \"slide_content\": [\"...\", \"...\"], \"image_base64\": null }} ]"

# --- 9. (â˜…) Step C: AIãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ (ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å‡¦ç†) ---
# (â˜…) å»ƒæ­¢: generate_step_c_prompt() ã¯ä¸è¦ã«ãªã‚Šã¾ã—ãŸã€‚

def run_step_c_analysis(
    jsonl_data_string: str,
    model_name: str,
    progress_bar: st.delta_generator.DeltaGenerator,
    log_placeholder: st.delta_generator.DeltaGenerator
) -> str:
    """
    (â˜…) Step C: AIãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ (ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ»RateLimitå¯¾å¿œç‰ˆ)
    
    [æ–°ãƒ­ã‚¸ãƒƒã‚¯] ãƒãƒ³ã‚°ã‚¢ãƒƒãƒ— (504 Timeout) ã‚’å›é¿ã™ã‚‹ãŸã‚ã€AIã«Base64ç”»åƒ(å·¨å¤§ãƒˆãƒ¼ã‚¯ãƒ³)ã‚’
    æ¸¡ã™ã®ã‚’ *ã‚„ã‚* ã€ã€Œãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã€ã®ã¿ã‚’æ¸¡ã—ã¦è€ƒå¯Ÿã‚’ç”Ÿæˆã•ã›ã‚‹ã€‚
    Pythonå´ã§ã€AIã®è€ƒå¯Ÿ(ãƒ†ã‚­ã‚¹ãƒˆ)ã¨ã€å…ƒã®Base64ç”»åƒã‚’ã€Œå†çµåˆã€ã™ã‚‹ã€‚
    """
    logger.info(f"Step C AIãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ (ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å‡¦ç†) é–‹å§‹... (Model: {model_name})")

    # (â˜…) --- 1. ãƒ¢ãƒ‡ãƒ«ã®RPMåˆ¶é™ã¨ã‚¹ãƒªãƒ¼ãƒ—æ™‚é–“ã‚’å®šç¾© ---
    if model_name == MODEL_PRO:
        rpm_limit = 2
        tpm_limit = 125000
    else: # (â˜…) ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ Flash
        model_name = MODEL_FLASH
        rpm_limit = 10
        tpm_limit = 250000
        
    sleep_time = (60 / rpm_limit) + 0.5 # (e.g., Pro: 30.5s, Flash: 6.5s)
    
    logger.info(f"ãƒ¢ãƒ‡ãƒ«: {model_name}, RPM: {rpm_limit}, å¾…æ©Ÿ: {sleep_time:.1f}ç§’")

    # (â˜…) --- 2. ãƒãƒ£ãƒ³ã‚¯ç”Ÿæˆç”¨ã®AIãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’å®šç¾© ---
    # (â˜…) ä¿®æ­£: AIã¯ã€Œç”»åƒ(image_base64)ã€ã‚’è¦‹ãªã„å‰æã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¤‰æ›´
    ITERATIVE_SLIDE_PROMPT_TEMPLATE = """
    ã‚ãªãŸã¯ã‚·ãƒ‹ã‚¢ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒŠãƒªã‚¹ãƒˆã§ã‚ã‚Šã€ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå‘ã‘ãƒ¬ãƒãƒ¼ãƒˆã®ã€Œã‚¹ãƒ©ã‚¤ãƒ‰1æšã€ã®
    ã€ãƒ†ã‚­ã‚¹ãƒˆéƒ¨åˆ†ã€‘ã‚’ä½œæˆã—ã¦ã„ã¾ã™ã€‚
    æä¾›ã•ã‚Œã‚‹ã€Œåˆ†æã‚¿ã‚¹ã‚¯ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ†ã‚­ã‚¹ãƒˆã¨æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã®ã¿ï¼‰ã€ã‚’èª­ã¿ã€ã“ã®ã‚¿ã‚¹ã‚¯å°‚ç”¨ã®
    ã‚¹ãƒ©ã‚¤ãƒ‰ã‚¿ã‚¤ãƒˆãƒ«ã¨è€ƒå¯Ÿï¼ˆslide_contentï¼‰ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

    # åˆ†æã‚¿ã‚¹ã‚¯ãƒ‡ãƒ¼ã‚¿ (ãƒ†ã‚­ã‚¹ãƒˆãƒ»æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã®ã¿):
    {task_data_text_only}

    # æŒ‡ç¤º:
    1.  **ã‚¿ã‚¤ãƒˆãƒ«**: `task_data_text_only` ã® `analysis_task` åã«åŸºã¥ãã€ professional ãªã€Œslide_titleã€ã‚’è€ƒæ¡ˆã—ã¦ãã ã•ã„ã€‚
    2.  **è€ƒå¯Ÿ (æœ€é‡è¦)**: `task_data_text_only` ã® `summary` ã¨ `data`ï¼ˆãƒ†ãƒ¼ãƒ–ãƒ«ãƒ‡ãƒ¼ã‚¿ï¼‰ã‚’è§£é‡ˆã—ã€ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒçŸ¥ã‚‹ã¹ãã€ã‚¤ãƒ³ã‚µã‚¤ãƒˆï¼ˆç™ºè¦‹ï¼‰ã€‘ã‚’ã€Œslide_contentã€ã¨ã—ã¦2ã€œ4ç‚¹ã®è©³ç´°ãªç®‡æ¡æ›¸ãï¼ˆæ–‡å­—åˆ—ãƒªã‚¹ãƒˆï¼‰ã§è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚
        (æ³¨: ã‚ãªãŸã«ã¯ã‚°ãƒ©ãƒ•ç”»åƒã¯æä¾›ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚`data` ã®æ•°å€¤ã‚„ `summary` ã®ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã‚’æ ¹æ‹ ã«è€ƒå¯Ÿã‚’è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚)

    # å‡ºåŠ›å½¢å¼ (å³å®ˆ):
    * JSONä»¥å¤–ã®ãƒ†ã‚­ã‚¹ãƒˆã¯çµ¶å¯¾ã«å«ã‚ãšã€ã€å˜ä¸€ã®JSONã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã€‘`{{ ... }}` ã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
    * ä»¥ä¸‹ã®æ§‹é€ ã‚’å³æ ¼ã«å®ˆã£ã¦ãã ã•ã„ã€‚
        {{
          "slide_title": "ï¼ˆæŒ‡ç¤º1ã§è€ƒæ¡ˆã—ãŸã‚¿ã‚¤ãƒˆãƒ«ï¼‰",
          "slide_content": [
            "ï¼ˆæŒ‡ç¤º2ã§è¨˜è¿°ã—ãŸã‚¤ãƒ³ã‚µã‚¤ãƒˆ1ï¼‰",
            "ï¼ˆæŒ‡ç¤º2ã§è¨˜è¿°ã—ãŸã‚¤ãƒ³ã‚µã‚¤ãƒˆ2ï¼‰"
          ]
        }}

    # å›ç­” (å˜ä¸€ã®JSONã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ã¿):
    """
    
    prompt = PromptTemplate.from_template(ITERATIVE_SLIDE_PROMPT_TEMPLATE)

    # (â˜…) --- ä¿®æ­£: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’ 120ç§’ (2åˆ†) ã«è¨­å®š ---
    # (â˜…) ãƒªã‚¯ã‚¨ã‚¹ãƒˆã¯ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ãªã®ã§ã€300ç§’ã‚‚ä¸è¦
    llm = get_llm(model_name=model_name, temperature=0.2, timeout_seconds=120)
    if llm is None:
        st.error(f"AIãƒ¢ãƒ‡ãƒ«({model_name})ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")
        return "[]" # ç©ºã®JSONãƒªã‚¹ãƒˆ
    
    chain = prompt | llm | StrOutputParser()
    # (â˜…) --- ã“ã“ã¾ã§ãŒä¿®æ­£ç‚¹ ---

    # (â˜…) --- 3. é€æ¬¡å‡¦ç†ãƒ«ãƒ¼ãƒ— (ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°ã¯å»ƒæ­¢) ---
    report_slides_list = []
    log_messages_ui = []
    
    tasks_all = jsonl_data_string.strip().splitlines()
    
    # (â˜…) 3.1. OverallSummaryã‚’æŠ½å‡ºã—ã€æ®‹ã‚Šã‚’å‡¦ç†å¯¾è±¡ã‚¿ã‚¹ã‚¯ã¨ã™ã‚‹
    summary_line = "{}"
    tasks_to_process = []
    for line in tasks_all:
        if '"analysis_task": "OverallSummary"' in line:
            summary_line = line
        else:
            tasks_to_process.append(line)
            
    if not tasks_to_process:
        logger.warning("å‡¦ç†å¯¾è±¡ã®åˆ†æã‚¿ã‚¹ã‚¯ãŒ0ä»¶ã§ã™ã€‚")
        return "[]"

    total_tasks = len(tasks_to_process)
    logger.info(f"å…¨ {total_tasks} ã‚¿ã‚¹ã‚¯ã‚’é€æ¬¡å‡¦ç†ã—ã¾ã™ã€‚")
    
    # (â˜…) 3.2. è¡¨ç´™ã‚¹ãƒ©ã‚¤ãƒ‰ã‚’è¿½åŠ 
    report_slides_list.append({
        "slide_title": "SNSãƒ‡ãƒ¼ã‚¿åˆ†æãƒ¬ãƒãƒ¼ãƒˆ",
        "slide_layout": "title_only",
        "slide_content": ["AI-Generated Analysis (Powered by Gemini)"],
        "image_base64": None
    })
    
    # (â˜…) 3.3. ç›®æ¬¡ã‚¹ãƒ©ã‚¤ãƒ‰ã‚’è¿½åŠ  (ã“ã®æ™‚ç‚¹ã§ã¯ã‚¿ã‚¹ã‚¯åã®ã¿)
    try:
        agenda_items = []
        for i, task_line in enumerate(tasks_to_process):
             # (â˜…) å·¨å¤§ã‚¿ã‚¹ã‚¯åˆ†å‰²(JSONãƒ‘ãƒ¼ã‚¹å¤±æ•—)ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            try:
                task_name = json.loads(task_line).get('analysis_task', f'åˆ†æã‚¿ã‚¹ã‚¯ {i+1}')
            except json.JSONDecodeError:
                task_name = f'åˆ†æã‚¿ã‚¹ã‚¯ {i+1} (èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼)'
            agenda_items.append(f"{i+1}. {task_name}")

        agenda_items.append(f"{len(tasks_to_process) + 1}. çµè«–ã¨æˆ¦ç•¥çš„æè¨€")
        report_slides_list.append({
            "slide_title": "æœ¬æ—¥ã®ã‚¢ã‚¸ã‚§ãƒ³ãƒ€",
            "slide_layout": "title_and_content",
            "slide_content": agenda_items,
            "image_base64": None
        })
    except Exception as e:
        logger.error(f"ç›®æ¬¡ã‚¹ãƒ©ã‚¤ãƒ‰ã®ç”Ÿæˆã«å¤±æ•—: {e}")

    # (â˜…) 3.4. ãƒ¡ã‚¤ãƒ³ã®åˆ†æã‚¹ãƒ©ã‚¤ãƒ‰ã‚’ãƒ«ãƒ¼ãƒ—å‡¦ç†
    for i, task_line in enumerate(tasks_to_process):
        
        task_name = f"Task {i+1}/{total_tasks}"
        original_task_json = {}
        
        try:
            # (â˜…) --- 3.4.1. ã‚¿ã‚¹ã‚¯ã®ãƒ‘ãƒ¼ã‚¹ã¨ç”»åƒ/ãƒ†ã‚­ã‚¹ãƒˆã®åˆ†é›¢ ---
            original_task_json = json.loads(task_line)
            task_name = original_task_json.get('analysis_task', task_name)

            # (â˜…) 1. ç”»åƒã‚’Pythonå¤‰æ•°ã«é€€é¿
            image_to_pass_through = original_task_json.get("image_base64")
            
            # (â˜…) 2. AIã«æ¸¡ã™ã€Œãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã€ã®JSONã‚’ä½œæˆ
            text_only_task_json = original_task_json.copy()
            text_only_task_json["image_base64"] = None # (â˜…) ç”»åƒã‚’å‰Šé™¤
            # (â˜…) dataãŒå·¨å¤§ã™ãã‚‹å ´åˆã‚‚è€ƒæ…®ã—ã€dataã‚‚1000æ–‡å­—ã«åˆ¶é™
            if "data" in text_only_task_json and len(json.dumps(text_only_task_json["data"])) > 1000:
                text_only_task_json["data"] = f"ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼: {str(text_only_task_json['data'])[:1000]}...ï¼‰"
            
            task_data_text_only_str = json.dumps(text_only_task_json)
            
        except Exception as e:
            logger.error(f"ã‚¿ã‚¹ã‚¯ '{task_name}' ã®JSONãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—: {e}")
            log_messages_ui.append(f"  -> ERROR: '{task_name}' ã®JSONãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            continue # (â˜…) ã“ã®ã‚¿ã‚¹ã‚¯ã¯ã‚¹ã‚­ãƒƒãƒ—

        # (â˜…) --- 3.4.2. UIï¼ˆé€²æ—ãƒãƒ¼ãƒ»ãƒ­ã‚°ï¼‰ã®æ›´æ–° ---
        progress_percent = (i + 1) / (total_tasks + 1) # (â˜…) +1 ã¯çµè«–ã‚¹ãƒ©ã‚¤ãƒ‰åˆ†
        progress_bar.progress(progress_percent, text=f"Step C (ã‚¹ãƒ©ã‚¤ãƒ‰ç”Ÿæˆä¸­): {i+1}/{total_tasks} (ãƒ¢ãƒ‡ãƒ«: {model_name})")
        log_messages_ui.append(f"[{i+1}/{total_tasks}] '{task_name}' ã®å‡¦ç†ã‚’é–‹å§‹ (æ–‡å­—æ•°: {len(task_data_text_only_str):,})...")
        log_placeholder.text_area("å®Ÿè¡Œãƒ­ã‚°:", "\n".join(log_messages_ui[::-1]), height=250, key=f"step_c_log_{i}")

        try:
            # (â˜…) --- 3.4.3. AIã¸ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆ (ãƒ†ã‚­ã‚¹ãƒˆã®ã¿) ---
            log_messages_ui.append(f"  -> AI ({model_name}) ã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡... (Timeout: 120s)")
            log_placeholder.text_area("å®Ÿè¡Œãƒ­ã‚°:", "\n".join(log_messages_ui[::-1]), height=250, key=f"step_c_log_{i}_sending")
            
            response_str = chain.invoke({"task_data_text_only": task_data_text_only_str})
            
            log_messages_ui.append(f"  -> AI ãŒå¿œç­”ã—ã¾ã—ãŸã€‚ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è§£æä¸­...")
            log_placeholder.text_area("å®Ÿè¡Œãƒ­ã‚°:", "\n".join(log_messages_ui[::-1]), height=250, key=f"step_c_log_{i}_received")

            # (â˜…) AIã®å›ç­” (ã‚¿ã‚¤ãƒˆãƒ«ã¨ã‚³ãƒ³ãƒ†ãƒ³ãƒˆã®ã¿) ã‚’ãƒ‘ãƒ¼ã‚¹
            match = re.search(r'\{.*\}', response_str, re.DOTALL)
            if match:
                ai_response_json = json.loads(match.group(0))
                
                # (â˜…) --- 3.4.4. AIã®è€ƒå¯Ÿã¨ã€é€€é¿ã•ã›ãŸç”»åƒã‚’ã€Œå†çµåˆã€ ---
                final_slide_object = {
                    "slide_title": ai_response_json.get("slide_title", task_name),
                    "slide_layout": "text_and_image" if image_to_pass_through else "title_and_content",
                    "slide_content": ai_response_json.get("slide_content", ["AIã«ã‚ˆã‚‹è€ƒå¯Ÿã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚"]),
                    "image_base64": image_to_pass_through # (â˜…) ã“ã“ã§ç”»åƒã‚’æˆ»ã™
                }
                report_slides_list.append(final_slide_object)
                log_messages_ui.append(f"  -> SUCCESS: ã‚¹ãƒ©ã‚¤ãƒ‰ '{final_slide_object.get('slide_title')}' ã‚’ç”Ÿæˆã—ã¾ã—ãŸã€‚")
            else:
                raise Exception("AIãŒJSONã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ `{{...}}` ã‚’è¿”ã—ã¾ã›ã‚“ã§ã—ãŸã€‚")
        
        except Exception as e:
            logger.error(f"ã‚¿ã‚¹ã‚¯ '{task_name}' ã®å‡¦ç†ã«å¤±æ•—: {e}", exc_info=True)
            log_messages_ui.append(f"  -> ERROR: '{task_name}' ã®å‡¦ç†ã«å¤±æ•—ã€‚{e}")
            report_slides_list.append({
                "slide_title": f"ã‚¨ãƒ©ãƒ¼: {task_name}",
                "slide_layout": "title_and_content",
                "slide_content": [f"ã“ã®ã‚¹ãƒ©ã‚¤ãƒ‰ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚", f"ã‚¨ãƒ©ãƒ¼: {e}"],
                "image_base64": None
            })
        
        # (â˜…) --- 3.4.5. Rate Limit ã®ãŸã‚ã®å¾…æ©Ÿ ---
        if i < total_tasks: # (â˜…) çµè«–ã‚¹ãƒ©ã‚¤ãƒ‰ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒã¾ã æ®‹ã£ã¦ã„ã‚‹ãŸã‚ã€å¿…ãšå¾…æ©Ÿ
            log_messages_ui.append(f"  -> Rate Limit (RPM) ã®ãŸã‚ {sleep_time:.1f} ç§’å¾…æ©Ÿã—ã¾ã™...")
            log_placeholder.text_area("å®Ÿè¡Œãƒ­ã‚°:", "\n".join(log_messages_ui[::-1]), height=250, key=f"step_c_log_{i}_sleep")
            time.sleep(sleep_time)

    # (â˜…) 4. çµè«–ã‚¹ãƒ©ã‚¤ãƒ‰ã®ç”Ÿæˆ
    try:
        chunk_name = f"çµè«–ã‚¹ãƒ©ã‚¤ãƒ‰"
        progress_percent = 1.0
        progress_bar.progress(progress_percent, text=f"Step C (ãƒãƒ£ãƒ³ã‚¯å‡¦ç†ä¸­): {chunk_name} (ãƒ¢ãƒ‡ãƒ«: {model_name})")
        log_messages_ui.append(f"[{total_tasks+1}/{total_tasks+1}] {chunk_name} ã®å‡¦ç†ã‚’é–‹å§‹...")
        log_placeholder.text_area("å®Ÿè¡Œãƒ­ã‚°:", "\n".join(log_messages_ui[::-1]), height=250, key="step_c_log_final")

        conclusion_llm = get_llm(model_name=model_name, temperature=0.2, timeout_seconds=120)
        if conclusion_llm is None:
            raise Exception("çµè«–ã‚¹ãƒ©ã‚¤ãƒ‰ç”¨AIãƒ¢ãƒ‡ãƒ«ã®å–å¾—ã«å¤±æ•—")

        CONCLUSION_PROMPT_TEMPLATE = """
        ã‚ãªãŸã¯ã‚·ãƒ‹ã‚¢ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒŠãƒªã‚¹ãƒˆã§ã™ã€‚
        ä»¥ä¸‹ã®ã€Œåˆ†æã‚µãƒãƒªãƒ¼ã€ã¨ã€Œã“ã‚Œã¾ã§ç”Ÿæˆã—ãŸã‚¹ãƒ©ã‚¤ãƒ‰ã®ã‚¿ã‚¤ãƒˆãƒ«ãƒªã‚¹ãƒˆã€ã«åŸºã¥ãã€
        ãƒ¬ãƒãƒ¼ãƒˆã®ç· ã‚ããã‚Šã¨ãªã‚‹ã€çµè«–ã¨æˆ¦ç•¥çš„æè¨€ã€‘ã®ã‚¹ãƒ©ã‚¤ãƒ‰1æšåˆ†ã®JSONã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

        # åˆ†æã‚µãƒãƒªãƒ¼ (OverallSummary):
        {summary_data_line}
        
        # ç”Ÿæˆæ¸ˆã¿ã‚¹ãƒ©ã‚¤ãƒ‰ã‚¿ã‚¤ãƒˆãƒ«:
        {slide_titles}

        # æŒ‡ç¤º:
        1.  ã‚¿ã‚¤ãƒˆãƒ«ã¯ã€Œçµè«–ã¨æˆ¦ç•¥çš„æè¨€ã€ã¨ã—ã¾ã™ã€‚
        2.  ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã¯ã€Œtitle_and_contentã€ã¨ã—ã¾ã™ã€‚
        3.  å†…å®¹ã¯ã€åˆ†æå…¨ä½“ã‹ã‚‰å°ã‹ã‚Œã‚‹ã€Œçµè«–ã€ã¨ã€ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒæ¬¡ã«å–ã‚‹ã¹ãã€Œå…·ä½“çš„ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆæè¨€ï¼‰ã€ã‚’3ã€œ5ç‚¹ã®ç®‡æ¡æ›¸ãã§è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚
        4.  ç”»åƒ (image_base64) ã¯ null ã¨ã—ã¾ã™ã€‚

        # å‡ºåŠ›å½¢å¼ (å³å®ˆ):
        * JSONä»¥å¤–ã®ãƒ†ã‚­ã‚¹ãƒˆã¯çµ¶å¯¾ã«å«ã‚ãšã€ã€å˜ä¸€ã®JSONã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã€‘`{{ ... }}` ã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

        # å›ç­” (å˜ä¸€ã®JSONã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ã¿):
        """
        
        conclusion_prompt = PromptTemplate.from_template(CONCLUSION_PROMPT_TEMPLATE)
        conclusion_chain = conclusion_prompt | conclusion_llm | StrOutputParser()
        
        log_messages_ui.append(f"  -> AI ({model_name}) ã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡... (Timeout: 120s)")
        log_placeholder.text_area("å®Ÿè¡Œãƒ­ã‚°:", "\n".join(log_messages_ui[::-1]), height=250, key="step_c_log_final_sending")
        
        response_str = conclusion_chain.invoke({
            "summary_data_line": summary_line,
            "slide_titles": json.dumps([s.get('slide_title') for s in report_slides_list], ensure_ascii=False)
        })
        
        log_messages_ui.append(f"  -> AI ãŒå¿œç­”ã—ã¾ã—ãŸã€‚ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è§£æä¸­...")
        log_placeholder.text_area("å®Ÿè¡Œãƒ­ã‚°:", "\n".join(log_messages_ui[::-1]), height=250, key="step_c_log_final_received")

        match = re.search(r'\{.*\}', response_str, re.DOTALL)
        if match:
            report_slides_list.append(json.loads(match.group(0)))
            log_messages_ui.append(f"  -> SUCCESS: çµè«–ã‚¹ãƒ©ã‚¤ãƒ‰ã‚’ç”Ÿæˆã—ã¾ã—ãŸã€‚")
        else:
            raise Exception("AIãŒçµè«–ã‚¹ãƒ©ã‚¤ãƒ‰ã®JSONã‚’è¿”ã—ã¾ã›ã‚“ã§ã—ãŸã€‚")
            
    except Exception as e:
         logger.error(f"çµè«–ã‚¹ãƒ©ã‚¤ãƒ‰ã®ç”Ÿæˆã«å¤±æ•—: {e}")
         log_messages_ui.append(f"  -> ERROR: çµè«–ã‚¹ãƒ©ã‚¤ãƒ‰ã®ç”Ÿæˆã«å¤±æ•—ã€‚{e}")
         report_slides_list.append({
                "slide_title": "çµè«–ã¨æˆ¦ç•¥çš„æè¨€ (ç”Ÿæˆå¤±æ•—)",
                "slide_layout": "title_and_content",
                "slide_content": [f"çµè«–ã‚¹ãƒ©ã‚¤ãƒ‰ã®è‡ªå‹•ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚", f"ã‚¨ãƒ©ãƒ¼: {e}"],
                "image_base64": None
            })

    # (â˜…) 5. æœ€çµ‚çš„ãªJSONæ–‡å­—åˆ—ã‚’è¿”ã™
    progress_bar.progress(1.0, text="Step C: å®Œäº†ï¼")
    log_placeholder.text_area("å®Ÿè¡Œãƒ­ã‚°:", "\n".join(log_messages_ui[::-1]), height=250, key="step_c_log_done")
    
    return json.dumps(report_slides_list, ensure_ascii=False, indent=2)

def render_step_c():
    """(Step C) AIãƒ¬ãƒãƒ¼ãƒˆç”ŸæˆUIã‚’æç”»ã™ã‚‹"""
    st.title(f"ğŸ–‹ï¸ Step C: AIåˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ") # (â˜…) ãƒ¢ãƒ‡ãƒ«åã‚’ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰å‰Šé™¤

    # Step C å›ºæœ‰ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆ
    if 'step_c_jsonl_data' not in st.session_state:
        st.session_state.step_c_jsonl_data = None
    if 'step_c_prompt' not in st.session_state:
        st.session_state.step_c_prompt = None # (â˜…) ã“ã®å¤‰æ•°ã¯ã‚‚ã†ä½¿ç”¨ã—ã¾ã›ã‚“
    if 'step_c_report_json' not in st.session_state:
        st.session_state.step_c_report_json = None
    if 'step_c_model' not in st.session_state:
        st.session_state.step_c_model = MODEL_FLASH # (â˜…) ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’Flashã«

    # --- 1. ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ ---
    st.header("Step 1: åˆ†æãƒ‡ãƒ¼ã‚¿ (JSON) ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
    st.info("Step B ã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã—ãŸ `analysis_data.json` ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
    uploaded_report_file = st.file_uploader(
        "åˆ†æãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ« (analysis_data.json)",
        type=['json', 'jsonl', 'txt'],
        key="step_c_uploader"
    )

    if uploaded_report_file:
        try:
            # (â˜…) ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã‚‚ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆã¯ *è¡Œã‚ãªã„*
            if st.session_state.step_c_jsonl_data is None:
                jsonl_data_string = uploaded_report_file.getvalue().decode('utf-8')
                st.session_state.step_c_jsonl_data = jsonl_data_string
                st.session_state.step_c_report_json = None # (â˜…) çµæœã‚’ãƒªã‚»ãƒƒãƒˆ
                st.success(f"ãƒ•ã‚¡ã‚¤ãƒ«ã€Œ{uploaded_report_file.name}ã€èª­è¾¼å®Œäº†")
        
        except Exception as e:
            logger.error(f"Step C ãƒ•ã‚¡ã‚¤ãƒ«èª­è¾¼ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            return
    else:
        st.session_state.step_c_jsonl_data = None
        st.session_state.step_c_prompt = None
        st.session_state.step_c_report_json = None
        st.warning("åˆ†æã‚’ç¶šã‘ã‚‹ã«ã¯ã€Step B ã§ç”Ÿæˆã—ãŸ JSON ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
        return

    # (â˜…) --- ä¿®æ­£: æ—§Step 2 (ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç·¨é›†) ã‚’å‰Šé™¤ ---
    # (â˜…) é€æ¬¡å‡¦ç†ãƒ¢ãƒ‡ãƒ«ã«å¤‰æ›´ã—ãŸãŸã‚ã€å·¨å¤§ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ç·¨é›†ã¯ä¸è¦ã«ãªã‚Šã¾ã—ãŸã€‚
    # (â˜…) --- ã“ã“ã¾ã§ãŒä¿®æ­£ç‚¹ ---


    # --- 2. åˆ†æãƒ¬ãƒãƒ¼ãƒˆã®å®Ÿè¡Œ (â˜… æ—§Step 3) ---
    st.header("Step 2: AIåˆ†æãƒ¬ãƒãƒ¼ãƒˆã®å®Ÿè¡Œ") # (â˜…) ã‚¹ãƒ†ãƒƒãƒ—ç•ªå·ã‚’ 2 ã«å¤‰æ›´

    # (â˜…) --- ä¿®æ­£: ãƒ¢ãƒ‡ãƒ«é¸æŠUI ---
    st.markdown("åˆ†æã«ä½¿ç”¨ã™ã‚‹AIãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
    
    model_options = [MODEL_FLASH, MODEL_PRO]
    try:
        default_index = model_options.index(st.session_state.step_c_model)
    except ValueError:
        default_index = 0
        
    selected_model_name = st.radio(
        "ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«",
        options=model_options,
        index=default_index,
        key="step_c_model_radio",
        horizontal=True,
    )
    st.session_state.step_c_model = selected_model_name

    if selected_model_name == MODEL_PRO:
        st.warning(
            f"**`{MODEL_PRO}` (ç„¡æ–™æ ) ã¯ 2 RPM (30ç§’/ãƒªã‚¯ã‚¨ã‚¹ãƒˆ) ã®åˆ¶é™ãŒã‚ã‚Šã¾ã™ã€‚**\n"
            f"ã‚¹ãƒ©ã‚¤ãƒ‰10æšã®ç”Ÿæˆã«ã¯ç´„5åˆ†ã‹ã‹ã‚Šã¾ã™ã€‚ã”æ³¨æ„ãã ã•ã„ã€‚"
        )
    else:
        st.info(
            f"**`{MODEL_FLASH}` (ç„¡æ–™æ ) ã¯ 10 RPM (6ç§’/ãƒªã‚¯ã‚¨ã‚¹ãƒˆ) ã®åˆ¶é™ãŒã‚ã‚Šã¾ã™ã€‚**\n"
            f"æ¯”è¼ƒçš„ é«˜é€Ÿã«ç”Ÿæˆã§ãã¾ã™ã€‚ï¼ˆæ¨å¥¨ï¼‰"
        )
    # (â˜…) --- ã“ã“ã¾ã§ãŒä¿®æ­£ç‚¹ ---

    
    if st.button(f"åˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ (Step 2)", key="execute_button_C", type="primary", use_container_width=True):
        if not st.session_state.step_c_jsonl_data:
            st.error("ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚Step 1ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
            return
        
        # (â˜…) --- ä¿®æ­£: é€²æ—ãƒãƒ¼ã¨ãƒ­ã‚°ç”¨ã®ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ã‚’å®šç¾© ---
        progress_bar = st.progress(0.0, text="Step C: åˆ†æå¾…æ©Ÿä¸­...")
        log_placeholder = st.empty()
        # (â˜…) --- ã“ã“ã¾ã§ãŒä¿®æ­£ç‚¹ ---

        selected_model = st.session_state.step_c_model
        
        try:
            # (â˜…) ä¿®æ­£: é€æ¬¡å‡¦ç†ã‚’è¡Œã† run_step_c_analysis ã«å¤‰æ›´
            st.session_state.step_c_report_json = run_step_c_analysis(
                st.session_state.step_c_jsonl_data,
                selected_model,
                progress_bar, # (â˜…) é€²æ—ãƒãƒ¼ã‚’æ¸¡ã™
                log_placeholder # (â˜…) ãƒ­ã‚°è¡¨ç¤ºã‚’æ¸¡ã™
            )
            st.success("AIã«ã‚ˆã‚‹åˆ†æãƒ¬ãƒãƒ¼ãƒˆãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸï¼")
            
        except Exception as e:
            # (â˜…) å®Ÿè¡Œæ™‚ã‚¨ãƒ©ãƒ¼ã®ã‚­ãƒ£ãƒƒãƒ
            logger.error(f"Step C å®Ÿè¡Œä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            st.error(f"åˆ†æå®Ÿè¡Œä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            progress_bar.progress(1.0, text="ã‚¨ãƒ©ãƒ¼ã«ã‚ˆã‚Šä¸­æ–­")


    # --- 3. çµæœã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã¨ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ (â˜… æ—§Step 4) ---
    if st.session_state.step_c_report_json:
        st.header("Step 3: åˆ†æãƒ¬ãƒãƒ¼ãƒˆï¼ˆJSONï¼‰ã®ç¢ºèªã¨ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ") # (â˜…) ã‚¹ãƒ†ãƒƒãƒ—ç•ªå·ã‚’ 3 ã«å¤‰æ›´
        st.info("ä»¥ä¸‹ã®æ§‹é€ åŒ–JSONã¯ã€Step D (PowerPointç”Ÿæˆ) ã§ä½¿ç”¨ã—ã¾ã™ã€‚")

        st.download_button(
            label="åˆ†æãƒ¬ãƒãƒ¼ãƒˆ (report_for_powerpoint.json) ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            data=st.session_state.step_c_report_json,
            file_name="report_for_powerpoint.json",
            mime="application/json",
            type="primary",
            use_container_width=True
        )

        st.markdown("---")
        st.subheader("ç”Ÿæˆã•ã‚ŒãŸãƒ¬ãƒãƒ¼ãƒˆ ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")
        
        try:
            report_data = json.loads(st.session_state.step_c_report_json)
            if isinstance(report_data, list) and all(isinstance(item, dict) for item in report_data):
                st.text_area(
                    "AIãŒç”Ÿæˆã—ãŸæ§‹é€ åŒ–JSON (ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼: å…ˆé ­5000æ–‡å­—)",
                    value=st.session_state.step_c_report_json[:5000] + "...",
                    height=300,
                    key="json_preview_C",
                    disabled=True
                )
                
                st.markdown("---")
                st.subheader(f"ã‚¹ãƒ©ã‚¤ãƒ‰æ§‹æˆ ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ ({len(report_data)}æš)")
                for i, slide in enumerate(report_data):
                    title = slide.get('slide_title', 'ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ãªã—ï¼‰')
                    layout = slide.get('slide_layout', 'N/A')
                    
                    slide_content_list = slide.get('slide_content')
                    if isinstance(slide_content_list, list) and slide_content_list:
                        content_preview = str(slide_content_list[0]) if slide_content_list[0] else "ï¼ˆç©ºã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ï¼‰"
                    else:
                        content_preview = "ï¼ˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãªã—ï¼‰"

                    has_image = "æœ‰ã‚Š" if slide.get("image_base64") else "ç„¡ã—"
                    
                    expander_label = f"**{i+1}: {title}** (Layout: {layout}, Image: {has_image})"
                    with st.expander(expander_label):
                        st.markdown(f"**å†…å®¹ (æŠœç²‹):**\n- {content_preview}...")
            else:
                st.error("AIã®å›ç­”ãŒæœŸå¾…ã—ãŸã‚¹ãƒ©ã‚¤ãƒ‰ã®ãƒªã‚¹ãƒˆå½¢å¼ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
                st.text_area("AIã®ç”Ÿå›ç­” (JSON):", value=st.session_state.step_c_report_json, height=200, disabled=True)
                
        except Exception as e:
            st.error(f"ãƒ¬ãƒãƒ¼ãƒˆã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            st.text_area("AIã®ç”Ÿå›ç­” (ãƒ‘ãƒ¼ã‚¹å¤±æ•—):", value=st.session_state.step_c_report_json, height=200, disabled=True)
            
        st.success("ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€Step D (PowerPointç”Ÿæˆ) ã«é€²ã‚“ã§ãã ã•ã„ã€‚")


# (â˜…) ---Step D---
try:
    import pptx
    from pptx import Presentation
    from pptx.util import Inches, Pt
    from pptx.enum.shapes import MSO_SHAPE
    from pptx.enum.dml import MSO_THEME_COLOR
except ImportError:
    st.error(
        "PowerPointç”Ÿæˆãƒ©ã‚¤ãƒ–ãƒ©ãƒª(python-pptx)ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
        "pip install python-pptx ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚"
    )

try:
    import pptx
    from pptx import Presentation
    from pptx.util import Inches, Pt
    from pptx.enum.shapes import MSO_SHAPE
    from pptx.enum.dml import MSO_THEME_COLOR
except ImportError:
    st.error(
        "PowerPointç”Ÿæˆãƒ©ã‚¤ãƒ–ãƒ©ãƒª(python-pptx)ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
        "pip install python-pptx ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚"
    )

def find_layout_by_name(prs: pptx.presentation.Presentation, layout_name: str) -> Optional[pptx.slide.SlideLayout]:
    """
    (â˜…) ãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒã‚¹ã‚¿ãƒ¼ã‹ã‚‰ã€æŒ‡å®šã•ã‚ŒãŸãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆåï¼ˆå®Œå…¨ä¸€è‡´ï¼‰ã§ã‚¹ãƒ©ã‚¤ãƒ‰ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã‚’æ¢ã™ã€‚
    """
    for layout in prs.slide_layouts:
        if layout.name == layout_name:
            return layout
    logger.warning(f"  -> '{layout_name}' ã«ä¸€è‡´ã™ã‚‹ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
    return None

def create_powerpoint_presentation(
    template_file: Optional[BytesIO],
    report_data: List[Dict[str, Any]],
    layout_map_names: Dict[str, str] # (â˜…) é¸æŠã•ã‚ŒãŸãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆåã‚’å—ã‘å–ã‚‹
) -> BytesIO:
    """
    (Step D) ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ(.pptx)ã¨ã‚¹ãƒ©ã‚¤ãƒ‰æ§‹æˆ(JSON)ã«åŸºã¥ãã€
    python-pptx ã‚’ä½¿ç”¨ã—ã¦æœ€çµ‚çš„ãªPowerPointãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆã™ã‚‹ã€‚
    (â˜…) ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒé¸æŠã—ãŸãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆåã‚’ä½¿ç”¨ã™ã‚‹ã‚ˆã†ãƒ­ã‚¸ãƒƒã‚¯ã‚’æ”¹ä¿®
    """
    logger.info("PowerPointç”Ÿæˆå‡¦ç† é–‹å§‹...")
    
    try:
        # (â˜…) 1. ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®èª­ã¿è¾¼ã¿
        if template_file:
            template_file.seek(0)
            prs = Presentation(template_file)
            logger.info("ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½¿ç”¨ã—ã¦PPTXã‚’ç”Ÿæˆã—ã¾ã™ã€‚")
        else:
            prs = Presentation()
            logger.info("ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½¿ç”¨ã—ã¦PPTXã‚’ç”Ÿæˆã—ã¾ã™ã€‚")

        # (â˜…) 2. ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒé¸æŠã—ãŸãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã‚’ãƒãƒƒãƒ”ãƒ³ã‚°
        layout_map = {
            "title_only": find_layout_by_name(prs, layout_map_names.get("title")),
            "agenda": find_layout_by_name(prs, layout_map_names.get("agenda")),
            "title_and_content": find_layout_by_name(prs, layout_map_names.get("content_text")),
            "text_and_image": find_layout_by_name(prs, layout_map_names.get("content_image")),
        }
        
        # (â˜…) ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ (æœ€ã‚‚æ±ç”¨çš„ãªã‚‚ã®)
        fallback_layout = prs.slide_layouts[1] # ã€Œã‚¿ã‚¤ãƒˆãƒ«ã¨ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€
        fallback_title_layout = prs.slide_layouts[0] # ã€Œã‚¿ã‚¤ãƒˆãƒ« ã‚¹ãƒ©ã‚¤ãƒ‰ã€
        
        if layout_map["title_only"] is None:
             layout_map["title_only"] = fallback_title_layout
             logger.warning("ã€Œè¡¨ç´™ã€ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã€Œã‚¿ã‚¤ãƒˆãƒ« ã‚¹ãƒ©ã‚¤ãƒ‰ã€ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
        if layout_map["agenda"] is None:
             layout_map["agenda"] = fallback_layout
             logger.warning("ã€Œç›®æ¬¡ã€ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã€Œã‚¿ã‚¤ãƒˆãƒ«ã¨ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
        if layout_map["title_and_content"] is None:
             layout_map["title_and_content"] = fallback_layout
             logger.warning("ã€Œãƒ†ã‚­ã‚¹ãƒˆã€ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã€Œã‚¿ã‚¤ãƒˆãƒ«ã¨ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
        if layout_map["text_and_image"] is None:
             layout_map["text_and_image"] = fallback_layout # ç”»åƒã‚ã‚Šã‚‚æœ€æ‚ªã“ã‚Œã§ä»£ç”¨
             logger.warning("ã€Œç”»åƒ+ãƒ†ã‚­ã‚¹ãƒˆã€ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã€Œã‚¿ã‚¤ãƒˆãƒ«ã¨ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")

        logger.info(f"ä½¿ç”¨ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆãƒãƒƒãƒ”ãƒ³ã‚°: {layout_map_names}")

        # (â˜…) 3. ã‚¹ãƒ©ã‚¤ãƒ‰ã®ç”Ÿæˆ (JSONãƒ‡ãƒ¼ã‚¿ã‚’ãƒ«ãƒ¼ãƒ—)
        
        # (â˜…) --- 3.1. è¡¨ç´™ã‚¹ãƒ©ã‚¤ãƒ‰ ---
        first_slide_data = report_data[0]
        if first_slide_data.get("slide_layout") == "title_only":
            slide = prs.slides.add_slide(layout_map["title_only"]) # (â˜…) é¸æŠã•ã‚ŒãŸãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ
            try:
                slide.shapes.title.text = first_slide_data.get("slide_title", "åˆ†æãƒ¬ãƒãƒ¼ãƒˆ")
            except: pass
            try:
                # (â˜…) å¤šãã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã§ã¯ Title Layout ã® Placeholder[1] ãŒ Subtitle
                if len(slide.placeholders) > 1 and slide.placeholders[1]:
                     slide.placeholders[1].text = first_slide_data.get("slide_content", [""])[0]
            except: pass
            
            report_data = report_data[1:]
        
        # (â˜…) --- 3.2. ç›®æ¬¡(Agenda)ã‚¹ãƒ©ã‚¤ãƒ‰ã®è‡ªå‹•ç”Ÿæˆ ---
        try:
            logger.info("ç›®æ¬¡ã‚¹ãƒ©ã‚¤ãƒ‰ã‚’è‡ªå‹•ç”Ÿæˆã—ã¾ã™...")
            agenda_slide = prs.slides.add_slide(layout_map["agenda"]) # (â˜…) é¸æŠã•ã‚ŒãŸãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ
            
            # (â˜…) ã‚¿ã‚¤ãƒˆãƒ«ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ã‚’æ¢ã™
            title_shape = None
            try:
                title_shape = agenda_slide.shapes.title
            except AttributeError:
                for shape in agenda_slide.placeholders:
                    # 0 or 100 (Title) or 136 (Center Title)
                    if shape.placeholder_format.idx == 0 or shape.placeholder_format.idx == 100 or shape.placeholder_format.idx == 136:
                        title_shape = shape
                        break
            if title_shape:
                title_shape.text = "æœ¬æ—¥ã®ã‚¢ã‚¸ã‚§ãƒ³ãƒ€"
            
            # (â˜…) ç›®æ¬¡ç”¨ã®æœ¬æ–‡ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ã‚’æ¢ã™
            agenda_body_shape = None
            for shape in agenda_slide.placeholders:
                 # 1 (Body) or 101 (Content)
                 if shape.placeholder_format.idx == 1 or shape.placeholder_format.idx == 101: 
                     agenda_body_shape = shape
                     break
            
            if agenda_body_shape:
                tf = agenda_body_shape.text_frame
                tf.clear()
                for i, slide_data in enumerate(report_data):
                    p = tf.add_paragraph()
                    p.text = f"{i+1}. {slide_data.get('slide_title', 'ï¼ˆç„¡é¡Œã®ã‚¹ãƒ©ã‚¤ãƒ‰ï¼‰')}"
                    p.level = 0
            else:
                 logger.warning("ç›®æ¬¡ã‚¹ãƒ©ã‚¤ãƒ‰ã®æœ¬æ–‡ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                 
        except Exception as e:
            logger.error(f"ç›®æ¬¡ã‚¹ãƒ©ã‚¤ãƒ‰ã®è‡ªå‹•ç”Ÿæˆã«å¤±æ•—: {e}")

        # (â˜…) --- 3.3. ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¹ãƒ©ã‚¤ãƒ‰ (æ®‹ã‚Š) ---
        for i, slide_data in enumerate(report_data):
            slide_title = slide_data.get("slide_title", f"ã‚¹ãƒ©ã‚¤ãƒ‰ {i+3}")
            slide_layout_key = slide_data.get("slide_layout", "title_and_content")
            slide_content = slide_data.get("slide_content", ["ï¼ˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãªã—ï¼‰"])
            image_base64 = slide_data.get("image_base664") # (â˜…) æ—¢å­˜ã‚³ãƒ¼ãƒ‰ã® typo ã‚’ä¿®æ­£ (664 -> 64)
            if image_base64 is None:
                image_base64 = slide_data.get("image_base64") # (â˜…) æ­£ã—ã„ã‚­ãƒ¼ã§ã‚‚å–å¾—

            if image_base64 and slide_layout_key == "title_and_content":
                slide_layout_key = "text_and_image"
            
            # (â˜…) ãƒãƒƒãƒ—ã‹ã‚‰ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã‚’å–å¾—
            if image_base64:
                layout_to_use = layout_map["text_and_image"]
            else:
                layout_to_use = layout_map["title_and_content"]
            
            slide = prs.slides.add_slide(layout_to_use)
            
            try:
                slide.shapes.title.text = slide_title
            except Exception as e:
                logger.warning(f"ã‚¹ãƒ©ã‚¤ãƒ‰ {i+3} ã®ã‚¿ã‚¤ãƒˆãƒ«è¨­å®šå¤±æ•—: {e}")

            # (â˜…) --- ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¨ç”»åƒã®é…ç½® (ãƒ­ã‚¸ãƒƒã‚¯ã‚’å …ç‰¢åŒ–) ---
            try:
                # (â˜…) ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ã‚’ã‚¿ã‚¤ãƒ—åˆ¥ã«åˆ†é¡
                text_placeholders = []
                image_placeholders = []
                
                for shape in slide.placeholders:
                    if shape.placeholder_format.idx == 0: continue # ã‚¿ã‚¤ãƒˆãƒ«ã¯é™¤å¤–
                    
                    if shape.has_text_frame:
                        text_placeholders.append(shape)
                    # (â˜…) ç”»åƒç”¨ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ (idx 101-107, 114-118 ãªã©) ã‚’æ¨æ¸¬
                    elif shape.placeholder_format.idx > 100: 
                        image_placeholders.append(shape)

                # (â˜…) ç”»åƒãŒã‚ã‚‹å ´åˆã®å‡¦ç† (text_and_image)
                if image_base64:
                    # (â˜…) 1. ãƒ†ã‚­ã‚¹ãƒˆã‚’æŒ¿å…¥ (æœ€åˆã«è¦‹ã¤ã‹ã£ãŸãƒ†ã‚­ã‚¹ãƒˆãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ã«)
                    if text_placeholders:
                        tf = text_placeholders[0].text_frame
                        tf.clear()
                        p = tf.paragraphs[0]
                        p.text = str(slide_content[0])
                        for item in slide_content[1:]:
                            p = tf.add_paragraph()
                            p.text = str(item)
                    
                    # (â˜…) 2. ç”»åƒã‚’æŒ¿å…¥ (æœ€åˆã«è¦‹ã¤ã‹ã£ãŸç”»åƒãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ã«)
                    image_ph = None
                    if image_placeholders:
                        image_ph = image_placeholders[0]
                    elif len(text_placeholders) > 1:
                        # (â˜…) ç”»åƒç”¨ãŒãªã‘ã‚Œã°ã€2ç•ªç›®ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ã‚’ä»£ç”¨
                        image_ph = text_placeholders[1] 

                    if image_ph:
                        try:
                            img_bytes = base64.b64decode(image_base64)
                            img_stream = BytesIO(img_bytes)
                            image_ph.insert_picture(img_stream)
                            logger.info(f"ã‚¹ãƒ©ã‚¤ãƒ‰ '{slide_title}': ã‚°ãƒ©ãƒ•ç”»åƒã®æŒ¿å…¥ã«æˆåŠŸã€‚")
                        except Exception as e:
                            logger.error(f"ã‚¹ãƒ©ã‚¤ãƒ‰ '{slide_title}': ã‚°ãƒ©ãƒ•ç”»åƒã®æŒ¿å…¥ã«å¤±æ•—: {e}")
                            if image_ph.has_text_frame:
                                image_ph.text_frame.text = f"ï¼ˆç”»åƒæŒ¿å…¥ã‚¨ãƒ©ãƒ¼: {e}ï¼‰"
                    else:
                         logger.warning(f"ã‚¹ãƒ©ã‚¤ãƒ‰ '{slide_title}': ç”»åƒç”¨ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")

                # (â˜…) ç”»åƒãŒãªã„å ´åˆã®å‡¦ç† (title_and_content)
                else:
                    if not text_placeholders:
                         logger.warning(f"ã‚¹ãƒ©ã‚¤ãƒ‰ '{slide_title}': ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
                         continue
                    tf = text_placeholders[0].text_frame
                    tf.clear()
                    p = tf.paragraphs[0]
                    p.text = str(slide_content[0])
                    for item in slide_content[1:]:
                        p = tf.add_paragraph()
                        p.text = str(item)

            except Exception as e:
                logger.error(f"ã‚¹ãƒ©ã‚¤ãƒ‰ {i+3} ('{slide_title}') ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„/ç”»åƒè¨­å®šä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)

        logger.info("PowerPointç”Ÿæˆå‡¦ç† å®Œäº†ã€‚")
        file_stream = BytesIO()
        prs.save(file_stream)
        file_stream.seek(0)
        return file_stream

    except Exception as e:
        logger.error(f"create_powerpoint_presentation å…¨ä½“ã§ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        st.error(f"PowerPointã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return None

def render_step_d():
    """(Step D) PowerPointç”ŸæˆUIã‚’æç”»ã™ã‚‹"""
    st.title(f"ãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ (PowerPoint) ç”Ÿæˆ (Step D)")

    # Step D å›ºæœ‰ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆ
    if 'step_d_template_file' not in st.session_state:
        st.session_state.step_d_template_file = None
    if 'step_d_report_data' not in st.session_state:
        st.session_state.step_d_report_data = []
    if 'step_d_generated_pptx' not in st.session_state:
        st.session_state.step_d_generated_pptx = None
    if 'step_d_layout_map' not in st.session_state:
        st.session_state.step_d_layout_map = {} # (â˜…) é¸æŠã•ã‚ŒãŸãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆåã‚’ä¿æŒ
    if 'tips_list' not in st.session_state:
        st.session_state.tips_list = []
    if 'current_tip_index' not in st.session_state:
        st.session_state.current_tip_index = 0
    if 'last_tip_time' not in st.session_state:
        st.session_state.last_tip_time = time.time()

    # --- 1. ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ (â˜…) ---
    st.header("Step 1: ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ PowerPoint ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
    st.info("ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰ä½¿ç”¨ã—ãŸã„ .pptx ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãŒã‚ã‚Œã°ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚ãªã‘ã‚Œã°ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‡ã‚¶ã‚¤ãƒ³ã§ç”Ÿæˆã•ã‚Œã¾ã™ã€‚")
    template_file = st.file_uploader(
        "PowerPoint ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ (.pptx)",
        type=['pptx'],
        key="step_d_template_uploader"
    )
    
    template_layout_names = []
    default_layouts = {
        "title": "ã‚¿ã‚¤ãƒˆãƒ« ã‚¹ãƒ©ã‚¤ãƒ‰",
        "agenda": "ã‚»ã‚¯ã‚·ãƒ§ãƒ³è¦‹å‡ºã—",
        "content_text": "ã‚¿ã‚¤ãƒˆãƒ«ã¨ã‚³ãƒ³ãƒ†ãƒ³ãƒ„",
        "content_image": "2ã¤ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„"
    }
    
    if template_file:
        try:
            # (â˜…) ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ™‚ã«ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆåã‚’èª­ã¿è¾¼ã‚€
            template_file.seek(0)
            template_bytes = template_file.getvalue()
            prs = Presentation(BytesIO(template_bytes))
            template_layout_names = [layout.name for layout in prs.slide_layouts]
            
            # (â˜…) ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãŒå¤‰æ›´ã•ã‚ŒãŸã‚‰ã€ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆãƒãƒƒãƒ—ã‚’ãƒªã‚»ãƒƒãƒˆã™ã‚‹ãŸã‚ã«Noneã‚’ã‚»ãƒƒãƒˆ
            if (st.session_state.step_d_template_file is None or 
                st.session_state.step_d_template_file.getvalue() != template_bytes):
                
                st.success(f"ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã€Œ{template_file.name}ã€ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")
                st.session_state.step_d_template_file = BytesIO(template_bytes)
                st.session_state.step_d_layout_map = {} # (â˜…) ãƒãƒƒãƒ—ã‚’ãƒªã‚»ãƒƒãƒˆ
            
        except Exception as e:
            st.error(f"ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
            template_layout_names = []
            st.session_state.step_d_template_file = None

    else:
        # (â˜…) ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚¯ãƒªã‚¢ã•ã‚ŒãŸã‚‰ãƒªã‚»ãƒƒãƒˆ
        if st.session_state.step_d_template_file is not None:
             st.session_state.step_d_template_file = None
             st.session_state.step_d_layout_map = {}


    # --- 2. Step C åˆ†æçµæœã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ ---
    st.header("Step 2: Step C åˆ†æãƒ¬ãƒãƒ¼ãƒˆ (JSON) ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
    st.info("Step C ã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã—ãŸ `report_for_powerpoint.json` ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
    report_file = st.file_uploader(
        "åˆ†æãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ« (report_for_powerpoint.json)",
        type=['json'],
        key="step_d_report_uploader"
    )

    if report_file:
        try:
            # (â˜…) è¾æ›¸ã®ãƒªã‚¹ãƒˆã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
            report_json_string = report_file.getvalue().decode('utf-8')
            report_data = json.loads(report_json_string)
            
            if isinstance(report_data, list) and all(isinstance(item, dict) for item in report_data):
                # (â˜…) ãƒ‡ãƒ¼ã‚¿ã®å‚ç…§ã‚’æ›´æ–°ã™ã‚‹
                if st.session_state.step_d_report_data != report_data:
                    st.success(f"åˆ†æãƒ¬ãƒãƒ¼ãƒˆã€Œ{report_file.name}ã€ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ ({len(report_data)}ã‚¹ãƒ©ã‚¤ãƒ‰)ã€‚")
                    st.session_state.step_d_report_data = report_data
            else:
                st.error("ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸJSONãŒæœŸå¾…ã™ã‚‹å½¢å¼ï¼ˆã‚¹ãƒ©ã‚¤ãƒ‰ã®ãƒªã‚¹ãƒˆï¼‰ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
                st.session_state.step_d_report_data = []
        except Exception as e:
            logger.error(f"Step D JSONãƒ¬ãƒãƒ¼ãƒˆèª­è¾¼ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            st.error(f"åˆ†æãƒ¬ãƒãƒ¼ãƒˆã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            st.session_state.step_d_report_data = []
    
    if not st.session_state.step_d_report_data:
        st.session_state.step_d_report_data = []
        st.session_state.step_d_generated_pptx = None
        st.warning("PowerPointã‚’ç”Ÿæˆã™ã‚‹ã«ã¯ã€Step C ã§ç”Ÿæˆã—ãŸ JSON ãƒ¬ãƒãƒ¼ãƒˆã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
        return

    # --- 3. (â˜…) ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆå‰²ã‚Šå½“ã¦ ---
    st.header("Step 3: ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã®å‰²ã‚Šå½“ã¦")
    
    if not st.session_state.step_d_template_file:
        st.info("Step 1 ã§ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã¨ã€ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆåã‚’é¸æŠã§ãã¾ã™ã€‚ï¼ˆç¾åœ¨ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šï¼‰")
        layout_options = list(default_layouts.values()) # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆåã‚’è¡¨ç¤º
    else:
        st.info("ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‹ã‚‰èª­ã¿è¾¼ã‚“ã ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆåã‚’ã€å„ã‚¹ãƒ©ã‚¤ãƒ‰ã‚¿ã‚¤ãƒ—ã«å‰²ã‚Šå½“ã¦ã¦ãã ã•ã„ã€‚")
        layout_options = template_layout_names
        
    if not layout_options:
         st.error("ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
         layout_options = list(default_layouts.values())

    # (â˜…) ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ¢ã™ãƒ˜ãƒ«ãƒ‘ãƒ¼
    def get_default_index(default_name_key):
        # (â˜…) 1. ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã«ä¿å­˜ã•ã‚ŒãŸå€¤ãŒã‚ã‚Œã°ãã‚Œã‚’å„ªå…ˆ
        if default_name_key in st.session_state.step_d_layout_map:
            saved_name = st.session_state.step_d_layout_map[default_name_key]
            if saved_name in layout_options:
                return layout_options.index(saved_name)
        
        # (â˜…) 2. ãªã‘ã‚Œã°ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆåï¼ˆ"ã‚¿ã‚¤ãƒˆãƒ« ã‚¹ãƒ©ã‚¤ãƒ‰" ãªã©ï¼‰ã‚’æ¢ã™
        target_name = default_layouts[default_name_key]
        if target_name in layout_options:
            return layout_options.index(target_name)
            
        # (â˜…) 3. ãã‚Œã‚‚ãªã‘ã‚Œã°ã€éƒ¨åˆ†ä¸€è‡´ï¼ˆ"ã‚¿ã‚¤ãƒˆãƒ«"ãªã©ï¼‰ã§æ¢ã™
        for i, opt in enumerate(layout_options):
            if default_name_key in opt.lower(): # "title"
                return i
            if target_name.split(' ')[0] in opt: # "ã‚¿ã‚¤ãƒˆãƒ«"
                return i

        return 0 # è¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã°å…ˆé ­

    layout_map = {}
    col1, col2 = st.columns(2)
    with col1:
        layout_map["title"] = st.selectbox(
            "1. è¡¨ç´™ (Title) ã‚¹ãƒ©ã‚¤ãƒ‰:", layout_options, 
            index=get_default_index("title"), key="layout_select_title"
        )
        layout_map["agenda"] = st.selectbox(
            "2. ç›®æ¬¡ (Agenda) ã‚¹ãƒ©ã‚¤ãƒ‰:", layout_options, 
            index=get_default_index("agenda"), key="layout_select_agenda"
        )
    with col2:
        layout_map["content_text"] = st.selectbox(
            "3. åˆ†æ (ãƒ†ã‚­ã‚¹ãƒˆã®ã¿) ã‚¹ãƒ©ã‚¤ãƒ‰:", layout_options, 
            index=get_default_index("content_text"), key="layout_select_text"
        )
        layout_map["content_image"] = st.selectbox(
            "4. åˆ†æ (ãƒ†ã‚­ã‚¹ãƒˆ+ç”»åƒ) ã‚¹ãƒ©ã‚¤ãƒ‰:", layout_options, 
            index=get_default_index("content_image"), key="layout_select_image"
        )
    
    # (â˜…) é¸æŠçµæœã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã«å³æ™‚ä¿å­˜
    st.session_state.step_d_layout_map = layout_map


    # --- 4. ã‚¹ãƒ©ã‚¤ãƒ‰æ§‹æˆã®ç·¨é›† (â˜… æ—§Step 3) ---
    st.header("Step 4: ã‚¹ãƒ©ã‚¤ãƒ‰æ§‹æˆã®ç¢ºèªãƒ»ç·¨é›†")
    st.info("ï¼ˆ(â˜…) ãƒã‚¦ã‚¹ã®ãƒ‰ãƒ©ãƒƒã‚°ï¼†ãƒ‰ãƒ­ãƒƒãƒ—ã§ã‚¹ãƒ©ã‚¤ãƒ‰ã®é †ç•ªã‚’å…¥ã‚Œæ›¿ãˆã‚‹ã“ã¨ãŒã§ãã¾ã™ï¼‰")

    try:
        headers_list = []
        header_to_item_map = {}
        
        if not st.session_state.step_d_report_data:
             st.warning("JSONãƒ‡ãƒ¼ã‚¿ãŒç©ºã‹ã€æ­£ã—ãèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
             return

        for i, item in enumerate(st.session_state.step_d_report_data):
            if not isinstance(item, dict):
                st.error(f"ãƒ‡ãƒ¼ã‚¿å½¢å¼ã‚¨ãƒ©ãƒ¼: {item} ã¯è¾æ›¸ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
                continue
            
            title = item.get('slide_title', 'ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ãªã—ï¼‰')
            layout = item.get('slide_layout', 'N/A')
            has_image = "ğŸ–¼ï¸" if (item.get("image_base64") or item.get("image_base664")) else "ğŸ“„" # (â˜…) Typoä¿®æ­£
            header_str = f"**{i+1}: {title}** (Layout: `{layout}`, {has_image})"
            headers_list.append(header_str)
            header_to_item_map[header_str] = item

        if not all(isinstance(h, str) for h in headers_list):
            st.error("å†…éƒ¨ã‚¨ãƒ©ãƒ¼: ãƒ˜ãƒƒãƒ€ãƒ¼ãƒªã‚¹ãƒˆã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
            return

        sorted_headers = sort_items(
            items=headers_list,
            key="sortable_slides_v4" # (â˜…) ã‚­ãƒ¼ã‚’æ›´æ–°
        )
        
        cleaned_sorted_data = []
        for header in sorted_headers:
            if header in header_to_item_map:
                cleaned_sorted_data.append(header_to_item_map[header])
            else:
                logger.error(f"ãƒãƒƒãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: ã‚½ãƒ¼ãƒˆå¾Œã®ãƒ˜ãƒƒãƒ€ãƒ¼ '{header}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            
        st.session_state.step_d_report_data = cleaned_sorted_data
        
    except Exception as e:
        logger.error(f"streamlit-sortables å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        st.error(f"ã‚¹ãƒ©ã‚¤ãƒ‰ç·¨é›†UIã®æç”»ã«å¤±æ•—: {e}ã€‚")


    # --- 5. AIã«ã‚ˆã‚‹ä¿®æ­£æŒ‡ç¤º (â˜… æ—§Step 4) ---
    st.header("Step 5: (ã‚ªãƒ—ã‚·ãƒ§ãƒ³) AIã«ã‚ˆã‚‹å†…å®¹ã®ä¿®æ­£æŒ‡ç¤º")
    st.markdown(f"ï¼ˆ(â˜…) ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: `{MODEL_PRO}`ï¼‰")
    
    with st.expander("AIã«ã‚¹ãƒ©ã‚¤ãƒ‰å†…å®¹ã®ä¿®æ­£ã‚’æŒ‡ç¤ºã™ã‚‹"):
        correction_prompt = st.text_area(
            "ä¿®æ­£å†…å®¹ã‚’å…·ä½“çš„ã«æŒ‡ç¤ºã—ã¦ãã ã•ã„:",
            placeholder=(
                "ä¾‹: ã€Œã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ãƒ»ã‚µãƒãƒªãƒ¼ã€ã‚¹ãƒ©ã‚¤ãƒ‰ã®ç®‡æ¡æ›¸ãã‚’3ç‚¹ã«è¦ç´„ã—ã¦ã€‚\n"
                "ä¾‹: ã€Œå…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã€ã‚¹ãƒ©ã‚¤ãƒ‰ã‚’å‰Šé™¤ã—ã¦ã€‚"
            ),
            key="step_d_correction_prompt"
        )
        
        if st.button("AIã§ã‚¹ãƒ©ã‚¤ãƒ‰æ§‹æˆã‚’ä¿®æ­£", key="run_ai_correction_D", type="secondary"):
            if correction_prompt.strip():
                with st.spinner(f"AI ({MODEL_PRO}) ãŒã‚¹ãƒ©ã‚¤ãƒ‰æ§‹æˆ (JSON) ã‚’ä¿®æ­£ä¸­..."):
                    current_json_str = json.dumps(st.session_state.step_d_report_data, ensure_ascii=False)
                    corrected_json_str = run_step_d_ai_correction(current_json_str, correction_prompt)
                    
                    try:
                        corrected_data = json.loads(corrected_json_str)
                        if isinstance(corrected_data, list):
                            st.session_state.step_d_report_data = corrected_data
                            st.success("AIã«ã‚ˆã‚‹ã‚¹ãƒ©ã‚¤ãƒ‰æ§‹æˆã®ä¿®æ­£ãŒå®Œäº†ã—ã¾ã—ãŸã€‚Step 4 ã®æ§‹æˆãŒæ›´æ–°ã•ã‚Œã¦ã„ã¾ã™ã€‚")
                            st.rerun() # UIã‚’å³æ™‚æ›´æ–°
                        else:
                            st.error("AIãŒãƒªã‚¹ãƒˆå½¢å¼ã§ãªã„ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã—ã¾ã—ãŸã€‚ä¿®æ­£ã¯ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸã€‚")
                    except Exception as e:
                        st.error(f"AIã®å›ç­”ã®ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—: {e}ã€‚ä¿®æ­£ã¯ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸã€‚")
            else:
                st.warning("ä¿®æ­£æŒ‡ç¤ºã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

    # --- 6. PowerPointç”Ÿæˆ (â˜… æ—§Step 5) ---
    st.header("Step 6: PowerPointã®ç”Ÿæˆã¨ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ")
    
    tip_placeholder_d = st.empty()
    
    if st.button("PowerPointã‚’ç”Ÿæˆ (Step 6)", key="generate_pptx_D", type="primary", use_container_width=True):
        st.session_state.step_d_generated_pptx = None
        
        if not st.session_state.tips_list or len(st.session_state.tips_list) <= 1:
            with st.spinner("åˆ†æTIPSã‚’AIã§ç”Ÿæˆä¸­..."):
                st.session_state.tips_list = get_analysis_tips_list_from_ai()
                if st.session_state.tips_list:
                    st.session_state.current_tip_index = random.randint(0, len(st.session_state.tips_list) - 1)
                    st.session_state.last_tip_time = time.time()
        
        with st.spinner("PowerPointãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆä¸­..."):
            now = time.time()
            if (now - st.session_state.last_tip_time > 10):
                if len(st.session_state.tips_list) > 1:
                    st.session_state.current_tip_index = (st.session_state.current_tip_index + 1) % len(st.session_state.tips_list)
                st.session_state.last_tip_time = now
            if st.session_state.tips_list:
                current_tip = st.session_state.tips_list[st.session_state.current_tip_index]
                tip_placeholder_d.info(f"ğŸ’¡ ãƒ‡ãƒ¼ã‚¿åˆ†æTIPS: {current_tip}")

            # (â˜…) --- ä¿®æ­£: é¸æŠã•ã‚ŒãŸãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆãƒãƒƒãƒ—ã‚’æ¸¡ã™ ---
            generated_file_stream = create_powerpoint_presentation(
                st.session_state.step_d_template_file,
                st.session_state.step_d_report_data,
                st.session_state.step_d_layout_map # (â˜…) ã“ã“ã§æ¸¡ã™
            )
            
            tip_placeholder_d.empty()
            
            if generated_file_stream:
                st.session_state.step_d_generated_pptx = generated_file_stream.getvalue()
                st.success("PowerPointãƒ•ã‚¡ã‚¤ãƒ«ã®ç”ŸæˆãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
            else:
                st.error("PowerPointãƒ•ã‚¡ã‚¤ãƒ«ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚")

    if st.session_state.step_d_generated_pptx:
        st.download_button(
            label="ç”Ÿæˆã•ã‚ŒãŸ PowerPoint ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            data=st.session_state.step_d_generated_pptx,
            file_name="AI_Analysis_Report_v3.pptx", # (â˜…) v3
            mime="application/vnd.openxmlformats-officedocument.presentationml.presentation",
            use_container_width=True
        )
        st.balloons()


# --- 11. (â˜…) Mainé–¢æ•° (ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ) ---
def main():
    """Streamlitã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    
    # (â˜…) --- st.set_page_config() ã‚’æœ€åˆã«å®Ÿè¡Œ ---
    st.set_page_config(page_title="AI Data Analysis App", layout="wide")
    
    # (â˜…) --- ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã®åˆæœŸåŒ–ã‚’ *å…¨ã¦* ã“ã“ã«ç§»å‹• ---
    if 'log_messages' not in st.session_state:
        st.session_state.log_messages = []
    if 'current_step' not in st.session_state:
        st.session_state.current_step = 'A' # åˆæœŸã‚¹ãƒ†ãƒƒãƒ—
    if 'tips_list' not in st.session_state:
        st.session_state.tips_list = []
    if 'current_tip_index' not in st.session_state:
        st.session_state.current_tip_index = 0
    if 'last_tip_time' not in st.session_state:
        st.session_state.last_tip_time = time.time()
    # (â˜…) --- ã“ã“ã¾ã§ãŒä¿®æ­£ç‚¹ ---

    # (â˜…) --- .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã‚€ ---
    try:
        load_dotenv()
        logger.info(".env ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿è©¦è¡Œå®Œäº†ã€‚")
    except Exception as e:
        logger.warning(f".env ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}") 
    
    # --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ (ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³) ---
    with st.sidebar:
        st.title("AI ãƒ¬ãƒãƒ¼ãƒ†ã‚£ãƒ³ã‚° App")
        st.markdown("---")
        
        st.header("âš™ï¸ AI è¨­å®š")
        
        if not os.getenv("GOOGLE_API_KEY"):
            st.warning(
                "Google APIã‚­ãƒ¼ãŒ.envãƒ•ã‚¡ã‚¤ãƒ«ã«è¨­å®šã•ã‚Œã¦ã„ãªã„ã‹ã€èª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸã€‚\n\n"
                "(.envãƒ•ã‚¡ã‚¤ãƒ«ã« `GOOGLE_API_KEY='ã‚ãªãŸã®APIã‚­ãƒ¼'` ã¨è¨˜è¼‰ã—ã¦ãã ã•ã„)"
            )
        else:
            st.success("Google APIã‚­ãƒ¼ èª­è¾¼å®Œäº†")
            # (â˜…) ã‚¢ãƒ—ãƒªèµ·å‹•æ™‚ã«LLMã¨spaCyã®ãƒ­ãƒ¼ãƒ‰ã‚’è©¦ã¿ã‚‹
            if 'llm_checked' not in st.session_state:
                if get_llm(MODEL_FLASH_LITE) is None: 
                    st.error("LLMã®åˆæœŸåŒ–ã«å¤±æ•—ã€‚APIã‚­ãƒ¼ãŒæ­£ã—ã„ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
                if load_spacy_model() is None:
                    st.error("spaCyãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã€‚")
                st.session_state.llm_checked = True # (â˜…) æ¯ãƒªãƒ©ãƒ³æ™‚ã«ãƒã‚§ãƒƒã‚¯ã—ãªã„ã‚ˆã†
        
        st.markdown("---")
        
        # (â˜…) --- Step Aã€œD ã®ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ãƒœã‚¿ãƒ³ ---
        st.header("ğŸ”„ ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³")
        current_step = st.session_state.current_step
        
        if st.button(
            "Step A: AIã‚¿ã‚°ä»˜ã‘", key="nav_A", use_container_width=True,
            type=("primary" if current_step == 'A' else "secondary")
        ):
            if st.session_state.current_step != 'A':
                st.session_state.current_step = 'A'; st.rerun()

        if st.button(
            "Step B: åˆ†æå®Ÿè¡Œ", key="nav_B", use_container_width=True,
            type=("primary" if current_step == 'B' else "secondary")
        ):
            if st.session_state.current_step != 'B':
                st.session_state.current_step = 'B'; st.rerun()

        if st.button(
            "Step C: AIãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ", key="nav_C", use_container_width=True,
            type=("primary" if current_step == 'C' else "secondary")
        ):
            if st.session_state.current_step != 'C':
                st.session_state.current_step = 'C'; st.rerun()

        if st.button(
            "Step D: PowerPointç”Ÿæˆ", key="nav_D", use_container_width=True,
            type=("primary" if current_step == 'D' else "secondary")
        ):
            if st.session_state.current_step != 'D':
                st.session_state.current_step = 'D'; st.rerun()
                
        # (â˜…) --- ä¿®æ­£: ã‚°ãƒ­ãƒ¼ãƒãƒ«Tipsã®ç”Ÿæˆãƒˆãƒªã‚¬ãƒ¼ã‚’å‰Šé™¤ ---
        # st.markdown("---")
        # if st.button("åˆ†æTIPSã‚’æ›´æ–°", key="reload_tips", use_container_width=True):
        #      ... (ãƒ–ãƒ­ãƒƒã‚¯å…¨ä½“ã‚’å‰Šé™¤) ...
        # (â˜…) --- ã“ã“ã¾ã§ãŒä¿®æ­£ç‚¹ ---

    # (â˜…) æ—¢å­˜ã® main() é–¢æ•°ã®ãƒ­ã‚¸ãƒƒã‚¯ (ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆåˆæœŸåŒ–) ã‚’ç§»å‹•
    if 'llm_checked' not in st.session_state:
        if os.getenv("GOOGLE_API_KEY"):
            if get_llm(MODEL_FLASH_LITE) is None: 
                pass # (â˜…) ã‚¨ãƒ©ãƒ¼ã¯ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§è¡¨ç¤º
            if load_spacy_model() is None:
                pass # (â˜…) ã‚¨ãƒ©ãƒ¼ã¯ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§è¡¨ç¤º
        st.session_state.llm_checked = True


    # --- ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ (ã‚¹ãƒ†ãƒƒãƒ—ã«å¿œã˜ã¦æç”») ---
    if st.session_state.current_step == 'A':
        render_step_a()
    elif st.session_state.current_step == 'B':
        render_step_b()
    elif st.session_state.current_step == 'C':
        render_step_c()
    elif st.session_state.current_step == 'D':
        render_step_d()
    else:
        st.error("ä¸æ˜ãªã‚¹ãƒ†ãƒƒãƒ—ã§ã™ã€‚Step Aã«æˆ»ã‚Šã¾ã™ã€‚")
        st.session_state.current_step = 'A'; st.rerun()

if __name__ == "__main__":
    main()