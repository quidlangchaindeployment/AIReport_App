import streamlit as st
import pandas as pd
import numpy as np
import spacy
from logging import getLogger, StreamHandler, DEBUG, Formatter, Handler, LogRecord
import re
from io import BytesIO
import os
import json
import sys
import time
from collections import deque

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import PromptTemplate
from langchain.schema.output_parser import StrOutputParser

try:
    from geography_db import JAPAN_GEOGRAPHY_DB
except ImportError:
    JAPAN_GEOGRAPHY_DB = None

# --- ãƒ­ã‚¬ãƒ¼è¨­å®š ---
logger = getLogger(__name__)
logger.setLevel(DEBUG)

# --- UIãƒ­ã‚°è¡¨ç¤ºç”¨ã‚«ã‚¹ã‚¿ãƒ ãƒãƒ³ãƒ‰ãƒ© ---
class StreamlitLogHandler(Handler):
    def __init__(self, max_lines=15):
        super().__init__()
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆ 'log_messages' ãŒãªã‘ã‚Œã°åˆæœŸåŒ–
        if 'log_messages' not in st.session_state:
            st.session_state.log_messages = deque(maxlen=max_lines)

    def emit(self, record: LogRecord):
        log_entry = self.format(record)
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆ 'log_messages' ãŒã‚ã‚Œã°è¿½è¨˜
        if 'log_messages' in st.session_state:
            st.session_state.log_messages.append(log_entry)
# ------------------------------------

# --- LLMåˆæœŸåŒ–é–¢æ•° ---
def get_llm():
    if not os.getenv("GOOGLE_API_KEY"):
        print("ERROR: Google APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚", file=sys.stderr)
        return None
    try:
        return ChatGoogleGenerativeAI(model="gemini-2.5-flash-lite", temperature=0.0)
    except Exception as e:
        print(f"LLM (gemini-2.5-flash-lite) ã®åˆæœŸåŒ–ã«å¤±æ•—: {e}", file=sys.stderr)
        # ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚ã¯ãƒ­ã‚°ã«è©³ç´°ã‚’è¨˜éŒ² (UIã«ã¯è¡¨ç¤ºã—ãªã„)
        logger.error(f"LLM (gemini-2.5-flash-lite) initialization failed: {e}", exc_info=True)
        return None

# --- spaCyãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ ---
@st.cache_resource # Streamlitã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½ã‚’åˆ©ç”¨
def load_spacy_model():
    """spaCyã®æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
    try:
        nlp = spacy.load("ja_core_news_sm") # ãƒ¢ãƒ‡ãƒ«åã‚’ç¢ºèª
    except OSError:
        st.info("åˆå›èµ·å‹•: æ—¥æœ¬èªNLPãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™...")
        try:
            from spacy.cli import download
            download("ja_core_news_sm")
            nlp = spacy.load("ja_core_news_sm")
        except Exception as e:
            st.error(f"spaCyãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰/ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—: {e}")
            logger.error(f"spaCy model download/load failed: {e}", exc_info=True)
            nlp = None # å¤±æ•—ã—ãŸå ´åˆã¯Noneã‚’è¿”ã™
    return nlp


# --- ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ & å‰å‡¦ç†ãƒ„ãƒ¼ãƒ« ---
def load_and_preprocess_data(uploaded_file, text_column):
    """ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸã€Œ1ã¤ã®ã€ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€å‰å‡¦ç†ã‚’è¡Œã†"""
    logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ« {uploaded_file.name} èª­è¾¼ãƒ»å‰å‡¦ç†é–‹å§‹...")
    df = None
    try:
        uploaded_file.seek(0) # ãƒ•ã‚¡ã‚¤ãƒ«ãƒã‚¤ãƒ³ã‚¿ã‚’å…ˆé ­ã«
        if uploaded_file.name.endswith('.csv'):
            df = pd.read_csv(uploaded_file, encoding="utf-8-sig")
        elif uploaded_file.name.endswith(('.xls', '.xlsx')):
            df = pd.read_excel(uploaded_file)
        else:
            st.warning(f"ã‚µãƒãƒ¼ãƒˆå¤–å½¢å¼: {uploaded_file.name}"); return None, None
    except Exception as e:
        st.error(f"ãƒ•ã‚¡ã‚¤ãƒ« {uploaded_file.name} èª­è¾¼å¤±æ•—: {e}"); logger.error(f"File reading failed: {e}", exc_info=True); return None, None # ãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯è¿½åŠ 

    if df is None: st.error(f"ãƒ•ã‚¡ã‚¤ãƒ« {uploaded_file.name} èª­è¾¼å¤±æ•—"); return None, None
    if text_column not in df.columns: st.error(f"ãƒ•ã‚¡ã‚¤ãƒ« {uploaded_file.name} ã«åˆ— '{text_column}' ãªã—"); return None, None

    # ãƒ†ã‚­ã‚¹ãƒˆåˆ—ã®æ¬ æå€¤å‡¦ç†ã¨ç©ºè¡Œãƒã‚§ãƒƒã‚¯
    df.dropna(subset=[text_column], inplace=True)
    if df.empty: logger.warning(f"{uploaded_file.name} ã¯ãƒ†ã‚­ã‚¹ãƒˆåˆ— '{text_column}' ã®æ¬ æå€¤é™¤å»å¾Œã€0è¡Œ"); return None, []

    df = df.reset_index(drop=True)
    metadata_columns = [col for col in df.columns if col != text_column]

    # å‹å¤‰æ› (ã‚¨ãƒ©ãƒ¼ã‚’ç„¡è¦–ã—ã€æ—¥ä»˜å¤‰æ›ã¯è©¦è¡Œ)
    for col in metadata_columns:
        df[col] = pd.to_numeric(df[col], errors='ignore')
        if df[col].dtype == 'object':
            try: df[col] = pd.to_datetime(df[col])
            except (ValueError, TypeError, OverflowError): pass # ã‚¨ãƒ©ãƒ¼ãŒå‡ºã¦ã‚‚å‡¦ç†ã‚’ç¶šã‘ã‚‹

    logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ« {uploaded_file.name} å‡¦ç†å®Œäº† ({len(df)} è¡Œ)")
    return df, metadata_columns


# --- ãƒ‡ãƒ¼ã‚¿æ§‹é€ åˆ†æãƒ„ãƒ¼ãƒ« ---
def analyze_data_structure(df):
    """DataFrameã®æ§‹é€ ã‚’åˆ†æã—ã€ã‚µãƒãƒªãƒ¼ã‚’ç”Ÿæˆã™ã‚‹"""
    logger.info("ãƒ‡ãƒ¼ã‚¿æ§‹é€ åˆ†æé–‹å§‹...")
    if df is None or df.empty:
        logger.warning("åˆ†æå¯¾è±¡ã®DataFrameãŒç©ºã§ã™ã€‚")
        return {"total_rows": 0, "total_columns": 0, "column_details": {}}

    structure_info = {"total_rows": len(df), "total_columns": len(df.columns)}
    column_details = {}
    for col in df.columns:
        try: # åˆ—ã”ã¨ã®å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚ç¶™ç¶š
             column_details[col] = {"type": str(df[col].dtype), "unique_values": df[col].nunique(), "missing_values": int(df[col].isnull().sum())} # int()è¿½åŠ 
        except Exception as e:
             logger.error(f"åˆ— '{col}' ã®åˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
             column_details[col] = {"type": "Error", "unique_values": -1, "missing_values": -1}

    structure_info["column_details"] = column_details
    logger.info("ãƒ‡ãƒ¼ã‚¿æ§‹é€ åˆ†æå®Œäº†")
    return structure_info

# --- AIã«ã‚ˆã‚‹å‹•çš„ã‚«ãƒ†ã‚´ãƒªå®šç¾© ---
def get_dynamic_categories(analysis_prompt, llm):
    """LLMã‚’ä½¿ã„ã€æŒ‡é‡ã‹ã‚‰ã€ŒæŠ½å‡ºã™ã¹ãã‚«ãƒ†ã‚´ãƒªã€ã®ã‚¹ã‚­ãƒ¼ãƒã‚’å‹•çš„ã«ç”Ÿæˆã™ã‚‹"""
    logger.info("AIå‹•çš„ã‚«ãƒ†ã‚´ãƒªå®šç¾©ç”Ÿæˆé–‹å§‹...")
    if llm is None: return {"å¸‚åŒºç”ºæ‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰": "ãƒ†ã‚­ã‚¹ãƒˆãŒè¨€åŠã—ã¦ã„ã‚‹æ—¥æœ¬ã®å¸‚åŒºç”ºæ‘åã€‚"} # LLMãŒãªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã¿
    prompt = PromptTemplate.from_template(
        """
        ã‚ãªãŸã¯ãƒ‡ãƒ¼ã‚¿åˆ†æã®ã‚¹ã‚­ãƒ¼ãƒè¨­è¨ˆè€…ã§ã™ã€‚ã€Œåˆ†ææŒ‡é‡ã€ã‚’èª­ã¿ã€ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æŠ½å‡ºã™ã‚‹ã¹ãã€Œãƒˆãƒ”ãƒƒã‚¯ã®ã‚«ãƒ†ã‚´ãƒªã€ã‚’è€ƒæ¡ˆã—ã¦ãã ã•ã„ã€‚ã€Œå¸‚åŒºç”ºæ‘ã€ã¯å¿…é ˆã‚«ãƒ†ã‚´ãƒªã¨ã—ã¦è‡ªå‹•ã§è¿½åŠ ã•ã‚Œã‚‹ãŸã‚ã€ãã‚Œä»¥å¤–ã®ã‚«ãƒ†ã‚´ãƒªã‚’å®šç¾©ã—ã¦ãã ã•ã„ã€‚
        # æŒ‡ç¤º: 1.ã€Œåˆ†ææŒ‡é‡ã€ã®ãƒˆãƒ”ãƒƒã‚¯ã‚’ã‚«ãƒ†ã‚´ãƒªåŒ– 2.å„ã‚«ãƒ†ã‚´ãƒªã®èª¬æ˜è¨˜è¿° 3.å³æ ¼ãªJSONè¾æ›¸å‡ºåŠ› 4.åœ°åã‚«ãƒ†ã‚´ãƒªç¦æ­¢ 5.è©²å½“ãªã‘ã‚Œã°ç©ºJSON
        # åˆ†ææŒ‡é‡:{user_prompt}
        # å›ç­” (JSONè¾æ›¸å½¢å¼):
        """
    )
    chain = prompt | llm | StrOutputParser()
    try:
        response_str = chain.invoke({"user_prompt": analysis_prompt})
        logger.info(f"AIã‚«ãƒ†ã‚´ãƒªå®šç¾©(ç”Ÿ): {response_str}")
        match = re.search(r'\{.*\}', response_str, re.DOTALL)
        if not match: raise ValueError("Invalid JSON format")
        json_str = match.group(0); category_map = json.loads(json_str)
        if not isinstance(category_map, dict): raise ValueError("Response is not a dict")
        final_categories = {"å¸‚åŒºç”ºæ‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰": "ãƒ†ã‚­ã‚¹ãƒˆãŒè¨€åŠã—ã¦ã„ã‚‹æ—¥æœ¬ã®å¸‚åŒºç”ºæ‘åã€‚"}
        # (â˜…) AIãŒç”Ÿæˆã—ãŸã‚­ãƒ¼ã®ç©ºç™½ã‚’é™¤å»
        cleaned_category_map = {k.strip(): v for k, v in category_map.items()}
        final_categories.update(cleaned_category_map)
        logger.info(f"æœ€çµ‚ã‚«ãƒ†ã‚´ãƒªå®šç¾©: {final_categories}")
        return final_categories
    except Exception as e:
        logger.error(f"AIå‹•çš„ã‚«ãƒ†ã‚´ãƒªå®šç¾©å¤±æ•—: {e}", exc_info=True)
        st.warning(f"AIå‹•çš„ã‚«ãƒ†ã‚´ãƒªå®šç¾©å¤±æ•—: {e}")
        return {"å¸‚åŒºç”ºæ‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰": "ãƒ†ã‚­ã‚¹ãƒˆãŒè¨€åŠã—ã¦ã„ã‚‹æ—¥æœ¬ã®å¸‚åŒºç”ºæ‘åã€‚"}
# AIãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãƒ­ã‚¸ãƒƒã‚¯
def filter_relevant_data_by_ai(df_batch, analysis_prompt, llm):
    """
    AIã‚’ä½¿ã„ã€åˆ†ææŒ‡é‡ã¨ç„¡é–¢ä¿‚ãªè¡Œã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹ (relevant: true/false)ã€‚
    """
    if llm is None:
        logger.error("filter_relevant_data_by_ai: LLMãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        st.error("AIãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚APIã‚­ãƒ¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        return pd.DataFrame() # ç©ºã®DF (ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¤±æ•—)

    logger.debug(f"{len(df_batch)}ä»¶ AIé–¢é€£æ€§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°é–‹å§‹...")
    
    # ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’JSONLå½¢å¼ã«å¤‰æ› (IDã¨ãƒ†ã‚­ã‚¹ãƒˆã®ã¿)
    input_texts_jsonl = df_batch.apply(lambda row: json.dumps({"id": row['id'], "text": str(row['ANALYSIS_TEXT_COLUMN'])[:500]}, ensure_ascii=False), axis=1).tolist()
    
    prompt = PromptTemplate.from_template(
        """
        ã‚ãªãŸã¯ãƒ‡ãƒ¼ã‚¿åˆ†æã®ã‚­ãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼ã§ã™ã€‚ã€Œåˆ†ææŒ‡é‡ã€ã«åŸºã¥ãã€ã€Œãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿(JSONL)ã€ã®å„è¡ŒãŒåˆ†æå¯¾è±¡ã¨ã—ã¦ã€é–¢é€£ã—ã¦ã„ã‚‹ã‹ (relevant: true)ã€‘ã€ã€ç„¡é–¢ä¿‚ã‹ (relevant: false)ã€‘ã‚’åˆ¤å®šã—ã¦ãã ã•ã„ã€‚

        # åˆ†ææŒ‡é‡ (Analysis Scope):
        {analysis_prompt}

        # æŒ‡ç¤º:
        1. ã€Œåˆ†ææŒ‡é‡ã€ã¨ã€å¼·ãé–¢é€£ã€‘ã™ã‚‹æŠ•ç¨¿ã®ã¿ã‚’ `true` ã¨ã™ã‚‹ã€‚
        2. å˜ãªã‚‹å®£ä¼ï¼ˆä¾‹: "ã‚»ãƒ¼ãƒ«é–‹å‚¬ä¸­ï¼"ï¼‰ã€æŒ¨æ‹¶ã®ã¿ï¼ˆä¾‹: "ã‚ã‘ã¾ã—ã¦ãŠã‚ã§ã¨ã†"ï¼‰ã€æŒ‡é‡ã¨ç„¡é–¢ä¿‚ãªåœ°åŸŸã®è¨€åŠï¼ˆä¾‹: æŒ‡é‡ãŒã€Œåºƒå³¶ã€ãªã®ã«ã€ŒåŒ—æµ·é“ã€ã®è©±ã®ã¿ï¼‰ã¯ `false` ã¨ã™ã‚‹ã€‚
        3. å‡ºåŠ›ã¯ã€JSONLå½¢å¼ã®ã¿ã€‘ï¼ˆid ã¨ relevant (boolean) ã‚’å«ã‚€è¾æ›¸ï¼‰ã€‚

        # ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ (JSONL):
        {text_data_jsonl}

        # å›ç­” (JSONLå½¢å¼ã®ã¿):
        """
    )
    chain = prompt | llm | StrOutputParser()
    try:
        invoke_params = {
            "analysis_prompt": analysis_prompt,
            "text_data_jsonl": "\n".join(input_texts_jsonl)
        }
        
        logger.debug(f"AI Filtering - Invoking LLM...")
        response_str = chain.invoke(invoke_params)
        logger.debug(f"AI Filtering - Raw response received.")
        
        results = []
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ (JSONL) ã®ãƒ‘ãƒ¼ã‚¹
        match = re.search(r'```(?:jsonl|json)?\s*([\s\S]*?)\s*```', response_str, re.DOTALL)
        jsonl_content = match.group(1).strip() if match else response_str.strip()
        
        for line in jsonl_content.strip().split('\n'):
            cleaned_line = line.strip()
            if not cleaned_line: continue
            try:
                data = json.loads(cleaned_line)
                if isinstance(data.get("relevant"), bool):
                    results.append({"id": data.get("id"), "relevant": data.get("relevant")})
                else:
                    # AIãŒ true/false ä»¥å¤– (ä¾‹: "true") ã§è¿”ã—ãŸå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                    results.append({"id": data.get("id"), "relevant": str(data.get("relevant")).lower() == 'true'})
            except json.JSONDecodeError as json_e:
                logger.warning(f"AIãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å›ç­”ãƒ‘ãƒ¼ã‚¹å¤±æ•—: {cleaned_line} - Error: {json_e}")
                # ãƒ‘ãƒ¼ã‚¹å¤±æ•—æ™‚ã¯å®‰å…¨ã®ãŸã‚ relevant=True (æ®‹ã™) ã¨ã—ã¦æ‰±ã†
                id_match = re.search(r'"id":\s*(\d+)', cleaned_line)
                if id_match:
                    results.append({"id": int(id_match.group(1)), "relevant": True})
        
        return pd.DataFrame(results) if results else pd.DataFrame(columns=['id', 'relevant'])
        
    except Exception as e:
        logger.error(f"AIãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãƒãƒƒãƒå‡¦ç†ä¸­ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        st.error(f"AIãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å…¨ä»¶ True (æ®‹ã™) ã¨ã—ã¦è¿”ã™
        return df_batch[['id']].copy().assign(relevant=True)

# --- AIã«ã‚ˆã‚‹ç›´æ¥ã‚¿ã‚°ä»˜ã‘ ---
def perform_ai_tagging(df_batch, categories_to_tag, llm, analysis_prompt=""): # (â˜…) analysis_prompt ã‚’å¼•æ•°ã«è¿½åŠ 
    """ãƒ†ã‚­ã‚¹ãƒˆã®ãƒãƒƒãƒã‚’å—ã‘å–ã‚Šã€AIãŒã€æŒ‡å®šã•ã‚ŒãŸã‚«ãƒ†ã‚´ãƒªå®šç¾©ã€‘ã«åŸºã¥ã„ã¦ç›´æ¥ã‚¿ã‚°ä»˜ã‘ã‚’è¡Œã†"""
    if llm is None: # LLMãŒãªã„å ´åˆã¯å‡¦ç†ä¸å¯
        logger.error("perform_ai_tagging: LLMãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        st.error("AIãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚APIã‚­ãƒ¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        return pd.DataFrame() # ç©ºã®DFã‚’è¿”ã™

    logger.debug(f"AI Tagging - Received categories: {json.dumps(categories_to_tag, ensure_ascii=False)}")
    logger.info(f"{len(df_batch)}ä»¶ AIã‚¿ã‚°ä»˜ã‘é–‹å§‹ (ã‚«ãƒ†ã‚´ãƒª: {list(categories_to_tag.keys())})")
    
    # (â˜…) åˆ†ææŒ‡é‡ (analysis_prompt) ã«åŸºã¥ã„ã¦ã€AIã«æ¸¡ã™åœ°åè¾æ›¸ã‚’çµã‚Šè¾¼ã‚€
    relevant_geo_db = {}
    if JAPAN_GEOGRAPHY_DB:
        prompt_lower = analysis_prompt.lower()
        # (ä¾‹: "åºƒå³¶" ãŒæŒ‡é‡ã«ã‚ã‚Œã°ã€"åºƒå³¶çœŒ" ã¨ "åºƒå³¶å¸‚" ã®ãƒ‡ãƒ¼ã‚¿ã®ã¿æ¸¡ã™)
        keys_found = [
            key for key in JAPAN_GEOGRAPHY_DB.keys() 
            if any(hint in key for hint in [
                "åºƒå³¶", "ç¦å²¡", "å¤§é˜ª", "æ±äº¬", "åŒ—æµ·é“", "æ„›çŸ¥", "å®®åŸ", "æœ­å¹Œ", "æ¨ªæµœ", "åå¤å±‹", "äº¬éƒ½", "ç¥æˆ¸", "ä»™å°"
            ]) and any(hint in prompt_lower for hint in [
                "åºƒå³¶", "ç¦å²¡", "å¤§é˜ª", "æ±äº¬", "åŒ—æµ·é“", "æ„›çŸ¥", "å®®åŸ", "æœ­å¹Œ", "æ¨ªæµœ", "åå¤å±‹", "äº¬éƒ½", "ç¥æˆ¸", "ä»™å°"
            ])
        ]
        # (â˜…) æŒ‡é‡ã‹ã‚‰é–¢é€£ã‚­ãƒ¼ã‚’æ¨æ¸¬ (ç°¡æ˜“ç‰ˆ)
        if "åºƒå³¶" in prompt_lower: keys_found.extend(["åºƒå³¶çœŒ", "åºƒå³¶å¸‚"])
        if "æ±äº¬" in prompt_lower: keys_found.extend(["æ±äº¬éƒ½", "æ±äº¬23åŒº"])
        if "å¤§é˜ª" in prompt_lower: keys_found.extend(["å¤§é˜ªåºœ", "å¤§é˜ªå¸‚"])
        
        for key in set(keys_found):
            if key in JAPAN_GEOGRAPHY_DB:
                relevant_geo_db[key] = JAPAN_GEOGRAPHY_DB[key]
        
        # (â˜…) ã‚‚ã—ä½•ã‚‚è¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã° (ã¾ãŸã¯æŒ‡é‡ãŒç©ºãªã‚‰)ã€ä¸»è¦éƒ½å¸‚ã®ã¿
        if not relevant_geo_db:
            logger.warning("åœ°åè¾æ›¸ã®çµã‚Šè¾¼ã¿ãƒ’ãƒ³ãƒˆãªã—ã€‚ä¸»è¦éƒ½å¸‚ã®ã¿æ¸¡ã—ã¾ã™ã€‚")
            default_keys = ["æ±äº¬éƒ½", "æ±äº¬23åŒº", "å¤§é˜ªåºœ", "å¤§é˜ªå¸‚", "åºƒå³¶çœŒ", "åºƒå³¶å¸‚"]
            for key in default_keys:
                 if key in JAPAN_GEOGRAPHY_DB:
                     relevant_geo_db[key] = JAPAN_GEOGRAPHY_DB[key]
        
        geo_context_str = json.dumps(relevant_geo_db, ensure_ascii=False, indent=2)
        # (â˜…) ãƒˆãƒ¼ã‚¯ãƒ³æ•°å‰Šæ¸›ã®ãŸã‚ã€5000æ–‡å­—ã‚’è¶…ãˆã‚‹å ´åˆã¯ç¸®å°
        if len(geo_context_str) > 5000:
            logger.warning(f"åœ°åè¾æ›¸ãŒå¤§ãã™ã ({len(geo_context_str)}B)ã€‚ã‚­ãƒ¼ã®ã¿ã«ç¸®å°ã€‚")
            geo_context_str = json.dumps(list(relevant_geo_db.keys()), ensure_ascii=False)
    else:
        geo_context_str = "{}"
        
    logger.info(f"AIã«æ¸¡ã™åœ°åè¾æ›¸(çµè¾¼æ¸ˆ): {list(relevant_geo_db.keys())}")
    
    input_texts_jsonl = df_batch.apply(lambda row: json.dumps({"id": row['id'], "text": str(row['ANALYSIS_TEXT_COLUMN'])[:500]}, ensure_ascii=False), axis=1).tolist() # (â˜…) str() ã§å›²ã‚€
    logger.debug(f"AI Tagging - Input sample: {input_texts_jsonl[0] if input_texts_jsonl else 'None'}")
    prompt = PromptTemplate.from_template(
        """
        ã‚ãªãŸã¯é«˜ç²¾åº¦ãƒ‡ãƒ¼ã‚¿åˆ†æã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ã€Œã‚«ãƒ†ã‚´ãƒªå®šç¾©ã€ã€Œåœ°åè¾æ›¸ã€ã€Œåˆ†ææŒ‡é‡ã€ã«åŸºã¥ãã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºã—ã¾ã™ã€‚
        # åˆ†ææŒ‡é‡ (Analysis Scope): {analysis_prompt}
        # åœ°åè¾æ›¸ (JAPAN_GEOGRAPHY_DB): {geo_context}
        # ã‚«ãƒ†ã‚´ãƒªå®šç¾© (categories): {categories}
        # ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ (JSONL): {text_data_jsonl}

        # æŒ‡ç¤º:
        1. ã€Œãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿(JSONL)ã€ã®å„è¡Œã‚’å‡¦ç†ã™ã‚‹ã€‚
        2. ã€Œã‚«ãƒ†ã‚´ãƒªå®šç¾©ã€ã®ã‚­ãƒ¼åã‚’ã€å³æ ¼ã«ã€‘ä½¿ç”¨ã—ã€å…¨ã‚«ãƒ†ã‚´ãƒªã‚’æŠ½å‡ºã™ã‚‹ã€‚
        3. ã€"å¸‚åŒºç”ºæ‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰" ä»¥å¤–ã®ã‚«ãƒ†ã‚´ãƒªã€‘:
           - å€¤ã¯å¿…ãšã€ãƒªã‚¹ãƒˆå½¢å¼ã€‘ã§å‡ºåŠ›ï¼ˆè©²å½“ãªã‘ã‚Œã°ç©ºãƒªã‚¹ãƒˆ []ï¼‰ã€‚
        4. ã€"å¸‚åŒºç”ºæ‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰" (æœ€é‡è¦ãƒ»å˜ä¸€å›ç­”)ã€‘:
           - å€¤ã¯ã€å˜ä¸€ã®æ–‡å­—åˆ—ã€‘ã§å‡ºåŠ›ã™ã‚‹ (è©²å½“ãªã‘ã‚Œã°ç©ºæ–‡å­—åˆ— "")ã€‚ãƒªã‚¹ãƒˆå½¢å¼ã¯ã€å³ç¦ã€‘ã€‚
           - æŠ½å‡ºãƒ«ãƒ¼ãƒ«:
             a. ã€Œåœ°åè¾æ›¸ã€ã®ã€å€¤ã€‘(ä¾‹: "å‘‰å¸‚", "å»¿æ—¥å¸‚å¸‚", "åºƒå³¶å¸‚ä¸­åŒº") ã¾ãŸã¯ã€ã‚­ãƒ¼ã€‘(ä¾‹: "åºƒå³¶å¸‚") ã«ä¸€è‡´ã™ã‚‹ã€æœ€ã‚‚æ–‡è„ˆã«é–¢é€£æ€§ã®é«˜ã„ã‚‚ã®ã‚’ã€1ã¤ã ã‘ã€‘é¸ã¶ã€‚
             b. (ä¾‹: "åºƒå³¶å¸‚" ã¨ "åºƒå³¶å¸‚ä¸­åŒº" ãŒä¸¡æ–¹è¨€åŠã•ã‚Œã¦ã„ã‚Œã°ã€ã‚ˆã‚Šè©³ç´°ãª "åºƒå³¶å¸‚ä¸­åŒº" ã‚’å„ªå…ˆã™ã‚‹)
             c. "å®®å³¶" ã®ã‚ˆã†ãªãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯åã¯ã€ãã‚ŒãŒå±ã™ã‚‹ã€Œåœ°åè¾æ›¸ã€ã®å¸‚åŒºç”ºæ‘å (ä¾‹: "å»¿æ—¥å¸‚å¸‚") ã«ã€å¿…ãšå¤‰æ›ã€‘ã—ã¦å›ç­”ã™ã‚‹ã€‚
             d. "åºƒå³¶" ã®ã‚ˆã†ãªæ›–æ˜§ãªè¡¨ç¾ã¯ã€æ–‡è„ˆã‹ã‚‰ (a) ã®ã„ãšã‚Œã‹ã«ç‰¹å®šã§ãã‚‹å ´åˆã®ã¿ (ä¾‹: "åºƒå³¶å¸‚") æŠ½å‡ºã—ã€ç‰¹å®šã§ããªã‘ã‚Œã°ã€ç©ºæ–‡å­—åˆ— ""ã€‘ã¨ã™ã‚‹ã€‚
             e. éƒ½é“åºœçœŒå (ä¾‹: "åºƒå³¶çœŒ")ã€ãŠã‚ˆã³ã€Œè¦³å…‰åœ°ã€ã®ã‚ˆã†ãªåœ°åä»¥å¤–ã®å˜èªã¯ã€çµ¶å¯¾ã«æŠ½å‡ºã—ãªã„ã€‘ã€‚
             f. ã€Œåˆ†ææŒ‡é‡ã€ã¨ç„¡é–¢ä¿‚ãªåœ°åŸŸã®åœ°åï¼ˆä¾‹: æŒ‡é‡ãŒã€Œåºƒå³¶ã€ãªã®ã«ãƒ†ã‚­ã‚¹ãƒˆãŒã€Œæ»‹è³€çœŒã€ï¼‰ã¯ã€æŠ½å‡ºã—ãªã„ã€‘ã€‚
        5. ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆæƒ…å ±ã®æé€ ï¼‰ç¦æ­¢ã€‚
        6. å‡ºåŠ›ã¯ã€JSONLå½¢å¼ã®ã¿ã€‘ï¼ˆid ã¨ categories ã‚’å«ã‚€è¾æ›¸ï¼‰ã€‚
        
        # å›ç­” (JSONLå½¢å¼ã®ã¿):
        """
    )
    chain = prompt | llm | StrOutputParser()
    try:
        # (â˜…) æ­£ã—ã„ invoke_params ã‚’å®šç¾©
        invoke_params = {
            "categories": json.dumps(categories_to_tag, ensure_ascii=False), 
            "geo_context": geo_context_str, # (â˜…) å¤‰æ›´
            "text_data_jsonl": "\n".join(input_texts_jsonl),
            "analysis_prompt": analysis_prompt # (â˜…) è¿½åŠ 
        }
        
        # (â˜…) invoke_params ã®å®šç¾©ç›´å¾Œã‹ã‚‰ã€ãã®ã¾ã¾ AI å‘¼ã³å‡ºã—å‡¦ç†ã‚’ç¶šã‘ã‚‹
        logger.debug(f"AI Tagging - Invoking LLM...")
        logger.info(f"Attempting AI call for ID: {df_batch.iloc[0]['id']}...")
        response_str = chain.invoke(invoke_params)
        logger.debug(f"AI Tagging - Raw response received.")
        logger.info(f"AIå¿œç­”å—ä¿¡å®Œäº†ã€‚")
        results = []
        match = re.search(r'```(?:jsonl|json)?\s*([\s\S]*?)\s*```', response_str, re.DOTALL)
        jsonl_content = match.group(1).strip() if match else response_str.strip()
        logger.debug(f"AI Tagging - Cleaned JSONL ready.")
        expected_keys = list(categories_to_tag.keys())
        for line in jsonl_content.strip().split('\n'):
            cleaned_line = line.strip();
            if not cleaned_line: continue
            try:
                data = json.loads(cleaned_line); row_result = {"id": data.get("id")}
                tag_source = data.get("categories") if isinstance(data.get("categories"), dict) else data
                for key in expected_keys:
                    # (â˜…) AIãŒè¿”ã™ã‚­ãƒ¼ã®æºã‚‰ãã«å¯¾å¿œã™ã‚‹ãŸã‚ã€ç©ºç™½é™¤å»ã—ã¦æ¯”è¼ƒ
                    found_key = None
                    for resp_key in tag_source.keys():
                        if resp_key.strip() == key:
                            found_key = resp_key
                            break
                    raw_value = tag_source.get(found_key) if found_key else None # è¦‹ã¤ã‹ã£ãŸã‚­ãƒ¼ã§å€¤ã‚’å–å¾—

                    # (â˜…) --- æ–°ã—ã„ãƒ­ã‚¸ãƒƒã‚¯ (ã“ã“ã‹ã‚‰) ---
                    if key == "å¸‚åŒºç”ºæ‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰":
                        # (â˜…) "å¸‚åŒºç”ºæ‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰" ã¯ã€å˜ä¸€æ–‡å­—åˆ—ã€‘ã¨ã—ã¦å‡¦ç†
                        processed_value = "" # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ç©ºæ–‡å­—åˆ—
                        if isinstance(raw_value, list) and raw_value:
                            # AIãŒæŒ‡ç¤ºã«åã—ã¦ãƒªã‚¹ãƒˆã§è¿”ã—ãŸå ´åˆã€æœ€åˆã®æœ‰åŠ¹ãªå€¤
                            processed_value = str(raw_value[0]).strip()
                        elif raw_value is not None and str(raw_value).strip():
                            # AIãŒæŒ‡ç¤ºé€šã‚Šå˜ä¸€æ–‡å­—åˆ—ã§è¿”ã—ãŸå ´åˆ
                            processed_value = str(raw_value).strip()
                        
                        # "è©²å½“ãªã—" ç­‰ã®AIã®è¿”ç­”ã‚’ç©ºæ–‡å­—åˆ—ã«æ­£è¦åŒ–
                        if processed_value.lower() in ["è©²å½“ãªã—", "none", "null", ""]:
                            row_result[key] = "" 
                        else:
                            row_result[key] = processed_value
                    
                    else:
                        # (â˜…) "å¸‚åŒºç”ºæ‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰" ä»¥å¤–ã®ã‚­ãƒ¼ã¯ã€ãƒªã‚¹ãƒˆã€‘ã¨ã—ã¦å‡¦ç† (å¾“æ¥ã®ãƒ­ã‚¸ãƒƒã‚¯)
                        processed_values = [] 
                        if isinstance(raw_value, list):
                            processed_values = sorted(list(set(str(val).strip() for val in raw_value if str(val).strip())))
                        elif raw_value is not None and str(raw_value).strip():
                            processed_values = [str(raw_value).strip()]
                        
                        row_result[key] = processed_values
                    # (â˜…) --- æ–°ã—ã„ãƒ­ã‚¸ãƒƒã‚¯ (ã“ã“ã¾ã§) ---
                    
                results.append(row_result)
            except json.JSONDecodeError as json_e:
                logger.warning(f"AIå›ç­”JSONLãƒ‘ãƒ¼ã‚¹å¤±æ•—: {cleaned_line} - Error: {json_e}")
                try: # IDã ã‘ã§ã‚‚å–å¾—è©¦è¡Œ
                    id_match = re.search(r'"id":\s*(\d+)', cleaned_line)
                    if id_match:
                         failed_id = int(id_match.group(1)); empty_result = {"id": failed_id}
                         for key in expected_keys: empty_result[key] = []
                         results.append(empty_result); logger.warning(f"ãƒ‘ãƒ¼ã‚¹å¤±æ•—è¡Œ ID:{failed_id} è£œå®Œ")
                    else: logger.error(f"ãƒ‘ãƒ¼ã‚¹å¤±æ•—è¡Œ IDæŠ½å‡ºä¸å¯: {cleaned_line}")
                except Exception as id_extract_e: logger.error(f"IDæŠ½å‡ºä¸­ã‚¨ãƒ©ãƒ¼: {id_extract_e}")
                continue
        logger.info(f"{len(results)}ä»¶ã‚¿ã‚°ä»˜ã‘çµæœãƒ‘ãƒ¼ã‚¹å®Œäº†ã€‚")
        if results: logger.debug(f"AI Tagging - Parsed sample: {results[0]}")
        return pd.DataFrame(results) if results else pd.DataFrame(columns=['id'] + expected_keys)
    except Exception as e:
        logger.error(f"AIã‚¿ã‚°ä»˜ã‘ãƒãƒƒãƒå‡¦ç†ä¸­ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        st.error(f"AIãƒãƒƒãƒå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        return pd.DataFrame()

# --- åˆ†ææ‰‹æ³•ææ¡ˆãƒ­ã‚¸ãƒƒã‚¯ ---
# --- (â˜…) åˆ†ææ‰‹æ³•ææ¡ˆãƒ­ã‚¸ãƒƒã‚¯ (ææ¡ˆæ‹¡å……ç‰ˆ) ---
# --- (â˜…) åˆ†ææ‰‹æ³•ææ¡ˆãƒ­ã‚¸ãƒƒã‚¯ (ææ¡ˆæ‹¡å……ç‰ˆ) ---
def suggest_analysis_techniques(df):
    """
    ãƒ•ãƒ©ã‚°ä»˜ããƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’åˆ†æã—ã€é©åˆ‡ãªåˆ†ææ‰‹æ³•ã‚’å„ªå…ˆåº¦é †ã«ææ¡ˆã™ã‚‹ã€‚
    """
    suggestions = []
    if df is None or df.empty: # ç©ºã®DFã‚‚ãƒã‚§ãƒƒã‚¯
        logger.error("suggest_analysis_techniques ã« None ã¾ãŸã¯ç©ºã®DataFrame"); return suggestions
    try:
        # ãƒ‡ãƒ¼ã‚¿å‹ã®å†ç¢ºèªã¨åˆ—ã®ç‰¹å®š (ã‚ˆã‚Šç¢ºå®Ÿã«)
        numeric_cols = df.select_dtypes(include=np.number).columns.tolist()
        object_cols = df.select_dtypes(include='object').columns.tolist() # objectå‹ã‚’ã¾ãšå–å¾—
        datetime_cols = []
        possible_dt_cols = [col for col in object_cols] # objectåˆ—ã‹ã‚‰å€™è£œã‚’æ¢ã™
        # æ—¥ä»˜å‹ã¸ã®å¤‰æ›ã‚’è©¦ã¿ã‚‹ (æ¬ æãŒå¤šã„åˆ—ã¯é™¤å¤–)
        for col in possible_dt_cols:
             if df[col].isnull().sum() / len(df) > 0.5: continue # æ¬ æãŒ5å‰²è¶…ãªã‚‰ã‚¹ã‚­ãƒƒãƒ—
             sample = df[col].dropna().head(50)
             if sample.empty: continue
             try:
                 pd.to_datetime(sample, errors='raise')
                 # å¤‰æ›æˆåŠŸ â†’ å…¨ä½“ã‚’å¤‰æ›ã—ã¦ç¢ºèª
                 temp_dt = pd.to_datetime(df[col], errors='coerce').dropna()
                 # å¹´æœˆæ—¥ã®ã„ãšã‚Œã‹ãŒè¤‡æ•°å­˜åœ¨ã™ã‚‹ã‹ã€ç‰¹å®šã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€ã‹ãªã©ã§åˆ¤æ–­
                 if not temp_dt.empty and (temp_dt.dt.year.nunique() > 1 or temp_dt.dt.month.nunique() > 1 or temp_dt.dt.day.nunique() > 1 or col.lower() in ['date', 'time', 'timestamp', 'æ—¥ä»˜', 'æ—¥æ™‚']):
                     datetime_cols.append(col)
                     logger.info(f"åˆ— '{col}' ã‚’æ—¥æ™‚åˆ—ã¨ã—ã¦èªè­˜ã—ã¾ã—ãŸã€‚")
             except (ValueError, TypeError, OverflowError, pd.errors.ParserError): pass # ã‚¨ãƒ©ãƒ¼ãŒå‡ºã¦ã‚‚ç„¡è¦–

        numeric_cols = [col for col in numeric_cols if col != 'id'] # idåˆ—é™¤å¤–
        # ANALYSIS_TEXT_COLUMN ã¨æ—¥æ™‚åˆ—ã‚’é™¤ã„ãŸã‚‚ã®ãŒã‚«ãƒ†ã‚´ãƒªåˆ—å€™è£œ
        categorical_cols = [col for col in object_cols if col != 'ANALYSIS_TEXT_COLUMN' and col not in datetime_cols]
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ—ï¼ˆãƒ•ãƒ©ã‚°åˆ—ï¼‰ã‚’ç‰¹å®š
        flag_cols = [col for col in categorical_cols if col.endswith('ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰')]
        other_categorical = [col for col in categorical_cols if not col.endswith('ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰')]
        logger.info(f"ææ¡ˆåˆ†æ - æ•°å€¤:{numeric_cols}, ã‚«ãƒ†ã‚´ãƒª(ãƒ•ãƒ©ã‚°):{flag_cols}, ã‚«ãƒ†ã‚´ãƒª(ä»–):{other_categorical}, æ—¥æ™‚:{datetime_cols}")

        # --- ææ¡ˆãƒªã‚¹ãƒˆ (å„ªå…ˆåº¦é †) ---
        potential_suggestions = []

        # å„ªå…ˆåº¦1: åŸºæœ¬é›†è¨ˆ (ã»ã¼å¿…é ˆ)
        if flag_cols:
            potential_suggestions.append({
                "priority": 1, "name": "å˜ç´”é›†è¨ˆï¼ˆé »åº¦åˆ†æï¼‰",
                "description": "å„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆã‚«ãƒ†ã‚´ãƒªï¼‰ãŒã©ã®ãã‚‰ã„ã®é »åº¦ã§å‡ºç¾ã—ãŸã‹ãƒˆãƒƒãƒ—Nã‚’è¡¨ç¤ºã—ã€å…¨ä½“åƒã‚’æŠŠæ¡ã—ã¾ã™ã€‚",
                "reason": f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ—({len(flag_cols)}å€‹)ã‚ã‚Šã€‚ã¾ãšè¦‹ã‚‹ã¹ãåŸºæœ¬æŒ‡æ¨™ã§ã™ã€‚",
                "suitable_cols": flag_cols
            })
        if numeric_cols:
             potential_suggestions.append({
                 "priority": 1, "name": "åŸºæœ¬çµ±è¨ˆé‡",
                 "description": f"æ•°å€¤ãƒ‡ãƒ¼ã‚¿({', '.join(numeric_cols)})ã®å¹³å‡ã€ä¸­å¤®å€¤ã€æœ€å¤§/æœ€å°å€¤ãªã©ã‚’ç®—å‡ºã—ã€ãƒ‡ãƒ¼ã‚¿ã®åˆ†å¸ƒã‚’ç¢ºèªã—ã¾ã™ã€‚",
                 "reason": f"æ•°å€¤åˆ—({len(numeric_cols)}å€‹)ã‚ã‚Šã€‚ãƒ‡ãƒ¼ã‚¿ã®åŸºæœ¬ç‰¹æ€§æŠŠæ¡ã«ã€‚",
                 "suitable_cols": numeric_cols
             })

        # å„ªå…ˆåº¦2: é–¢ä¿‚æ€§ã®åˆ†æ (ã‚¯ãƒ­ã‚¹é›†è¨ˆ)
        if len(flag_cols) >= 2:
            potential_suggestions.append({
                "priority": 2, "name": "ã‚¯ãƒ­ã‚¹é›†è¨ˆï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é–“ï¼‰",
                "description": "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é–“ã®çµ„ã¿åˆã‚ã›ã§å¤šãå‡ºç¾ã™ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¢ã‚Šã¾ã™ï¼ˆä¾‹: ç‰¹å®šã®å¸‚åŒºç”ºæ‘ã¨è¦³å…‰åœ°ã®çµ„ã¿åˆã‚ã›ï¼‰ã€‚",
                "reason": f"è¤‡æ•°ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ—({len(flag_cols)}å€‹)ã‚ã‚Šã€é–¢é€£æ€§ã®ç™ºè¦‹ã«ã€‚",
                "suitable_cols": flag_cols
            })
        if flag_cols and other_categorical:
             potential_suggestions.append({
                "priority": 2, "name": "ã‚¯ãƒ­ã‚¹é›†è¨ˆï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰Ã—å±æ€§ï¼‰",
                "description": f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰({flag_cols[0]}ãªã©)ã¨ä»–ã®å±æ€§({', '.join(other_categorical)})ã®é–¢ä¿‚æ€§ã‚’åˆ†æã—ã¾ã™ï¼ˆä¾‹: å¹´ä»£åˆ¥ã«ã‚ˆãå‡ºã‚‹è¦³å…‰åœ°ï¼‰ã€‚",
                "reason": f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ—ã¨ä»–ã‚«ãƒ†ã‚´ãƒªåˆ—({len(other_categorical)}å€‹)ã‚ã‚Šã€å±æ€§ã”ã¨ã®å‚¾å‘æŠŠæ¡ã«ã€‚",
                "suitable_cols": flag_cols + other_categorical
            })

        # (â˜…) å„ªå…ˆåº¦3: å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆ†æ (æ–°è¦è¿½åŠ )
        if len(flag_cols) >= 2:
            potential_suggestions.append({
                "priority": 3, "name": "å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆ†æ",
                "description": "ãƒ†ã‚­ã‚¹ãƒˆå†…ã§åŒæ™‚ã«å‡ºç¾ã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆä¾‹: ã€Œåºƒå³¶å¸‚ã€ã¨ã€Œå³å³¶ç¥ç¤¾ã€ï¼‰ã®é–¢ä¿‚æ€§ã‚’ç·šã§çµã³ã€ã©ã®å˜èªãŒä¸­å¿ƒçš„ãªå½¹å‰²ã‚’æœãŸã—ã¦ã„ã‚‹ã‹ã‚’å¯è¦–åŒ–ã—ã¾ã™ã€‚",
                "reason": f"è¤‡æ•°ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ—({len(flag_cols)}å€‹)ã‚ã‚Šã€‚å˜èªé–“ã®éš ã‚ŒãŸã¤ãªãŒã‚Šã‚’ç™ºè¦‹ã§ãã¾ã™ã€‚",
                "suitable_cols": flag_cols
            })
        

        # (â˜…) å„ªå…ˆåº¦4: ã‚°ãƒ«ãƒ¼ãƒ—æ¯”è¼ƒ (å„ªå…ˆåº¦å¤‰æ›´ 3 -> 4)
        if numeric_cols and flag_cols:
            potential_suggestions.append({
                "priority": 4, "name": "ã‚«ãƒ†ã‚´ãƒªåˆ¥é›†è¨ˆï¼ˆã‚°ãƒ«ãƒ¼ãƒ—æ¯”è¼ƒï¼‰",
                "description": f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚«ãƒ†ã‚´ãƒªï¼ˆ{flag_cols[0]}ãªã©ï¼‰ã”ã¨ã«æ•°å€¤ãƒ‡ãƒ¼ã‚¿({numeric_cols[0]}ãªã©)ã®å¹³å‡å€¤ã‚„åˆè¨ˆå€¤ã«å·®ãŒã‚ã‚‹ã‹æ¯”è¼ƒã—ã¾ã™ã€‚",
                "reason": f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ—ã¨æ•°å€¤åˆ—({len(numeric_cols)}å€‹)ã‚ã‚Šã€ã‚°ãƒ«ãƒ¼ãƒ—é–“ã®ç‰¹å¾´æ¯”è¼ƒã«ã€‚",
                "suitable_cols": {"numeric": numeric_cols, "grouping": flag_cols}
            })

        # (â˜…) å„ªå…ˆåº¦5: æ™‚ç³»åˆ—åˆ†æ (å„ªå…ˆåº¦å¤‰æ›´ 4 -> 5)
        if datetime_cols and flag_cols:
             potential_suggestions.append({
                "priority": 5, "name": "æ™‚ç³»åˆ—ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æ",
                "description": f"ç‰¹å®šã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å‡ºç¾æ•°ãŒæ™‚é–“ï¼ˆ{datetime_cols[0]}ãªã©ï¼‰ã¨ã¨ã‚‚ã«ã©ã†å¤‰åŒ–ã—ãŸã‹ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’å¯è¦–åŒ–ã—ã¾ã™ã€‚",
                "reason": f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ—ã¨æ—¥æ™‚åˆ—({len(datetime_cols)}å€‹)ã‚ã‚Šã€æ™‚é–“å¤‰åŒ–ã®æŠŠæ¡ã«ã€‚",
                "suitable_cols": {"datetime": datetime_cols, "keywords": flag_cols}
            })

        # (â˜…) å„ªå…ˆåº¦6: ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚° (å„ªå…ˆåº¦å¤‰æ›´ 5 -> 6)
        potential_suggestions.append({
            "priority": 6, "name": "ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°ï¼ˆé »å‡ºå˜èªãªã©ï¼‰",
            "description": "åŸæ–‡ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰é »å‡ºã™ã‚‹å˜èªã‚’æŠ½å‡ºã—ã€ã©ã®ã‚ˆã†ãªè¨€è‘‰ãŒå¤šãä½¿ã‚ã‚Œã¦ã„ã‚‹ã‹å…¨ä½“åƒã‚’æŠŠæ¡ã—ã¾ã™ã€‚",
            "reason": "åŸæ–‡ãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚Šã€ã‚¿ã‚°ä»˜ã‘ä»¥å¤–ã®è¦³ç‚¹ã‹ã‚‰ã®ã‚¤ãƒ³ã‚µã‚¤ãƒˆç™ºè¦‹ã«ã€‚",
            "suitable_cols": ['ANALYSIS_TEXT_COLUMN']
        })

        # (â˜…) å„ªå…ˆåº¦7: å¤šå¤‰é‡è§£æ (å„ªå…ˆåº¦å¤‰æ›´ 6 -> 7)
        if len(numeric_cols) >= 3:
             potential_suggestions.append({
                 "priority": 7, "name": "ä¸»æˆåˆ†åˆ†æ (PCA) / å› å­åˆ†æ",
                 "description": f"è¤‡æ•°ã®æ•°å€¤ãƒ‡ãƒ¼ã‚¿({', '.join(numeric_cols)})é–“ã®ç›¸é–¢é–¢ä¿‚ã‹ã‚‰ã€èƒŒå¾Œã«ã‚ã‚‹å…±é€šã®è¦å› ï¼ˆä¸»æˆåˆ†/å› å­ï¼‰ã‚’æ¢ã‚Šã¾ã™ã€‚",
                 "reason": f"è¤‡æ•°æ•°å€¤åˆ—({len(numeric_cols)}å€‹)ãŒã‚ã‚Šã€å¤‰æ•°é–“ã®è¤‡é›‘ãªé–¢ä¿‚æ€§ã®ç¸®ç´„ã‚„è§£é‡ˆã«ã€‚",
                 "suitable_cols": numeric_cols
             })

        # å„ªå…ˆåº¦ã§ã‚½ãƒ¼ãƒˆã—ã€ä¸Šä½8ä»¶ç¨‹åº¦ã‚’è¿”ã™
        suggestions = sorted(potential_suggestions, key=lambda x: x['priority'])
        logger.info(f"ææ¡ˆæ‰‹æ³•(ã‚½ãƒ¼ãƒˆå¾Œ): {[s['name'] for s in suggestions]}")
        return suggestions[:8] # (â˜…) ä¸Šé™ã‚’ 8 ã«å¤‰æ›´

    except Exception as e:
        logger.error(f"åˆ†ææ‰‹æ³•ææ¡ˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True); st.warning(f"åˆ†ææ‰‹æ³•ææ¡ˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
    return suggestions

# (â˜… ã“ã“ã‹ã‚‰æ–°è¦è¿½åŠ )
def get_suggestions_from_prompt(user_prompt, llm, df, existing_suggestions):
    """
    ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è‡ªç”±è¨˜è¿°ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã«åŸºã¥ãã€AIãŒè¿½åŠ ã®åˆ†ææ‰‹æ³•ã‚’ææ¡ˆã™ã‚‹ã€‚
    """
    logger.info("AIãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ™ãƒ¼ã‚¹ã®åˆ†æææ¡ˆã‚’é–‹å§‹...")
    if llm is None:
        logger.error("get_suggestions_from_prompt: LLMãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return []
    
    try:
        # AIã«æ¸¡ã™ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã‚µãƒãƒªãƒ¼ã‚’ä½œæˆ
        col_info = []
        for col in df.columns:
            col_info.append(f"- {col} (å‹: {df[col].dtype})")
        column_info_str = "\n".join(col_info)
        
        # æ—¢å­˜ã®ææ¡ˆåãƒªã‚¹ãƒˆ
        existing_names = [s['name'] for s in existing_suggestions]
        
        prompt = PromptTemplate.from_template(
        """
        ã‚ãªãŸã¯ãƒ‡ãƒ¼ã‚¿åˆ†æã®ã‚¹ã‚­ãƒ¼ãƒè¨­è¨ˆè€…ã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã€Œåˆ†ææŒ‡ç¤ºã€ã‚’è§£é‡ˆã—ã€ãã‚Œã‚’JSONãƒªã‚¹ãƒˆå½¢å¼ã®ã€Œåˆ†ææ‰‹æ³•ã€ã«å¤‰æ›ã—ã¦ãã ã•ã„ã€‚
        # ãƒ‡ãƒ¼ã‚¿æ§‹é€  (åˆ©ç”¨å¯èƒ½ãªåˆ—å):
        {column_info}
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®åˆ†ææŒ‡ç¤º (ã“ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’è§£é‡ˆå¯¾è±¡ã¨ã—ã¾ã™):
        {user_prompt}

        # æŒ‡ç¤º:
        1. ã€Œãƒ¦ãƒ¼ã‚¶ãƒ¼ã®åˆ†ææŒ‡ç¤ºã€ã«å«ã¾ã‚Œã‚‹åˆ†æé …ç›®ã‚’ã€1ã¤ãšã¤ã€‘è§£é‡ˆã—ã€ãã‚Œãã‚Œã‚’ã€Œåˆ†ææ‰‹æ³•ã€ã¨ã—ã¦å®šç¾©ã™ã‚‹ã€‚ (ä¾‹: ã€ŒæŠ•ç¨¿æ•°åˆ†æã€ã¯ã€ŒæŠ•ç¨¿æ•°åˆ†æã€ã¨ã„ã†åå‰ã®æ‰‹æ³•ã«ã™ã‚‹)
        2. å„ææ¡ˆã« `priority` (å„ªå…ˆåº¦: 6å›ºå®š), `name` (æ‰‹æ³•å), `description` (æ‰‹æ³•ã®ç°¡æ½”ãªèª¬æ˜), `reason` (ææ¡ˆç†ç”±: ã€Œãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡ç¤ºã«åŸºã¥ãã€ã¨è¨˜è¿°) ã‚’å«ã‚€JSONãƒªã‚¹ãƒˆå½¢å¼ã§å›ç­”ã™ã‚‹ã€‚(â˜…)
        3. æŒ‡ç¤ºãŒç©ºã€ã¾ãŸã¯è§£é‡ˆä¸èƒ½ãªå ´åˆã¯ã€ç©ºãƒªã‚¹ãƒˆ [] ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚
        
        # å›ç­” (JSONãƒªã‚¹ãƒˆå½¢å¼ã®ã¿):
        """
    )
        
        chain = prompt | llm | StrOutputParser()
        response_str = chain.invoke({
            "column_info": column_info_str,
            "user_prompt": user_prompt,
        })
        
        logger.info(f"AIè¿½åŠ ææ¡ˆ(ç”Ÿ): {response_str}")
        
        # AIã®å¿œç­” (JSONãƒªã‚¹ãƒˆ) ã‚’ãƒ‘ãƒ¼ã‚¹
        match = re.search(r'\[.*\]', response_str, re.DOTALL)
        if not match:
            logger.warning("AIãŒJSONãƒªã‚¹ãƒˆå½¢å¼ã§å¿œç­”ã—ã¾ã›ã‚“ã§ã—ãŸã€‚")
            return []
            
        json_str = match.group(0)
        ai_suggestions = json.loads(json_str)
        
        # priorityãŒ6æœªæº€ã®ã‚‚ã®ã¯ã€æ—¢å­˜ã®ææ¡ˆã¨ç«¶åˆã—ãªã„ã‚ˆã†èª¿æ•´
        # (â˜…) AIãŒç”Ÿæˆã—ãŸææ¡ˆã®å„ªå…ˆåº¦ã‚’ 1 (æœ€é«˜) ã«è¨­å®š
        for suggestion in ai_suggestions:
            suggestion['priority'] = 6 # ãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡ç¤ºã‚’æœ€å„ªå…ˆ
                
        logger.info(f"AIè¿½åŠ ææ¡ˆ(ãƒ‘ãƒ¼ã‚¹æ¸ˆ): {len(ai_suggestions)}ä»¶")
        return ai_suggestions
        
    except Exception as e:
        logger.error(f"AIè¿½åŠ ææ¡ˆã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        st.warning(f"AIè¿½åŠ ææ¡ˆã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return []

# (â˜… ã“ã“ã‹ã‚‰ Step C ç”¨ã®æ–°è¦é–¢æ•°ç¾¤)

def run_simple_count(df, flag_cols):
    """å˜ç´”é›†è¨ˆï¼ˆé »åº¦åˆ†æï¼‰ã‚’å®Ÿè¡Œã—ã€Streamlitã§å¯è¦–åŒ–ã™ã‚‹"""
    if not flag_cols:
        st.warning("é›†è¨ˆå¯¾è±¡ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ—ï¼ˆsuitable_colsï¼‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return
    
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«é›†è¨ˆå¯¾è±¡ã®åˆ—ã‚’1ã¤é¸ã‚“ã§ã‚‚ã‚‰ã†
    col_to_analyze = st.selectbox(
        "é›†è¨ˆã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ—ã‚’é¸æŠ:", 
        flag_cols, 
        key=f"sc_select_{flag_cols[0]}" # (â˜…) ã‚­ãƒ¼ã‚’ãƒ¦ãƒ‹ãƒ¼ã‚¯ã«ã™ã‚‹
    )
    
    if not col_to_analyze or col_to_analyze not in df.columns:
        st.error(f"åˆ— '{col_to_analyze}' ãŒãƒ‡ãƒ¼ã‚¿ã«å­˜åœ¨ã—ã¾ã›ã‚“ã€‚")
        return

    # ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã®æ–‡å­—åˆ—ã‚’å€‹åˆ¥ã®è¡Œã«åˆ†è§£
    try:
        # (â˜…) .strã‚¢ã‚¯ã‚»ã‚¹å‰ã« .astype(str) ã‚’æŒŸã¿ã€æ•°å€¤å‹ãªã©ã§ã®ã‚¨ãƒ©ãƒ¼ã‚’å›é¿
        s = df[col_to_analyze].astype(str).str.split(', ').explode()
        s = s[s.str.strip() != ''] # ç©ºç™½ã‚’é™¤å»
        s = s.str.strip() # å‰å¾Œã®ç©ºç™½ã‚’é™¤å»
        
        if s.empty:
            st.info("é›†è¨ˆå¯¾è±¡ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            return
            
        counts = s.value_counts().head(20) # ä¸Šä½20ä»¶
        st.bar_chart(counts)
        with st.expander("è©³ç´°ãƒ‡ãƒ¼ã‚¿ï¼ˆä¸Šä½20ä»¶ï¼‰"):
            st.dataframe(counts)
            
        return counts # (â˜…) æ­£å¸¸çµ‚äº†æ™‚ã«é›†è¨ˆçµæœã‚’è¿”ã™
            
    except Exception as e:
        st.error(f"å˜ç´”é›†è¨ˆã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        logger.error(f"run_simple_count error: {e}", exc_info=True)
    return None # (â˜…) ã‚¨ãƒ©ãƒ¼æ™‚ã¯ None ã‚’è¿”ã™


def run_basic_stats(df, numeric_cols):
    """åŸºæœ¬çµ±è¨ˆé‡ã‚’å®Ÿè¡Œã—ã€Streamlitã§è¡¨ç¤ºã™ã‚‹"""
    if not numeric_cols:
        st.warning("é›†è¨ˆå¯¾è±¡ã®æ•°å€¤åˆ—ï¼ˆsuitable_colsï¼‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return
    
    # å­˜åœ¨ã™ã‚‹åˆ—ã®ã¿ã‚’å¯¾è±¡
    existing_cols = [col for col in numeric_cols if col in df.columns]
    if not existing_cols:
        st.error("æŒ‡å®šã•ã‚ŒãŸæ•°å€¤åˆ—ãŒãƒ‡ãƒ¼ã‚¿ã«å­˜åœ¨ã—ã¾ã›ã‚“ã€‚")
        return
        
    stats_df = df[existing_cols].describe() # (â˜…) çµæœã‚’ä¸€æ—¦å¤‰æ•°ã«
    st.dataframe(stats_df)
    return stats_df # (â˜…) çµæœã‚’è¿”ã™


def run_crosstab(df, suitable_cols):
    """ã‚¯ãƒ­ã‚¹é›†è¨ˆã‚’å®Ÿè¡Œã—ã€Streamlitã§è¡¨ç¤ºã™ã‚‹"""
    if not suitable_cols or len(suitable_cols) < 2:
        st.warning("ã‚¯ãƒ­ã‚¹é›†è¨ˆã«ã¯2ã¤ä»¥ä¸Šã®åˆ—ãŒå¿…è¦ã§ã™ã€‚")
        return

    # å­˜åœ¨ã™ã‚‹åˆ—ã®ã¿ã‚’å¯¾è±¡
    existing_cols = [col for col in suitable_cols if col in df.columns]
    if len(existing_cols) < 2:
        st.error(f"ãƒ‡ãƒ¼ã‚¿å†…ã«å­˜åœ¨ã™ã‚‹åˆ†æå¯¾è±¡åˆ—ãŒ2ã¤æœªæº€ã§ã™: {existing_cols}")
        return

    st.info(f"åˆ†æå¯èƒ½ãªåˆ—: {', '.join(existing_cols)}")
    
    # (â˜…) ã‚­ãƒ¼ãŒé‡è¤‡ã—ãªã„ã‚ˆã†ã€suitable_cols[0] ã‚’ã‚­ãƒ¼ã«å«ã‚ã‚‹
    key_base = suitable_cols[0]
    col1 = st.selectbox("è¡Œ (Index) ã«è¨­å®šã™ã‚‹åˆ—:", existing_cols, key=f"ct_idx_{key_base}")
    
    # col1 ä»¥å¤–ã®åˆ—ã‚’ col2 ã®å€™è£œã¨ã™ã‚‹
    options_col2 = [c for c in existing_cols if c != col1]
    if not options_col2:
        st.error("2ã¤ç›®ã®åˆ—ã‚’é¸æŠã§ãã¾ã›ã‚“ã€‚")
        return
        
    col2 = st.selectbox("åˆ— (Column) ã«è¨­å®šã™ã‚‹åˆ—:", options_col2, key=f"ct_col_{key_base}")

    if not col1 or not col2:
        return

    try:
        # (â˜…) æ³¨æ„: ã“ã®å®Ÿè£…ã¯ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã®æ–‡å­—åˆ—ã‚’ã€Œãã®ã¾ã¾ã€é›†è¨ˆã—ã¾ã™ (ä¾‹: "åºƒå³¶å¸‚, å‘‰å¸‚")
        # ã“ã‚Œã‚’åˆ†è§£ï¼ˆexplodeï¼‰ã™ã‚‹ã¨çµ„ã¿åˆã‚ã›çˆ†ç™ºãŒèµ·ãã‚‹ãŸã‚ã€ã¾ãšç°¡æ˜“ç‰ˆã¨ã—ã¦å®Ÿè£…
        
        # (â˜…) .astype(str) ã‚’æŒŸã‚“ã§å®‰å…¨ã«å‡¦ç†
        crosstab_df = pd.crosstab(df[col1].astype(str), df[col2].astype(str))
        
        if crosstab_df.empty:
            st.info("ã‚¯ãƒ­ã‚¹é›†è¨ˆã®çµæœã€ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            return
        
        st.dataframe(crosstab_df)
        
        # (â˜…) ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—è¡¨ç¤º (ã‚ªãƒ—ã‚·ãƒ§ãƒ³)
        if st.checkbox("ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã§è¡¨ç¤º", key=f"ct_heatmap_{key_base}"):
             try:
                 import altair as alt
                 # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’AltairãŒæ‰±ãˆã‚‹å½¢å¼ï¼ˆlong formatï¼‰ã«å¤‰æ›
                 ct_long = crosstab_df.stack().reset_index().rename(columns={0: 'count'})
                 heatmap = alt.Chart(ct_long).mark_rect().encode(
                     x=alt.X(col2, type='ordinal', title=col2), # (â˜…) f-stringå½¢å¼ã‚’ã‚„ã‚ã€type='ordinal' ã§å‹ã‚’æŒ‡å®š
                     y=alt.Y(col1, type='ordinal', title=col1), # (â˜…) f-stringå½¢å¼ã‚’ã‚„ã‚ã€type='ordinal' ã§å‹ã‚’æŒ‡å®š
                     color=alt.Color('count', type='quantitative', title='Count', scale=alt.Scale(range='heatmap')), # (â˜…) åŒæ§˜ã«å¤‰æ›´
                     tooltip=[col1, col2, 'count']
                 ).properties(
                     title=f"ã‚¯ãƒ­ã‚¹é›†è¨ˆ: {col1} vs {col2}"
                 ).interactive() # ã‚ºãƒ¼ãƒ ã‚„ãƒ‘ãƒ³ã‚’å¯èƒ½ã«ã™ã‚‹
                 st.altair_chart(heatmap, use_container_width=True)
             except ImportError:
                 st.warning("ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—è¡¨ç¤ºã«ã¯ `altair` ãŒå¿…è¦ã§ã™ã€‚`st.dataframe` ã§è¡¨ç¤ºã—ã¾ã™ã€‚")
             except Exception as he:
                 st.warning(f"ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—æç”»ã‚¨ãƒ©ãƒ¼: {he}ã€‚`st.dataframe` ã§è¡¨ç¤ºã—ã¾ã™ã€‚")
        return crosstab_df # (â˜…) æ­£å¸¸çµ‚äº†æ™‚ã«é›†è¨ˆçµæœã‚’è¿”ã™
    except Exception as e:
        st.error(f"ã‚¯ãƒ­ã‚¹é›†è¨ˆã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        logger.error(f"run_crosstab error: {e}", exc_info=True)
    return None # (â˜…) ã‚¨ãƒ©ãƒ¼æ™‚ã¯ None ã‚’è¿”ã™


def run_timeseries(df, suitable_cols_dict):
    """æ™‚ç³»åˆ—åˆ†æã‚’å®Ÿè¡Œã—ã€Streamlitã§å¯è¦–åŒ–ã™ã‚‹"""
    if not isinstance(suitable_cols_dict, dict) or 'datetime' not in suitable_cols_dict or 'keywords' not in suitable_cols_dict:
        st.warning("æ™‚ç³»åˆ—åˆ†æã®ãŸã‚ã®åˆ—æƒ…å ±ï¼ˆdatetime, keywordsï¼‰ãŒä¸ååˆ†ã§ã™ã€‚")
        return
        
    dt_cols = [col for col in suitable_cols_dict['datetime'] if col in df.columns]
    kw_cols = [col for col in suitable_cols_dict['keywords'] if col in df.columns]

    if not dt_cols: st.error("æ—¥æ™‚åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"); return
    if not kw_cols: st.error("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"); return

    # (â˜…) ã‚­ãƒ¼ãŒé‡è¤‡ã—ãªã„ã‚ˆã†ã€dt_cols[0] ã‚’ã‚­ãƒ¼ã«å«ã‚ã‚‹
    key_base = dt_cols[0]
    dt_col = st.selectbox("ä½¿ç”¨ã™ã‚‹æ—¥æ™‚åˆ—:", dt_cols, key=f"ts_dt_{key_base}")
    kw_col = st.selectbox("é›†è¨ˆã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ—:", kw_cols, key=f"ts_kw_{key_base}")

    if not dt_col or not kw_col:
        return

    try:
        df_copy = df[[dt_col, kw_col]].copy()
        
        # æ—¥æ™‚åˆ—ã‚’å¤‰æ›
        df_copy[dt_col] = pd.to_datetime(df_copy[dt_col], errors='coerce')
        df_copy = df_copy.dropna(subset=[dt_col])
        if df_copy.empty: st.info("æœ‰åŠ¹ãªæ—¥æ™‚ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚"); return

        # (â˜…) ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ—ã‚’ .astype(str) ã«
        df_copy[kw_col] = df_copy[kw_col].astype(str)
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ—ãŒç©ºã§ãªã„è¡Œã®ã¿ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        df_copy = df_copy[df_copy[kw_col].str.strip() != ''] 
        if df_copy.empty: st.info(f"ã€Œ{kw_col}ã€ã«æœ‰åŠ¹ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"); return

        # æ—¥(Day)å˜ä½ã§ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¦ã‚«ã‚¦ãƒ³ãƒˆ
        # (â˜…) .resample('D') ã®å‰ã« .set_index() ãŒå¿…è¦
        time_df = df_copy.set_index(dt_col).resample('D').size().rename("æŠ•ç¨¿æ•°")
        
        if time_df.empty: st.info("æ™‚ç³»åˆ—é›†è¨ˆã®çµæœã€ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"); return
            
        st.line_chart(time_df)
        with st.expander("è©³ç´°ãƒ‡ãƒ¼ã‚¿"):
            st.dataframe(time_df)
            
        return time_df # (â˜…) æ­£å¸¸çµ‚äº†æ™‚ã«é›†è¨ˆçµæœã‚’è¿”ã™
            
    except Exception as e:
        st.error(f"æ™‚ç³»åˆ—åˆ†æã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        logger.error(f"run_timeseries error: {e}", exc_info=True)
    return None # (â˜…) ã‚¨ãƒ©ãƒ¼æ™‚ã¯ None ã‚’è¿”ã™

def run_text_mining(df, text_col='ANALYSIS_TEXT_COLUMN'):
    """
    spaCyã‚’ä½¿ç”¨ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°ï¼ˆé »å‡ºå˜èªåˆ†æï¼‰ã‚’å®Ÿè¡Œã—ã€å¯è¦–åŒ–ã™ã‚‹ã€‚
    APIã¯ä½¿ç”¨ã—ãªã„ã€‚
    """
    if text_col not in df.columns or df[text_col].empty:
        st.warning(f"åˆ†æå¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆåˆ— '{text_col}' ãŒãªã„ã‹ã€ç©ºã§ã™ã€‚")
        return

    # Step A ã§ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã®spaCyãƒ¢ãƒ‡ãƒ«ã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‹ã‚‰å–å¾—
    if 'nlp' not in st.session_state or st.session_state.nlp is None:
        # ã‚‚ã—Step Aã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦Step B/Cã«æ¥ãŸå ´åˆã€ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
        st.session_state.nlp = load_spacy_model()
        if st.session_state.nlp is None:
            st.error("spaCyæ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸã€‚Step Aã‹ã‚‰ã‚„ã‚Šç›´ã—ã¦ãã ã•ã„ã€‚")
            return
            
    nlp = st.session_state.nlp
    
    st.info("ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°å‡¦ç†ä¸­ï¼ˆãƒ‡ãƒ¼ã‚¿é‡ã«ã‚ˆã£ã¦æ™‚é–“ãŒã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ï¼‰...")

    try:
        # (â˜…) æ¬ æå€¤ã‚’é™¤å¤–ã—ã€æ–‡å­—åˆ—ã«å¤‰æ›
        texts = df[text_col].dropna().astype(str)
        if texts.empty:
            st.warning("åˆ†æå¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            return
            
        words = []
        # (â˜…) nlp.pipe ã§é«˜é€Ÿå‡¦ç†
        # å“è© (pos_) ãŒ åè©(NOUN), å›ºæœ‰åè©(PROPN), å½¢å®¹è©(ADJ) ã®ã¿æŠ½å‡º
        target_pos = {'NOUN', 'PROPN', 'ADJ'}
        
        # ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ï¼ˆä¸€èˆ¬çš„ã™ãã‚‹å˜èªï¼‰ã®ãƒªã‚¹ãƒˆ (ç°¡æ˜“ç‰ˆ)
        stop_words = {
            'ã®', 'ã«', 'ã¯', 'ã‚’', 'ãŒ', 'ã§', 'ã¦', 'ã§ã™', 'ã¾ã™', 'ã“ã¨', 'ã‚‚ã®', 'ãã‚Œ', 'ã‚ã‚Œ',
            'ã“ã‚Œ', 'ãŸã‚', 'ã„ã‚‹', 'ã™ã‚‹', 'ã‚ã‚‹', 'ãªã„', 'ã„ã†', 'ã‚ˆã†', 'ãã†', 'ãªã©', 'ã•ã‚“',
            'çš„', 'çš„', 'çš„', 'çš„', 'äºº', 'è‡ªåˆ†', 'ç§', 'åƒ•', 'ä½•', 'ãã®', 'ã“ã®', 'ã‚ã®'
        }

        # nlp.pipe ã§ãƒãƒƒãƒå‡¦ç†
        for doc in nlp.pipe(texts, disable=["parser", "ner"]): # æ§‹æ–‡è§£æã¨å›ºæœ‰è¡¨ç¾æŠ½å‡ºã¯ä¸è¦
            for token in doc:
                # (â˜…) è¦‹å‡ºã—èª(lemma_)ã‚’ä½¿ã„ã€å“è©ãƒã‚§ãƒƒã‚¯ã€ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰é™¤å¤–ã€1æ–‡å­—é™¤å¤–
                if (token.pos_ in target_pos) and (not token.is_stop) and (token.lemma_ not in stop_words) and (len(token.lemma_) > 1):
                    words.append(token.lemma_)

        if not words:
            st.warning("æŠ½å‡ºå¯èƒ½ãªæœ‰åŠ¹ãªå˜èªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            return

        # Pandasã§é »åº¦é›†è¨ˆ
        word_counts = pd.Series(words).value_counts().head(30) # ä¸Šä½30ä»¶

        st.subheader("é »å‡ºå˜èª Top 30")
        st.bar_chart(word_counts)
        with st.expander("è©³ç´°ãƒ‡ãƒ¼ã‚¿ï¼ˆTop 30ï¼‰"):
            st.dataframe(word_counts.reset_index(name="å‡ºç¾å›æ•°").rename(columns={"index": "å˜èª"}))        
        return word_counts # (â˜…) æ­£å¸¸çµ‚äº†æ™‚ã«é›†è¨ˆçµæœã‚’è¿”ã™

    except Exception as e:
        st.error(f"ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        logger.error(f"run_text_mining error: {e}", exc_info=True)
    return None # (â˜…) ã‚¨ãƒ©ãƒ¼æ™‚ã¯ None ã‚’è¿”ã™

def generate_ai_summary_prompt(results_dict, df):
    """
    Step C-1 ã§å¾—ã‚‰ã‚ŒãŸåˆ†æçµæœ(DataFrame)ã‚’AIç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¤‰æ›ã™ã‚‹ã€‚
    """
    logger.info("AIã‚µãƒãƒªãƒ¼ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ç”Ÿæˆé–‹å§‹...")
    if not results_dict:
        logger.warning("AIã‚µãƒãƒªãƒ¼ã®å…ƒã«ãªã‚‹åˆ†æçµæœãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return "ã‚¨ãƒ©ãƒ¼: AIã‚µãƒãƒªãƒ¼ã®å…ƒã«ãªã‚‹åˆ†æçµæœãŒã‚ã‚Šã¾ã›ã‚“ã€‚Step C-1ã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚"
    
    # ãƒ‡ãƒ¼ã‚¿å…¨ä½“ã®æ¦‚è¦
    context_str = f"## åˆ†æå¯¾è±¡ãƒ‡ãƒ¼ã‚¿ã®æ¦‚è¦\n"
    context_str += f"- ç·è¡Œæ•°: {len(df)}\n"
    context_str += f"- åˆ—ãƒªã‚¹ãƒˆ: {', '.join(df.columns.tolist())}\n\n"
    context_str += "## å€‹åˆ¥åˆ†æã®çµæœã‚µãƒãƒªãƒ¼\n"
    context_str += "ï¼ˆæ³¨ï¼šãƒˆãƒ¼ã‚¯ãƒ³æ•°ç¯€ç´„ã®ãŸã‚ã€å„åˆ†æçµæœã¯æœ€å¤§5ä»¶ã®ã¿æŠœç²‹ã—ã¦ã„ã¾ã™ï¼‰\n\n"
    
    # å„åˆ†æçµæœã‚’æ–‡å­—åˆ—ã«å¤‰æ›
    for name, data in results_dict.items():
        context_str += f"### {name}\n"
        if isinstance(data, (pd.DataFrame, pd.Series)):
            if data.empty:
                context_str += "(ãƒ‡ãƒ¼ã‚¿ãªã—)\n\n"
            else:
                # (â˜…) ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆ/ãƒˆãƒ¼ã‚¯ãƒ³æ•°ç¯€ç´„ã®ãŸã‚ã€.head(5) ã¨ .info() ã ã‘æ¸¡ã™
                if len(data) > 5:
                    context_str += f"ä¸Šä½5ä»¶:\n{data.head(5).to_string()}\n\n"
                else:
                    context_str += f"å…¨ä»¶:\n{data.to_string()}\n\n"
        else:
            context_str += f"{str(data)}\n\n"
    
    # AIã¸ã®æœ€çµ‚çš„ãªæŒ‡ç¤ºãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    final_prompt = f"""
ã‚ãªãŸã¯å„ªç§€ãªãƒ‡ãƒ¼ã‚¿ã‚¢ãƒŠãƒªã‚¹ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã®ã€Œåˆ†æå¯¾è±¡ãƒ‡ãƒ¼ã‚¿ã®æ¦‚è¦ã€ã¨ã€Œå€‹åˆ¥åˆ†æã®çµæœã‚µãƒãƒªãƒ¼ã€ã‚’èª­ã¿è§£ãã€ãƒ—ãƒ­ã®è¦–ç‚¹ã‹ã‚‰ç·åˆçš„ãªã€Œåˆ†æã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã€ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

# æŒ‡ç¤º:
1. å„åˆ†æçµæœã‚’æ¨ªæ–­çš„ã«è§£é‡ˆã—ã€é‡è¦ãªã‚¤ãƒ³ã‚µã‚¤ãƒˆï¼ˆæ´å¯Ÿï¼‰ã‚’æŠ½å‡ºã™ã‚‹ã€‚
2. å˜ãªã‚‹çµæœã®ç¾…åˆ—ã§ã¯ãªãã€ãƒ“ã‚¸ãƒã‚¹ä¸Šã®ç¤ºå”†ï¼ˆä¾‹: ã©ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒé‡è¦ã‹ã€ã©ã®å±æ€§ã«æ³¨ç›®ã™ã¹ãã‹ï¼‰ã‚’å°ãå‡ºã™ã€‚
3. ãƒ¬ãƒãƒ¼ãƒˆã¯æ—¥æœ¬ã®ãƒ“ã‚¸ãƒã‚¹ãƒãƒ³å‘ã‘ã«ã€è¦‹ã‚„ã™ã„ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ï¼ˆè¦‹å‡ºã—ã€ç®‡æ¡æ›¸ãï¼‰ã§æ§‹æˆã™ã‚‹ã€‚
4. çµè«–ã‹ã‚‰å…ˆã«è¿°ã¹ã€ãã®å¾Œã«è©³ç´°ãªæ ¹æ‹ ã‚’èª¬æ˜ã™ã‚‹ã€‚

---
[åˆ†æã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ]
{context_str}
---

[ã‚ãªãŸã®å›ç­”]
# åˆ†æã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆ
"""
    logger.info("AIã‚µãƒãƒªãƒ¼ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆå®Œäº†ã€‚")
    return final_prompt

def render_step_c():
    """Step C: åˆ†æçµæœã®å¯è¦–åŒ–ã‚’æç”»ã™ã‚‹"""
    st.title("ğŸ”¬ åˆ†æçµæœã®å¯è¦–åŒ– (Step C)")
    
    # (â˜…) Step C-2 (AIã‚µãƒãƒªãƒ¼) ç”¨ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã‚’åˆæœŸåŒ–
    if 'step_c_results' not in st.session_state: st.session_state.step_c_results = {}
    if 'ai_summary_prompt' not in st.session_state: st.session_state.ai_summary_prompt = None
    if 'ai_summary_result' not in st.session_state: st.session_state.ai_summary_result = None
    
    # --- å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ãŒã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ã‚ã‚‹ã‹ç¢ºèª ---
    if 'chosen_analysis_list' not in st.session_state or not st.session_state.chosen_analysis_list:
        st.warning("å®Ÿè¡Œã™ã‚‹åˆ†æãŒé¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚Step Bã«æˆ»ã£ã¦ãã ã•ã„ã€‚")
        if st.button("Step B ã«æˆ»ã‚‹"):
            st.session_state.current_step = 'B'; st.rerun()
        return

    if 'df_flagged_B' not in st.session_state or st.session_state.df_flagged_B.empty:
        st.warning("åˆ†æå¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚Step Bã§CSVã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
        if st.button("Step B ã«æˆ»ã‚‹"):
            st.session_state.current_step = 'B'; st.rerun()
        return
        
    if 'suggestions_B' not in st.session_state or not st.session_state.suggestions_B:
        st.warning("åˆ†ææ‰‹æ³•ã®ææ¡ˆãƒªã‚¹ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚Step Bã§å†ææ¡ˆã—ã¦ãã ã•ã„ã€‚")
        if st.button("Step B ã«æˆ»ã‚‹"):
            st.session_state.current_step = 'B'; st.rerun()
        return

    df = st.session_state.df_flagged_B
    selected_names = st.session_state.chosen_analysis_list
    all_suggestions = st.session_state.suggestions_B
    
    # é¸æŠã•ã‚ŒãŸåˆ†ææ‰‹æ³•ã®ã€Œå®Œå…¨ãªå®šç¾©ï¼ˆsuitable_colså«ã‚€ï¼‰ã€ã‚’å–å¾—
    analyses_to_run = [s for s in all_suggestions if s['name'] in selected_names]
    
    st.info(f"**å®Ÿè¡Œã™ã‚‹åˆ†æ:** {', '.join(selected_names)}")
    st.markdown("---")
    # (â˜…) AIã‚µãƒãƒªãƒ¼ç”Ÿæˆã«å‚™ãˆã€çµæœã‚’ãƒªã‚»ãƒƒãƒˆ
    st.session_state.step_c_results = {}
    # --- å„åˆ†ææ‰‹æ³•ã‚’ãƒ«ãƒ¼ãƒ—ã—ã¦å®Ÿè¡Œãƒ»æç”» ---
    for suggestion in analyses_to_run:
        name = suggestion['name']
        cols = suggestion.get('suitable_cols', []) # (â˜…) ææ¡ˆãƒ­ã‚¸ãƒƒã‚¯ã‹ã‚‰æ¸¡ã•ã‚ŒãŸåˆ—
        
        with st.container(border=True):
            st.subheader(f"ğŸ“ˆ åˆ†æçµæœ: {name}")
            
            try:
                result_data = None # (â˜…) çµæœæ ¼ç´ç”¨
                if name == "å˜ç´”é›†è¨ˆï¼ˆé »åº¦åˆ†æï¼‰":
                    result_data = run_simple_count(df, cols) # (â˜…)
                elif name == "åŸºæœ¬çµ±è¨ˆé‡":
                    result_data = run_basic_stats(df, cols) # (â˜…)
                elif name == "ã‚¯ãƒ­ã‚¹é›†è¨ˆï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é–“ï¼‰":
                    result_data = run_crosstab(df, cols) # (â˜…)
                elif name == "ã‚¯ãƒ­ã‚¹é›†è¨ˆï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰Ã—å±æ€§ï¼‰":
                    result_data = run_crosstab(df, cols) # (â˜…)
                elif name == "ã‚«ãƒ†ã‚´ãƒªåˆ¥é›†è¨ˆï¼ˆã‚°ãƒ«ãƒ¼ãƒ—æ¯”è¼ƒï¼‰":
                    # (â˜…) suitable_cols ãŒ dict {"numeric": [], "grouping": []} ã®ã¯ãš
                    if isinstance(cols, dict) and 'numeric' in cols and 'grouping' in cols:
                         # (â˜…) GroupBy ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã« .describe() ã‚’ç›´æ¥å‘¼ã³å‡ºã™
                         grouping_cols = cols['grouping']
                         numeric_cols_to_desc = [col for col in cols['numeric'] if col in df.columns]
                         
                         if not numeric_cols_to_desc:
                             st.warning("åˆ†æå¯¾è±¡ã®æ•°å€¤åˆ—ãŒãƒ‡ãƒ¼ã‚¿ã«ã‚ã‚Šã¾ã›ã‚“ã€‚")
                         elif not grouping_cols:
                             st.warning("åˆ†æå¯¾è±¡ã®ã‚°ãƒ«ãƒ¼ãƒ—åˆ—ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
                         else:
                             # (â˜…) è¤‡æ•°ã®ã‚°ãƒ«ãƒ¼ãƒ—åˆ—ã«å¯¾å¿œã™ã‚‹ãŸã‚ã€ãƒªã‚¹ãƒˆã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
                             if not isinstance(grouping_cols, list):
                                 grouping_cols = [grouping_cols]
                                 
                             # ã‚°ãƒ«ãƒ¼ãƒ—åˆ—ãŒãƒ‡ãƒ¼ã‚¿ã«å­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
                             existing_grouping_cols = [col for col in grouping_cols if col in df.columns]
                             if not existing_grouping_cols:
                                 st.warning(f"ã‚°ãƒ«ãƒ¼ãƒ—åŒ–åˆ— {grouping_cols} ãŒãƒ‡ãƒ¼ã‚¿ã«å­˜åœ¨ã—ã¾ã›ã‚“ã€‚")
                             else:
                                try:
                                     # (â˜…) ã‚°ãƒ«ãƒ¼ãƒ—åˆ—ã®å‹ã‚’æ–‡å­—åˆ—ã«å¤‰æ›ï¼ˆå®‰å…¨ã®ãŸã‚ï¼‰ - ã“ã®ãƒ–ãƒ­ãƒƒã‚¯ã‚’æˆ»ã™
                                     df_copy = df.copy()
                                     for col in existing_grouping_cols:
                                         df_copy[col] = df_copy[col].astype(str)
                                         
                                     # (â˜…) .describe() ã®çµæœã‚’å–å¾—
                                     result_df = df_copy.groupby(existing_grouping_cols)[numeric_cols_to_desc].describe()
                                     flat_cols = []
                                     for col in result_df.columns:
                                         # col is a tuple, e.g., ('è©•ä¾¡ãŠã‚ˆã³ã‚¹ã‚³ã‚¢', 'mean')
                                         flat_cols.append(f"{col[0]}_{col[1]}") # e.g., "è©•ä¾¡ãŠã‚ˆã³ã‚¹ã‚³ã‚¢_mean"
                                     result_df.columns = flat_cols
                                     
                                     # (â˜…) è¡Œã®Indexã‚’ãƒªã‚»ãƒƒãƒˆã—ã¦åˆ—ã«å¤‰æ›ã—ã€è¡¨ç¤º
                                     final_result_df = result_df.reset_index() # (â˜…)
                                     
                                     # (â˜…) ãƒ•ãƒ©ãƒƒãƒˆåŒ–ã•ã‚ŒãŸDataFrameã‚’è¡¨ç¤º
                                     st.dataframe(final_result_df) 
                                     
                                     # (â˜…) AIã‚µãƒãƒªãƒ¼ç”¨ã«çµæœã‚’ä¿å­˜
                                     result_data = final_result_df # (â˜…)
                                except Exception as group_e:
                                     st.error(f"ã‚°ãƒ«ãƒ¼ãƒ—åˆ¥é›†è¨ˆã‚¨ãƒ©ãƒ¼: {group_e}")
                elif name == "æ™‚ç³»åˆ—ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æ":
                    # ... (æ™‚ç³»åˆ—åˆ†æã®ãƒ­ã‚¸ãƒƒã‚¯ã¯ L418-L425 ã¾ã§ãã®ã¾ã¾) ...
                    if isinstance(cols, dict) and 'datetime' in cols and 'keywords' in cols:
                        result_data = run_timeseries(df, cols) # (â˜…)
                    else:
                         st.warning(f"ã€Œ{name}ã€ã®åˆ—å®šç¾©ãŒä¸é©åˆ‡ã§ã™: {cols}")
                elif name == "ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°ï¼ˆé »å‡ºå˜èªãªã©ï¼‰":
                    if cols and isinstance(cols, list) and cols[0] == 'ANALYSIS_TEXT_COLUMN':
                        result_data = run_text_mining(df, 'ANALYSIS_TEXT_COLUMN') # (â˜…)
                    else:
                        st.warning(f"ã€Œ{name}ã€ã®åˆ—å®šç¾©ãŒä¸é©åˆ‡ã§ã™: {cols}")
                elif name == "ä¸»æˆåˆ†åˆ†æ (PCA) / å› å­åˆ†æ":
                    st.warning("ã€Œä¸»æˆåˆ†åˆ†æã€ã¯ç¾åœ¨å®Ÿè£…ä¸­ã§ã™ã€‚")
                else:
                    st.warning(f"ã€Œ{name}ã€ã®å¯è¦–åŒ–ãƒ­ã‚¸ãƒƒã‚¯ã¯ã¾ã å®Ÿè£…ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
                
                # (â˜…) æ­£å¸¸ã«å®Ÿè¡Œã•ã‚ŒãŸçµæœã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ä¿å­˜
                if result_data is not None and not result_data.empty:
                    st.session_state.step_c_results[name] = result_data
            
            except Exception as e:
                st.error(f"ã€Œ{name}ã€ã®åˆ†æä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                logger.error(f"Step C Analysis Error ({name}): {e}", exc_info=True)

    st.markdown("---")
    st.success("Step C-1 (å¯è¦–åŒ–) ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
    
    # (â˜… ã“ã“ã‹ã‚‰ Step C-2 (AIã‚µãƒãƒªãƒ¼) ã®UI)
    st.header("Step C-2: AIã«ã‚ˆã‚‹åˆ†æã‚µãƒãƒªãƒ¼")
    
    if not st.session_state.step_c_results:
        st.warning("AIã‚µãƒãƒªãƒ¼ã®å…ƒã«ãªã‚‹åˆ†æçµæœãŒã‚ã‚Šã¾ã›ã‚“ã€‚Step C-1ã§æœ‰åŠ¹ãªåˆ†æã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
    else:
        st.info("ä¸Šè¨˜ã§å®Ÿè¡Œã•ã‚ŒãŸåˆ†æçµæœã‚’AIã«å…¥åŠ›ã—ã€ç·åˆçš„ãªã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¾ã™ã€‚")

        if st.button("ğŸ¤– AIã‚µãƒãƒªãƒ¼ç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ", key="gen_prompt_c2"):
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆé–¢æ•°ã‚’å‘¼ã³å‡ºã—
            st.session_state.ai_summary_prompt = generate_ai_summary_prompt(st.session_state.step_c_results, df)
            st.session_state.ai_summary_result = None # æ—¢å­˜ã®AIçµæœã‚’ãƒªã‚»ãƒƒãƒˆ
            st.rerun() # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¡¨ç¤ºã™ã‚‹ãŸã‚ã«å†æç”»

        # (â˜…) ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒç”Ÿæˆã•ã‚ŒãŸã‚‰ã€ç¢ºèªã‚¨ãƒªã‚¢ã‚’è¡¨ç¤º
        if st.session_state.ai_summary_prompt:
            st.subheader("AIã¸ã®æŒ‡ç¤ºãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆç¢ºèªãƒ»ç·¨é›†å¯ï¼‰")
            prompt_input = st.text_area(
                "ä»¥ä¸‹ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’AIã«é€ä¿¡ã—ã¾ã™:",
                value=st.session_state.ai_summary_prompt,
                height=300,
                key="ai_prompt_c2_input" # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒç·¨é›†ã§ãã‚‹ã‚ˆã†ã«ã‚­ãƒ¼ã‚’è¨­å®š
            )
            
            # (â˜…) AIå®Ÿè¡Œãƒœã‚¿ãƒ³
            if st.button("ğŸš€ ã“ã®å†…å®¹ã§AIã«æŒ‡ç¤ºã‚’é€ä¿¡", key="send_prompt_c2", type="primary"):
                if not os.getenv("GOOGLE_API_KEY"):
                    st.error("AIã®å®Ÿè¡Œã«ã¯ Google APIã‚­ãƒ¼ ãŒå¿…è¦ã§ã™ã€‚ï¼ˆã‚µã‚¤ãƒ‰ãƒãƒ¼ã§è¨­å®šã—ã¦ãã ã•ã„ï¼‰")
                else:
                    with st.spinner("AIãŒã‚µãƒãƒªãƒ¼ã‚’ç”Ÿæˆä¸­... (Rate Limitã«æ³¨æ„)"):
                        llm = get_llm()
                        if llm:
                            try:
                                # (â˜…) ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒç·¨é›†ã—ãŸå¯èƒ½æ€§ã®ã‚ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒªã‚¢ã®å†…å®¹ã‚’é€ä¿¡
                                response = llm.invoke(prompt_input) 
                                st.session_state.ai_summary_result = response.content
                            except Exception as e:
                                st.error(f"AIã®å‘¼ã³å‡ºã—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
                                logger.error(f"AI summary failed: {e}", exc_info=True)
                        else:
                            st.error("AIãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
            
        # (â˜…) AIã®å®Ÿè¡ŒçµæœãŒã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ã‚ã‚Œã°è¡¨ç¤º
        if st.session_state.ai_summary_result:
            st.subheader("AIã«ã‚ˆã‚‹åˆ†æã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆ")
            st.markdown(st.session_state.ai_summary_result)
    
    # (â˜… ã“ã“ã¾ã§ Step C-2 (AIã‚µãƒãƒªãƒ¼) ã®UI)

    st.markdown("---")
    if st.button("â¬…ï¸ Step B ã«æˆ»ã‚‹", key="back_to_b_c2"):
        st.session_state.current_step = 'B'; st.rerun()

# (â˜… ã“ã“ã¾ã§ Step C ç”¨ã®æ–°è¦é–¢æ•°ç¾¤)

def display_suggestions(suggestions, df):
    """
    ææ¡ˆã•ã‚ŒãŸåˆ†ææ‰‹æ³•ã‚’è¡¨ç¤ºã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒé¸æŠã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹ (â˜… ãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ç‰ˆ)
    """
    if not suggestions:
        st.info("ææ¡ˆå¯èƒ½ãªåˆ†ææ‰‹æ³•ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return

    st.subheader("ææ¡ˆã•ã‚ŒãŸåˆ†ææ‰‹æ³•:")
    st.markdown("---")
    
    # (â˜…) ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§é¸æŠã•ã‚Œã‚‹æ‰‹æ³•åãƒªã‚¹ãƒˆ (ä¸Šä½5ä»¶)
    # ã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’ã“ã“ã«ç§»å‹•
    default_selection_names = [s['name'] for s in suggestions[:min(len(suggestions), 5)]] 
    
    # (â˜…) --- Checkbox UI (MultiSelectã®ä»£ã‚ã‚Š) ---
    st.markdown("å®Ÿè¡Œã—ãŸã„åˆ†ææ‰‹æ³•ã‚’é¸æŠï¼ˆè¤‡æ•°å¯ï¼‰:")
    
    selected_technique_names = []
    
    # (â˜…) ææ¡ˆã•ã‚ŒãŸæ‰‹æ³•ã‚’ãƒ«ãƒ¼ãƒ—ã—ã¦ãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ã‚’ç”Ÿæˆ
    for suggestion in suggestions:
        name = suggestion['name']
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ãƒã‚§ãƒƒã‚¯ã‚’å…¥ã‚Œã‚‹ã‹
        is_default_checked = name in default_selection_names
        
        # ãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ã‚’ä½œæˆ
        is_checked = st.checkbox(
            name, 
            value=is_default_checked, 
            key=f"cb_{name}" # ã‚­ãƒ¼ã‚’ãƒ¦ãƒ‹ãƒ¼ã‚¯ã«
        )
        
        # ãƒã‚§ãƒƒã‚¯ã•ã‚ŒãŸã‚‰ãƒªã‚¹ãƒˆã«è¿½åŠ 
        if is_checked:
            selected_technique_names.append(name)
    
    # (â˜…) --- MultiSelectã®ã‚ã£ãŸå ´æ‰€ (L499-L505) ã¯å‰Šé™¤ ---

    # (â˜…) --- é¸æŠã•ã‚ŒãŸæ‰‹æ³•ã®è©³ç´°ã‚’è¡¨ç¤º ---
    if selected_technique_names:
        st.markdown("---")
        st.subheader("é¸æŠã•ã‚ŒãŸæ‰‹æ³•ã®è©³ç´°:")
        
        # é¸æŠã•ã‚ŒãŸæ‰‹æ³•ã®å®šç¾©ã‚’å–å¾—
        selected_suggestions = [s for s in suggestions if s['name'] in selected_technique_names]
        
        for suggestion in selected_suggestions:
            with st.expander(f"{suggestion['name']} (å„ªå…ˆåº¦: {suggestion['priority']})"):
                st.markdown(f"**<èª¬æ˜>**\n{suggestion['description']}")
                st.markdown(f"**<ææ¡ˆç†ç”±>**\n{suggestion['reason']}")

    st.markdown("---")
    # (â˜…) ãƒœã‚¿ãƒ³ã®æœ‰åŠ¹/ç„¡åŠ¹ã‚’è¨­å®š (æ‰‹æ³•ãŒ1ã¤ä»¥ä¸Šé¸æŠã•ã‚Œã¦ã„ã‚‹å ´åˆã®ã¿æœ‰åŠ¹)
    # (â˜…) --- å®Ÿè¡Œãƒœã‚¿ãƒ³ (ã“ã®ãƒ–ãƒ­ãƒƒã‚¯ãŒ1ã¤ã ã‘å­˜åœ¨ã™ã‚‹ã“ã¨ã‚’ç¢ºèª) ---
    if st.button("é¸æŠã—ãŸæ‰‹æ³•ã§åˆ†æã‚’å®Ÿè¡Œ (Step Cã¸)", key="execute_button_C_v2", disabled=not selected_technique_names, type="primary"):
         if selected_technique_names: # å¿µã®ãŸã‚å†ãƒã‚§ãƒƒã‚¯
             # st.info(f"ã‚¹ãƒ†ãƒƒãƒ—Cã¯ç¾åœ¨å®Ÿè£…ä¸­ã§ã™ã€‚é¸æŠã•ã‚ŒãŸæ‰‹æ³•: {', '.join(selected_technique_names)}")
             st.session_state.current_step = 'C' # (â˜…) å¤‰æ›´
             st.rerun() # (â˜…) å¤‰æ›´
         else:
             st.error("åˆ†æã‚’å®Ÿè¡Œã™ã‚‹ã«ã¯ã€å°‘ãªãã¨ã‚‚1ã¤ã®æ‰‹æ³•ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")

# --- Step A ã® UI ã¨ãƒ­ã‚¸ãƒƒã‚¯ ---
def render_step_a():
    # ... (Step A ã®ã‚³ãƒ¼ãƒ‰å…¨ä½“ - å¤‰æ›´ãªã—) ...
    st.title("ğŸ¤– [AIå¯¾å¿œ] ãƒ‡ãƒ¼ã‚¿åˆ†æãƒ»ãƒ•ãƒ©ã‚°ä»˜ã‘ãƒ„ãƒ¼ãƒ« (Step A: ã‚¿ã‚°ä»˜ã‘)")
    st.markdown("ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã€åˆ†ææŒ‡é‡ã«åŸºã¥ã„ã¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ã‚¿ã‚°ä»˜ã‘ã‚’è¡Œã„ã¾ã™ã€‚")
    logger = getLogger(__name__)
    if 'generated_categories' not in st.session_state: st.session_state.generated_categories = None
    if 'selected_categories' not in st.session_state: st.session_state.selected_categories = []
    if 'analysis_done' not in st.session_state: st.session_state.analysis_done = False
    if 'cancel_analysis' not in st.session_state: st.session_state.cancel_analysis = False
    st.header("Step 1: åˆ†æå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"); uploaded_files = st.file_uploader("åˆ†æã—ãŸã„ Excel / CSV ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆè¤‡æ•°å¯ï¼‰", type=['csv', 'xlsx', 'xls'], accept_multiple_files=True, key="uploader_A")
    if uploaded_files:
        st.header("Step 2: åˆ†ææŒ‡é‡ã®å…¥åŠ›"); analysis_prompt = st.text_area("ãƒ‡ãƒ¼ã‚¿ã®è£œè¶³ã‚„ãƒ•ãƒ©ã‚°ä»˜ã‘ã®æŒ‡é‡ã‚’å…¥åŠ›:", placeholder="ä¾‹ï¼šåºƒå³¶ã®è¦³å…‰ãƒ‡ãƒ¼ã‚¿...", key="analysis_prompt_A")
        st.header("Step 3: æŠ½å‡ºã‚«ãƒ†ã‚´ãƒªå€™è£œã®ç”Ÿæˆ")
        if st.button("ğŸ“ ã‚«ãƒ†ã‚´ãƒªå€™è£œã‚’ç”Ÿæˆ", key="generate_cat_button_A"):
            logger.info("ã‚«ãƒ†ã‚´ãƒªå€™è£œç”Ÿæˆãƒœã‚¿ãƒ³ã‚¯ãƒªãƒƒã‚¯");
            if not os.getenv("GOOGLE_API_KEY"): st.error("APIã‚­ãƒ¼æœªè¨­å®š")
            elif not analysis_prompt: st.error("åˆ†ææŒ‡é‡æœªå…¥åŠ›")
            else:
                with st.spinner("AIã‚«ãƒ†ã‚´ãƒªå€™è£œç”Ÿæˆä¸­..."):
                    llm = get_llm()
                    if llm: logger.info("get_dynamic_categories å‘¼ã³å‡ºã—..."); st.session_state.generated_categories = get_dynamic_categories(analysis_prompt, llm); logger.info(f"ç”Ÿæˆå€™è£œ: {st.session_state.generated_categories}"); st.session_state.selected_categories = ["å¸‚åŒºç”ºæ‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰"]; st.session_state.analysis_done = False
                    else: st.error("LLMåˆæœŸåŒ–å¤±æ•—")
        if st.session_state.generated_categories:
            st.header("Step 4: ã‚¿ã‚°ä»˜ã‘ã™ã‚‹ã‚«ãƒ†ã‚´ãƒªã‚’é¸æŠ"); all_possible_categories = list(st.session_state.generated_categories.keys()); mandatory_category = "å¸‚åŒºç”ºæ‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰"; selectable_categories = [cat for cat in all_possible_categories if cat != mandatory_category]; default_selection = [cat for cat in selectable_categories if cat in st.session_state.generated_categories]
            selected_dynamic_categories = st.multiselect("ã‚¿ã‚°ä»˜ã‘ã—ãŸã„ã‚«ãƒ†ã‚´ãƒªã‚’é¸æŠ:", options=selectable_categories, default=default_selection, key="category_multiselect_A"); st.session_state.selected_categories = [mandatory_category] + selected_dynamic_categories; st.write("---")
        st.header("Step 5: åˆ†æè¨­å®š (ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨)"); st.info("ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã«ã€åˆ†æå¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆåˆ—ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚")
        text_column_map, valid_files_present = {}, False
        for i, uploaded_file in enumerate(uploaded_files):
             preview_df = None
             try:
                uploaded_file.seek(0); preview_df = pd.read_csv(uploaded_file, encoding="utf-8-sig", nrows=5) if uploaded_file.name.endswith('.csv') else pd.read_excel(uploaded_file, nrows=5); uploaded_file.seek(0)
                if preview_df is not None and not preview_df.empty:
                    valid_files_present = True
                    with st.container(border=True):
                        st.markdown(f"**ãƒ•ã‚¡ã‚¤ãƒ«å: `{uploaded_file.name}`**"); columns = preview_df.columns.tolist(); session_key = f"text_col_{i}_{uploaded_file.name}"
                        default_col_index = 0; lower_columns = [c.lower() for c in columns]
                        if 'text' in lower_columns: default_col_index = lower_columns.index('text')
                        elif 'æœ¬æ–‡' in columns: default_col_index = columns.index('æœ¬æ–‡')
                        elif 'content' in lower_columns: default_col_index = lower_columns.index('content')
                        selected_col = st.selectbox(f"åˆ†æãƒ†ã‚­ã‚¹ãƒˆåˆ— ({uploaded_file.name}):", columns, index=default_col_index, key=session_key); text_column_map[uploaded_file.name] = selected_col
                    st.dataframe(preview_df)
             except Exception as e: st.error(f"ãƒ•ã‚¡ã‚¤ãƒ« {uploaded_file.name} ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼å¤±æ•—: {e}")
        if valid_files_present:
            st.header("Step 6: åˆ†æå®Ÿè¡Œ"); progress_placeholder = st.empty(); log_placeholder = st.empty(); cancel_button_placeholder = st.empty(); col1, col2 = st.columns([1, 5]); start_analysis = col1.button("ğŸ“ˆ åˆ†æå®Ÿè¡Œ", type="primary", key="analyze_button_A")
            if start_analysis:
                logger.critical("--- â˜…â˜…â˜… åˆ†æå®Ÿè¡Œãƒœã‚¿ãƒ³ CLICKED (Step A) â˜…â˜…â˜… ---"); st.session_state.analysis_done = False; st.session_state.cancel_analysis = False; st_handler = None
                try: # UIãƒ­ã‚°ãƒãƒ³ãƒ‰ãƒ©è¨­å®š
                    logger = getLogger(__name__);
                    for h in logger.handlers[:]:
                        if isinstance(h, StreamlitLogHandler): logger.removeHandler(h)
                    st.session_state.log_messages.clear(); st_handler = StreamlitLogHandler(); formatter = Formatter('%(asctime)s - %(levelname)s - %(message)s'); st_handler.setFormatter(formatter); logger.addHandler(st_handler)
                    log_placeholder.text_area("å®Ÿè¡Œãƒ­ã‚°:", "åˆ†æã‚’é–‹å§‹ã—ã¾ã™...", height=200, key="log_display_A_init", disabled=True); print("--- UIãƒ­ã‚°ãƒãƒ³ãƒ‰ãƒ©è¨­å®šå®Œäº† ---", file=sys.stderr)
                except Exception as handler_e: print(f"--- FATAL ERROR logger setup --- {handler_e}", file=sys.stderr); import traceback; traceback.print_exc(file=sys.stderr); st.error(f"ãƒ­ã‚°è¨­å®šã‚¨ãƒ©ãƒ¼: {handler_e}"); st.stop()
                with col2: # ã‚­ãƒ£ãƒ³ã‚»ãƒ«ãƒœã‚¿ãƒ³è¡¨ç¤º
                    if cancel_button_placeholder.button("â¹ï¸ åˆ†æã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«", key="cancel_button_A"): st.session_state.cancel_analysis = True; logger.warning("ã‚­ãƒ£ãƒ³ã‚»ãƒ«ãƒœã‚¿ãƒ³æŠ¼ä¸‹"); st.warning("åˆ†æã‚­ãƒ£ãƒ³ã‚»ãƒ«ä¸­...")
                logger.info("åˆ†æå‡¦ç†é–‹å§‹...");
                if not st.session_state.generated_categories or not st.session_state.selected_categories: logger.error("ã‚«ãƒ†ã‚´ãƒªæœªé¸æŠ"); st.error("ã‚«ãƒ†ã‚´ãƒªç”Ÿæˆãƒ»é¸æŠè¦"); st.stop()
                if not os.getenv("GOOGLE_API_KEY"): logger.error("APIã‚­ãƒ¼æœªè¨­å®š"); st.error("APIã‚­ãƒ¼è¨­å®šè¦"); st.stop()
                if not analysis_prompt: logger.error("åˆ†ææŒ‡é‡æœªå…¥åŠ›"); st.error("åˆ†ææŒ‡é‡å…¥åŠ›è¦"); st.stop()
                logger.info("åˆæœŸãƒã‚§ãƒƒã‚¯å®Œäº†ã€‚"); all_dfs = []; total_rows = 0; processed_rows = 0; st_handler_in_use = st_handler
                try: # ãƒ¡ã‚¤ãƒ³ã®åˆ†æå‡¦ç†
                    logger.info("AIãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–é–‹å§‹..."); llm = get_llm();
                    if llm is None: raise Exception("AIãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–å¤±æ•—")
                    logger.info("AIãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–å®Œäº†ã€‚"); st.session_state.nlp = load_spacy_model(); logger.info("spaCyãƒ¢ãƒ‡ãƒ«èª­è¾¼å®Œäº†ã€‚"); logger.info("ãƒ‡ãƒ¼ã‚¿èª­è¾¼ãƒ»çµåˆé–‹å§‹...")
                    temp_dfs = []; valid_file_count = 0
                    for up_file in uploaded_files:
                        if up_file.name not in text_column_map: logger.warning(f"{up_file.name} ã‚¹ã‚­ãƒƒãƒ—"); continue
                        text_col_name = text_column_map[up_file.name]; logger.info(f"{up_file.name} ({text_col_name}) å‡¦ç†ä¸­...")
                        df_single, _ = load_and_preprocess_data(up_file, text_col_name)
                        if df_single is None or df_single.empty: logger.warning(f"{up_file.name} ã‚¹ã‚­ãƒƒãƒ—"); continue
                        df_single.rename(columns={text_col_name: 'ANALYSIS_TEXT_COLUMN'}, inplace=True); temp_dfs.append(df_single); valid_file_count += 1; logger.info(f"{up_file.name} å‡¦ç†å®Œäº† ({len(df_single)} è¡Œ)")
                    if not temp_dfs: logger.error("æœ‰åŠ¹DFãªã—"); raise Exception("æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿ãªã—...")
                    logger.info(f"{valid_file_count} å€‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’çµåˆ..."); master_df = pd.concat(temp_dfs, ignore_index=True, sort=False); master_df['id'] = master_df.index; total_rows = len(master_df); logger.info(f"çµåˆå®Œäº†ã€‚ç·è¡Œæ•°: {total_rows}")
                    if master_df.empty: logger.error("çµåˆå¾ŒDFç©º"); raise Exception("åˆ†æå¯¾è±¡ãƒ‡ãƒ¼ã‚¿ç©º")

                    # (â˜… ã“ã“ã‹ã‚‰æ–°è¦è¿½åŠ : é‡è¤‡å‰Šé™¤ã¨AIãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°)
                    logger.info("Step A-2: é‡è¤‡å‰Šé™¤ é–‹å§‹...")
                    initial_row_count = len(master_df)
                    master_df.drop_duplicates(subset=['ANALYSIS_TEXT_COLUMN'], keep='first', inplace=True)
                    deduped_row_count = len(master_df)
                    logger.info(f"é‡è¤‡å‰Šé™¤ å®Œäº†ã€‚ {initial_row_count}è¡Œ -> {deduped_row_count}è¡Œ ({initial_row_count - deduped_row_count}è¡Œå‰Šé™¤)")
                    
                    logger.info("Step A-3: AIé–¢é€£æ€§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° é–‹å§‹...")
                    # (â˜…) ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å°‚ç”¨ã®ãƒãƒƒãƒã‚µã‚¤ã‚ºã¨å¾…æ©Ÿæ™‚é–“
                    filter_batch_size = 50 # (â˜…) 50ä»¶ãšã¤ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                    filter_sleep_time = 4.1 # (â˜…) 60s / 15 RPM = 4sã€‚ãƒãƒ¼ã‚¸ãƒ³è¾¼ã¿ã§ 4.1s
                    
                    total_filter_rows = len(master_df)
                    total_filter_batches = (total_filter_rows + filter_batch_size - 1) // filter_batch_size
                    all_filtered_results = []
                    
                    for i in range(0, total_filter_rows, filter_batch_size):
                        if st.session_state.cancel_analysis: logger.warning(f"ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ£ãƒ³ã‚»ãƒ« (ãƒãƒƒãƒ {i//filter_batch_size + 1})"); st.warning("åˆ†æã‚­ãƒ£ãƒ³ã‚»ãƒ«"); break
                        
                        batch_df = master_df.iloc[i:i+filter_batch_size]
                        current_batch_num = i // filter_batch_size + 1
                        logger.info(f"AIãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° ãƒãƒƒãƒ {current_batch_num}/{total_filter_batches} å‡¦ç†ä¸­...")
                        
                        # (â˜…) UIæ›´æ–°
                        progress_percent = min((i + filter_batch_size) / total_filter_rows, 1.0)
                        progress_text = f"[AIãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°] å‡¦ç†ä¸­: {min(i + filter_batch_size, total_filter_rows)}/{total_filter_rows} ä»¶ ({progress_percent:.0%})"
                        progress_placeholder.progress(progress_percent, text=progress_text)
                        log_text_for_ui = "\n".join(st.session_state.log_messages)
                        log_placeholder.text_area("å®Ÿè¡Œãƒ­ã‚°:", log_text_for_ui, height=200, key=f"log_update_A_filter_{i}", disabled=True)
                        
                        # (â˜…) AIãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œ
                        filtered_df = filter_relevant_data_by_ai(batch_df, analysis_prompt, llm)
                        if filtered_df is not None and not filtered_df.empty:
                            all_filtered_results.append(filtered_df)
                        else:
                            logger.warning(f"AIãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° ãƒãƒƒãƒ {current_batch_num} çµæœç©º")
                            
                        time.sleep(filter_sleep_time) # (â˜…) ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾…æ©Ÿ
                    
                    if st.session_state.cancel_analysis:
                        logger.warning("AIãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å‡¦ç†ãŒã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸã€‚")
                        raise Exception("åˆ†æãŒã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ") # (â˜…) ã‚­ãƒ£ãƒ³ã‚»ãƒ«æ™‚ã¯å‡¦ç†ä¸­æ–­

                    if not all_filtered_results:
                        logger.error("å…¨ãƒãƒƒãƒAIãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¤±æ•—"); raise Exception("AIãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å‡¦ç†å¤±æ•—")

                    logger.info("å…¨AIãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°çµæœçµåˆ...");
                    filter_results_df = pd.concat(all_filtered_results, ignore_index=True)
                    
                    # (â˜…) relevant=True ã® ID ãƒªã‚¹ãƒˆã‚’å–å¾—
                    relevant_ids = filter_results_df[filter_results_df['relevant'] == True]['id']
                    
                    # (â˜…) master_df ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                    filtered_master_df = master_df[master_df['id'].isin(relevant_ids)].copy()
                    filtered_row_count = len(filtered_master_df)
                    logger.info(f"AIãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° å®Œäº†ã€‚ {deduped_row_count}è¡Œ -> {filtered_row_count}è¡Œ ({deduped_row_count - filtered_row_count}è¡Œå‰Šé™¤)")
                    
                    if filtered_master_df.empty:
                        logger.error("AIãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã€ãƒ‡ãƒ¼ã‚¿ãŒ0ä»¶ã«ãªã‚Šã¾ã—ãŸã€‚"); raise Exception("åˆ†æå¯¾è±¡ãƒ‡ãƒ¼ã‚¿ç©º")
                    
                    # (â˜… ã“ã“ã¾ã§æ–°è¦è¿½åŠ )

                    logger.info("Step A-4: AIã‚¿ã‚°ä»˜ã‘å‡¦ç†é–‹å§‹..."); selected_category_definitions = { cat: desc for cat, desc in st.session_state.generated_categories.items() if cat in st.session_state.selected_categories }; logger.info(f"é¸æŠã‚«ãƒ†ã‚´ãƒª: {list(selected_category_definitions.keys())}")
                    
                    # (â˜…) AIã‚¿ã‚°ä»˜ã‘ã®å¯¾è±¡ã‚’ `filtered_master_df` ã«å¤‰æ›´
                    master_df_for_tagging = filtered_master_df
                    total_rows = len(master_df_for_tagging) # (â˜…) ç·è¡Œæ•°ã‚’æ›´æ–°
                    # (â˜…) åŠ¹ç‡åŒ–: ãƒãƒƒãƒã‚µã‚¤ã‚º10ã€å¾…æ©Ÿæ™‚é–“4.1ç§’ (ç´„15RPM) ã«å¤‰æ›´
                    batch_size = 10; all_tagged_results = []; total_batches = (total_rows + batch_size - 1) // batch_size; logger.info(f"ãƒãƒƒãƒã‚µã‚¤ã‚º {batch_size}, ç·ãƒãƒƒãƒæ•°: {total_batches}")
                    for i in range(0, total_rows, batch_size):
                        if st.session_state.cancel_analysis: logger.warning(f"ãƒ«ãƒ¼ãƒ—ã‚­ãƒ£ãƒ³ã‚»ãƒ« (ãƒãƒƒãƒ {i//batch_size + 1})"); st.warning("åˆ†æã‚­ãƒ£ãƒ³ã‚»ãƒ«"); break
                        
                        # (â˜…) ãƒãƒƒãƒå–å¾—å…ƒã‚’å¤‰æ›´
                        batch_df = master_df_for_tagging.iloc[i:i+batch_size]; current_batch_num = i // batch_size + 1; logger.info(f"ãƒãƒƒãƒ {current_batch_num}/{total_batches} å‡¦ç†ä¸­...")
                        
                        # (â˜…) UIæ›´æ–° (ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã®ãƒ†ã‚­ã‚¹ãƒˆå¤‰æ›´)
                        progress_percent = min((i + batch_size)/total_rows, 1.0); progress_text = f"[AIã‚¿ã‚°ä»˜ã‘] å‡¦ç†ä¸­: {min(i + batch_size, total_rows)}/{total_rows} ä»¶ ({progress_percent:.0%})"
                        progress_placeholder.progress(progress_percent, text=progress_text); log_text_for_ui = "\n".join(st.session_state.log_messages); log_placeholder.text_area("å®Ÿè¡Œãƒ­ã‚°:", log_text_for_ui, height=200, key=f"log_update_A_{i}", disabled=True)
                        logger.info(f"Calling perform_ai_tagging batch {current_batch_num}...")
                        # (â˜…) analysis_prompt ã‚’æ¸¡ã™ã‚ˆã†ã«å¤‰æ›´
                        tagged_df = perform_ai_tagging(batch_df, selected_category_definitions, llm, analysis_prompt)
                        logger.info(f"Finished perform_ai_tagging batch {current_batch_num}.")
                        if tagged_df is not None and not tagged_df.empty: all_tagged_results.append(tagged_df)
                        else: logger.warning(f"ãƒãƒƒãƒ {current_batch_num} AIå‡¦ç†çµæœç©º")
                        processed_rows += len(batch_df); time.sleep(4.1) # å¾…æ©Ÿæ™‚é–“
                    if not st.session_state.cancel_analysis:
                        if not all_tagged_results: logger.error("å…¨ãƒãƒƒãƒAIå‡¦ç†å¤±æ•—"); raise Exception("AIã‚¿ã‚°ä»˜ã‘å‡¦ç†å¤±æ•—")
                        logger.info("å…¨ãƒãƒƒãƒçµæœçµåˆ..."); results_df = pd.concat(all_tagged_results, ignore_index=True); logger.info("çµåˆå®Œäº†")
                        logger.info("AIå¿œç­”æ–‡å­—åˆ—å¤‰æ›..."); cols_to_convert = [col for col in results_df.columns if col != 'id']
                        for col in cols_to_convert:
                             if col in results_df: results_df[col] = results_df[col].apply( lambda x: ', '.join(x) if isinstance(x, list) else (str(x) if pd.notna(x) else '') )
                        logger.info("å¤‰æ›å®Œäº†")
                        logger.info("å…ƒãƒ‡ãƒ¼ã‚¿ã¨çµæœãƒãƒ¼ã‚¸...")
                        cols_in_results_df_except_id = [col for col in results_df.columns if col != 'id']; cols_to_drop_from_master = [col for col in cols_in_results_df_except_id if col in master_df.columns]
                        if cols_to_drop_from_master: logger.warning(f"é‡è¤‡åˆ—å‰Šé™¤: {cols_to_drop_from_master}"); master_df_for_merge = master_df_for_tagging.drop(columns=cols_to_drop_from_master) # (â˜…) å¤‰æ›´
                        else: master_df_for_merge = master_df_for_tagging
                        logger.info(f"Merging master ({master_df_for_merge.shape}) with results ({results_df.shape})"); final_df = master_df_for_merge.merge(results_df, on='id', how='left'); logger.info(f"ãƒãƒ¼ã‚¸å¾Œå½¢çŠ¶: {final_df.shape}")
                        logger.info(f"é¸æŠã‚«ãƒ†ã‚´ãƒªåˆ—ç¢ºèªãƒ»æ•´å½¢: {st.session_state.selected_categories}")
                        for cat in st.session_state.selected_categories:
                            if cat not in final_df.columns: logger.warning(f"åˆ— '{cat}' ãªã—ã€‚ç©ºåˆ—è¿½åŠ "); final_df[cat] = ''
                            else: final_df[cat] = final_df[cat].fillna('').astype(str); logger.info(f"åˆ— '{cat}' ç¢ºèª/ä½œæˆå®Œäº†")
                        logger.info("ãƒãƒ¼ã‚¸å®Œäº†")
                        logger.info("æ§‹é€ åˆ†æé–‹å§‹..."); st.session_state.structure_info = analyze_data_structure(final_df); st.session_state.df_flagged = final_df
                        st.session_state.analysis_done = True # æ­£å¸¸å®Œäº†
                        logger.info("åˆ†æå®Œäº†ï¼"); progress_placeholder.progress(1.0, text="å®Œäº†ï¼")
                        log_text_final = "\n".join(st.session_state.log_messages); log_placeholder.text_area("å®Ÿè¡Œãƒ­ã‚°:", log_text_final, height=200, key="log_final_A", disabled=True)
                    else: # ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚ŒãŸå ´åˆ
                        progress_placeholder.empty(); log_text_cancel = "\n".join(st.session_state.log_messages); log_placeholder.text_area("å®Ÿè¡Œãƒ­ã‚°:", log_text_cancel, height=200, key="log_cancel_A", disabled=True)
                except Exception as e:
                    error_message = f"åˆ†æå‡¦ç†ä¸­ã‚¨ãƒ©ãƒ¼: {e}"; st.error(error_message)
                    print(f"--- FATAL ERROR --- {error_message}", file=sys.stderr); import traceback; traceback.print_exc(file=sys.stderr)
                    logger.error(error_message, exc_info=True); st.session_state.analysis_done = False # ç•°å¸¸çµ‚äº†
                    progress_placeholder.empty(); log_text_error = "\n".join(st.session_state.log_messages); log_placeholder.text_area("å®Ÿè¡Œãƒ­ã‚°:", log_text_error, height=200, key="log_error_A", disabled=True)
                finally:
                    cancel_button_placeholder.empty()
                    if 'st_handler_in_use' in locals() and st_handler_in_use in logger.handlers:
                         logger.removeHandler(st_handler_in_use); print("--- UIãƒ­ã‚°ãƒãƒ³ãƒ‰ãƒ©å‰Šé™¤ ---", file=sys.stderr)
                    st.session_state.cancel_analysis = False # ãƒªã‚»ãƒƒãƒˆ
        else:
             if uploaded_files: st.warning("ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ­£ã—ãèª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸã€‚")

    # --- Step 7: çµæœè¡¨ç¤º (Step A) ---
    if st.session_state.get('analysis_done') and st.session_state.current_step == 'A':
        st.header("ğŸ‰ åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸ (Step A)")
        st.subheader("A-1: ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã®åˆ†æçµæœ"); info = st.session_state.structure_info; st.metric(label="ç·è¡Œæ•°", value=info['total_rows']); st.metric(label="ç·åˆ—æ•°", value=info['total_columns'])
        st.markdown("#### åˆ—ã”ã¨ã®è©³ç´°:"); st.dataframe(pd.DataFrame(info['column_details']).T)
        st.subheader("A-2: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºã®çµæœ (å…ˆé ­100ä»¶)"); df_result = st.session_state.df_flagged; dynamic_columns = st.session_state.get('selected_categories', ['å¸‚åŒºç”ºæ‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰'])
        def highlight_flags(row):
            color = 'background-color: #FFFFE0'
            for col in dynamic_columns:
                if row.get(col, ''): return [color] * len(row)
            return [''] * len(row)
        text_col = ['ANALYSIS_TEXT_COLUMN'] if 'ANALYSIS_TEXT_COLUMN' in df_result.columns else []
        other_cols = [col for col in df_result.columns if col not in dynamic_columns and col not in text_col and col != 'id']
        display_cols = other_cols + text_col + dynamic_columns; existing_display_cols = [col for col in display_cols if col in df_result.columns]
        st.dataframe(df_result[existing_display_cols].head(100).style.apply(highlight_flags, axis=1))
        @st.cache_data
        def convert_df_to_csv(df):
            output = BytesIO(); cols_to_download = [col for col in df.columns if col != 'id']
            df.to_csv(output, columns=cols_to_download, encoding="utf-8-sig", index=False)
            return output.getvalue()
        csv_data = convert_df_to_csv(df_result); st.download_button(label="ğŸ“¥ æŠ½å‡ºçµæœã‚’CSVã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", data=csv_data, file_name="keyword_extraction_result.csv", mime="text/csv", key="download_button_A")
        st.markdown("---"); st.header("Step B ã¸é€²ã‚€")
        if st.button("ğŸ“Š åˆ†ææ‰‹æ³•ã®ææ¡ˆã¸é€²ã‚€", key="goto_step_B"): st.session_state.current_step = 'B'; st.rerun()

# --- Step B ã® UI ã¨ãƒ­ã‚¸ãƒƒã‚¯ ---
def render_step_b():
    st.title("ğŸ“Š åˆ†ææ‰‹æ³•ã®ææ¡ˆ (Step B)")
    st.info("Step A ã§ãƒ•ãƒ©ã‚°ä»˜ã‘ã—ã€ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã—ãŸCSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚ãƒ‡ãƒ¼ã‚¿ã«é©ã—ãŸåˆ†ææ‰‹æ³•ã‚’ææ¡ˆã—ã¾ã™ã€‚")
    logger = getLogger(__name__)

    uploaded_flagged_file = st.file_uploader("ãƒ•ãƒ©ã‚°ä»˜ã‘æ¸ˆã¿CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=['csv'], key="step_b_uploader")
    
    # (â˜…) è¿½åŠ ã®åˆ†ææŒ‡ç¤ºç”¨ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒªã‚¢ã‚’è¿½åŠ 
    analysis_prompt_B = st.text_area(
        "ï¼ˆä»»æ„ï¼‰è¿½åŠ ã®åˆ†ææŒ‡ç¤º:", 
        placeholder="ä¾‹: ç‰¹å®šã®å¸‚åŒºç”ºæ‘ï¼ˆåºƒå³¶å¸‚ãªã©ï¼‰ã¨è¦³å…‰æ–½è¨­ã®ç›¸é–¢é–¢ä¿‚ã‚’æ·±æ˜ã‚Šã—ãŸã„ã€‚",
        key="step_b_prompt"
    )

    if uploaded_flagged_file:
        try:
            uploaded_flagged_file.seek(0)
            df_flagged = pd.read_csv(uploaded_flagged_file, encoding="utf-8-sig")
            st.session_state.df_flagged_B = df_flagged # (â˜…) Step C ã®ãŸã‚ã«ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ä¿å­˜
            st.success(f"ãƒ•ã‚¡ã‚¤ãƒ«ã€Œ{uploaded_flagged_file.name}ã€èª­è¾¼å®Œäº†")
            st.dataframe(df_flagged.head())

            if st.button("ğŸ’¡ åˆ†ææ‰‹æ³•ã‚’ææ¡ˆã•ã›ã‚‹", key="suggest_button_B"):
                with st.spinner("ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã¨æŒ‡ç¤ºå†…å®¹ã‚’åˆ†æã—ã€æ‰‹æ³•ã‚’ææ¡ˆä¸­..."):
                    # 1. æ§‹é€ ãƒ™ãƒ¼ã‚¹ã®ææ¡ˆ
                    base_suggestions = suggest_analysis_techniques(df_flagged)
                    
                    ai_suggestions = []
                    # 2. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ™ãƒ¼ã‚¹ã®ææ¡ˆ (æŒ‡ç¤ºãŒã‚ã‚‹å ´åˆã®ã¿)
                    if analysis_prompt_B.strip():
                        llm = get_llm()
                        if llm:
                            # (â˜…) existing_suggestions ã‚’æ¸¡ã•ãªã„ã‚ˆã†ã«å¤‰æ›´
                            ai_suggestions = get_suggestions_from_prompt(
                                analysis_prompt_B, llm, df_flagged, [] 
                            )
                        else:
                            st.error("AIãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚APIã‚­ãƒ¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

                    # 3. (â˜…) ææ¡ˆã®ãƒãƒ¼ã‚¸ã¨é‡è¤‡æ’é™¤ãƒ­ã‚¸ãƒƒã‚¯
                    base_suggestion_names = {s['name'] for s in base_suggestions} # (â˜…) æ§‹é€ ãƒ™ãƒ¼ã‚¹ã®åå‰ãƒªã‚¹ãƒˆ
                    
                    # ãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡ç¤º(AIææ¡ˆ)ã‹ã‚‰ã€æ§‹é€ ãƒ™ãƒ¼ã‚¹ã¨åå‰ãŒé‡è¤‡ã™ã‚‹ã‚‚ã®ã‚’é™¤å¤–
                    filtered_ai_suggestions = [
                        s for s in ai_suggestions if s['name'] not in base_suggestion_names # (â˜…) ãƒ­ã‚¸ãƒƒã‚¯é€†è»¢
                    ]
                    
                    # æ§‹é€ ãƒ™ãƒ¼ã‚¹(å„ªå…ˆåº¦1-5) + ãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡ç¤º(é‡è¤‡é™¤å¤–æ¸ˆ, å„ªå…ˆåº¦6) ã‚’çµåˆã—ã€å„ªå…ˆåº¦é †ã«ã‚½ãƒ¼ãƒˆ
                    all_suggestions = sorted(base_suggestions + filtered_ai_suggestions, key=lambda x: x['priority']) # (â˜…) é †åºå¤‰æ›´
                    st.session_state.suggestions_B = all_suggestions

            if 'suggestions_B' in st.session_state and st.session_state.suggestions_B:
                 display_suggestions(st.session_state.suggestions_B, df_flagged)

        except Exception as e:
            st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­è¾¼/åˆ†æææ¡ˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            logger.error("Step B error", exc_info=True)


# --- â–¼â–¼â–¼ ã“ã“ãŒé‡è¦ â–¼â–¼â–¼ ---
# Streamlit ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
# main é–¢æ•°ã‚’å®šç¾©ã—ã€ãã®ä¸­ã§ã‚¹ãƒ†ãƒƒãƒ—åˆ†å²ã‚’è¡Œã†
def main():
    st.set_page_config(page_title="ãƒ‡ãƒ¼ã‚¿åˆ†æãƒ»ãƒ•ãƒ©ã‚°ä»˜ã‘ãƒ„ãƒ¼ãƒ«", layout="wide")
    if 'log_messages' not in st.session_state:
        st.session_state.log_messages = deque(maxlen=15)
    # --- åŸºæœ¬ãƒ­ã‚¬ãƒ¼è¨­å®š ---
    logger = getLogger(__name__)
    if not any(isinstance(h, StreamHandler) and h.stream == sys.stdout for h in logger.handlers):
        if logger.hasHandlers(): logger.handlers.clear()
        handler_stdout = StreamHandler(sys.stdout)
        formatter_stdout = Formatter('%(asctime)s - %(levelname)s - %(message)s')
        handler_stdout.setFormatter(formatter_stdout)
        logger.addHandler(handler_stdout)
        logger.setLevel(DEBUG)
        # èµ·å‹•ãƒ­ã‚°ã¯ __main__ ãƒ–ãƒ­ãƒƒã‚¯ã§å‡ºã™

    if JAPAN_GEOGRAPHY_DB is None:
        st.error("é‡å¤§ãªã‚¨ãƒ©ãƒ¼: åœ°åè¾æ›¸ãƒ•ã‚¡ã‚¤ãƒ« (geography_db.py) ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        logger.critical("geography_db.py ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚åœæ­¢ã—ã¾ã™ã€‚")
        st.stop()

    # --- ã‚¹ãƒ†ãƒƒãƒ—ç®¡ç†ç”¨ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆåˆæœŸåŒ– ---
    if 'current_step' not in st.session_state:
        st.session_state.current_step = 'A' # åˆæœŸã‚¹ãƒ†ãƒƒãƒ—

    # (â˜…) --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ (å…±é€šãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³) ---
    with st.sidebar:
        st.title("Navigation")
        st.markdown("---")
        
        # APIã‚­ãƒ¼è¨­å®š (å…¨ã‚¹ãƒ†ãƒƒãƒ—ã§å…±é€šåŒ–)
        st.header("âš™ï¸ AI è¨­å®š")
        # (â˜…) ã‚­ãƒ¼ã‚’ "api_key_A" ã‹ã‚‰ "api_key_global" ã«å¤‰æ›´
        google_api_key = st.text_input("Google API Key", type="password", key="api_key_global")
        if google_api_key:
            os.environ["GOOGLE_API_KEY"] = google_api_key
        else:
            st.warning("AIæ©Ÿèƒ½ã‚’åˆ©ç”¨ã™ã‚‹ã«ã¯ Google APIã‚­ãƒ¼ ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
        
        st.markdown("---")
        
        # (â˜…) ã‚¹ãƒ†ãƒƒãƒ—é¸æŠ
        st.header("ğŸ”„ Step é¸æŠ")
        current_step = st.session_state.current_step
        
        if st.button("Step A: ã‚¿ã‚°ä»˜ã‘", key="nav_A", use_container_width=True, type=("primary" if current_step == 'A' else "secondary")):
            if st.session_state.current_step != 'A':
                st.session_state.current_step = 'A'
                st.rerun() # ãƒšãƒ¼ã‚¸ã‚’å³æ™‚å†æç”»

        if st.button("Step B: åˆ†ææ‰‹æ³•ææ¡ˆ", key="nav_B", use_container_width=True, type=("primary" if current_step == 'B' else "secondary")):
            if st.session_state.current_step != 'B':
                st.session_state.current_step = 'B'
                st.rerun() # ãƒšãƒ¼ã‚¸ã‚’å³æ™‚å†æç”»

    # --- ã‚¹ãƒ†ãƒƒãƒ—ã«å¿œã˜ã¦æç”»é–¢æ•°ã‚’å‘¼ã³å‡ºã— ---
    if st.session_state.current_step == 'A':
        render_step_a()
    elif st.session_state.current_step == 'B':
        render_step_b()
    elif st.session_state.current_step == 'C':
        render_step_c()


if __name__ == '__main__':
    # --- åŸºæœ¬ãƒ­ã‚¬ãƒ¼è¨­å®š (é‡è¤‡ã™ã‚‹ãŒã€ç›´æ¥å®Ÿè¡Œæ™‚ã«ã‚‚å¿…è¦) ---
    logger = getLogger(__name__)
    if not any(isinstance(h, StreamHandler) and h.stream == sys.stdout for h in logger.handlers):
        if logger.hasHandlers(): logger.handlers.clear()
        handler_stdout = StreamHandler(sys.stdout)
        formatter_stdout = Formatter('%(asctime)s - %(levelname)s - %(message)s')
        handler_stdout.setFormatter(formatter_stdout)
        logger.addHandler(handler_stdout)
        logger.setLevel(DEBUG)
        logger.info("--- Application Start ---") # èµ·å‹•ãƒ­ã‚°

    # --- ãƒ¡ã‚¤ãƒ³é–¢æ•°å‘¼ã³å‡ºã— ---
    main()
