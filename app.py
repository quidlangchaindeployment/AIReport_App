# ---
# app.py (AI Data Analysis App - Refactored Version)
#
# ã“ã®ã‚³ãƒ¼ãƒ‰ã¯ã€å•†ç”¨åˆ©ç”¨ãŒå®¹æ˜“ãªå¯›å®¹ãªï¼ˆpermissiveï¼‰ãƒ©ã‚¤ã‚»ãƒ³ã‚¹
# (ä¾‹: MIT, Apache License 2.0, BSD) ã®ä¸‹ã§åˆ©ç”¨å¯èƒ½ãª
# ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã€ã¾ãŸã¯ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã«ä¾å­˜ã—ãªã„ã‚³ãƒ¼ãƒ‰ã®ã¿ã‚’ä½¿ç”¨ã—ã¦å®Ÿè£…ã•ã‚Œã¦ã„ã¾ã™ã€‚
# GPL, AGPL, SSPLãªã©ã®ã‚³ãƒ”ãƒ¼ãƒ¬ãƒ•ãƒˆåŠ¹æœã‚’æŒã¤ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¯ä½¿ç”¨ã—ã¦ã„ã¾ã›ã‚“ã€‚
# ---

# --- 1. ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ---
import streamlit as st
import pandas as pd
import numpy as np
import os
import re
import json
import logging
import time
import spacy
import altair as alt
import networkx as nx
from networkx.algorithms import community
from pyvis.network import Network
import streamlit.components.v1 as components
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from io import StringIO, BytesIO
from typing import Optional, Dict, List, Any, Union, Set
from dotenv import load_dotenv  # (â˜…) .envèª­ã¿è¾¼ã¿ã®ãŸã‚ã«è¿½åŠ 
import matplotlib
matplotlib.use('Agg') # (â˜…) Streamlitã®ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã§å‹•ä½œã•ã›ã‚‹ãŸã‚ã®ãŠã¾ã˜ãªã„
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import base64
from wordcloud import WordCloud

# (â˜…) --- matplotlib æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š ---
# Dockerfileã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ãŸIPAãƒ•ã‚©ãƒ³ãƒˆã®ãƒ‘ã‚¹ã‚’æŒ‡å®š
# (â˜…) ã”æ³¨æ„: ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§å®Ÿè¡Œã™ã‚‹å ´åˆã€ã“ã®ãƒ•ã‚©ãƒ³ãƒˆãƒ‘ã‚¹ãŒç•°ãªã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
# (â˜…) Dockerç’°å¢ƒ (Debianãƒ™ãƒ¼ã‚¹) ã‚’å‰æã¨ã—ã¦ã„ã¾ã™ã€‚
try:
    # (â˜…) Dockerfileå†…ã®ãƒ‘ã‚¹
    font_path = '/usr/share/fonts/opentype/ipafont-gothic/ipagp.ttf' 
    if os.path.exists(font_path):
        fm.fontManager.addfont(font_path)
        plt.rcParams['font.family'] = 'IPAGothic'
        # logger.info(f"æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆ '{font_path}' ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")
    else:
        # (â˜…) ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ (ç’°å¢ƒã«ã‚ˆã£ã¦èª¿æ•´ãŒå¿…è¦)
        logger.warning(f"æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆ '{font_path}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚°ãƒ©ãƒ•ã®æ—¥æœ¬èªãŒæ–‡å­—åŒ–ã‘ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
        # (â˜…) ä»£æ›¿ãƒ•ã‚©ãƒ³ãƒˆã®æ¤œç´¢ (ã‚„ã‚„æ™‚é–“ãŒã‹ã‹ã‚‹ãŒå …ç‰¢)
        try:
            jp_font = fm.findfont(fm.FontProperties(family='IPAexGothic'))
            plt.rcParams['font.family'] = 'IPAexGothic'
            logger.info(f"ä»£æ›¿ãƒ•ã‚©ãƒ³ãƒˆ '{jp_font}' ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
        except:
             logger.error("ä»£æ›¿ã®æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã‚‚è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
             plt.rcParams['font.family'] = 'sans-serif'
except Exception as e:
    logger.error(f"matplotlibæ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®šã‚¨ãƒ©ãƒ¼: {e}")

# (â˜…) LangChain / Google Generative AI ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
# ãƒ©ã‚¤ã‚»ãƒ³ã‚¹: Apache License 2.0
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser

# (â˜…) Step D (PowerPointç”Ÿæˆ) ã§å¿…è¦ã¨ãªã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
# ãƒ©ã‚¤ã‚»ãƒ³ã‚¹: MIT License
try:
    import pptx
    from pptx import Presentation
    from pptx.util import Inches, Pt
except ImportError:
    st.error(
        "PowerPointç”Ÿæˆãƒ©ã‚¤ãƒ–ãƒ©ãƒª(python-pptx)ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
        "pip install python-pptx ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚"
    )

# (â˜…) Step D (ãƒ‰ãƒ©ãƒƒã‚°ï¼†ãƒ‰ãƒ­ãƒƒãƒ—UI) ã§å¿…è¦ã¨ãªã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
# ãƒ©ã‚¤ã‚»ãƒ³ã‚¹: MIT License
try:
    from streamlit_sortables import sort_items
except ImportError:
    st.error(
        "UIãƒ©ã‚¤ãƒ–ãƒ©ãƒª(streamlit-sortables)ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
        "pip install streamlit-sortables ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚"
    )

# æ—¢å­˜ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒª (openpyxl, ja_core_news_sm) ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    import openpyxl
except ImportError:
    st.error("Excel (openpyxl) ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚`pip install openpyxl` ã—ã¦ãã ã•ã„ã€‚")
try:
    import ja_core_news_sm
except ImportError:
    st.error("spaCyæ—¥æœ¬èªãƒ¢ãƒ‡ãƒ« (ja_core_news_sm) ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚`python -m spacy download ja_core_news_sm` ã—ã¦ãã ã•ã„ã€‚")

# --- 2. (â˜…) å®šæ•°å®šç¾© ---

# (â˜…) è¦ä»¶ã«åŸºã¥ãã€ä½¿ç”¨ã™ã‚‹AIãƒ¢ãƒ‡ãƒ«ã‚’å®šæ•°ã¨ã—ã¦å®šç¾©
MODEL_FLASH_LITE = "gemini-2.5-flash-lite" # Step A, B (é«˜é€Ÿãƒ»åŠ¹ç‡çš„)
MODEL_FLASH = "gemini-2.5-flash"         # Step D (ä»£æ›¿)
MODEL_PRO = "gemini-2.5-pro"             # Step C, D (é«˜å“è³ª)

# ãƒãƒƒãƒã‚µã‚¤ã‚ºã¨å¾…æ©Ÿæ™‚é–“ (KISS)
FILTER_BATCH_SIZE = 50
FILTER_SLEEP_TIME = 6.1  # Rate Limit å¯¾ç­– (10 requests per 60 seconds)
TAGGING_BATCH_SIZE = 50  
TAGGING_SLEEP_TIME = 6.1  # Rate Limit å¯¾ç­–

# åœ°åè¾æ›¸
try:
    from geography_db import JAPAN_GEOGRAPHY_DB
except ImportError:
    st.error("åœ°åè¾æ›¸ãƒ•ã‚¡ã‚¤ãƒ« (geography_db.py) ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
    JAPAN_GEOGRAPHY_DB = {}

COLOR_PALETTE = [
    "#FF5733", "#33FF57", "#3357FF", "#FF33A1", "#33FFF6",
    "#F3FF33", "#FF8C33", "#8C33FF", "#33FF8C", "#FF338C"
]

# --- 3. ãƒ­ã‚¬ãƒ¼è¨­å®š ---
class StreamlitLogHandler(logging.Handler):
    """Streamlitã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã«ãƒ­ã‚°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ ã™ã‚‹ãƒãƒ³ãƒ‰ãƒ©"""
    def __init__(self):
        super().__init__()
        if 'log_messages' not in st.session_state:
            st.session_state.log_messages = []

    def emit(self, record):
        log_entry = self.format(record)
        st.session_state.log_messages.append(log_entry)
        st.session_state.log_messages = st.session_state.log_messages[-500:]

logger = logging.getLogger(__name__)
if not logger.handlers:
    logger.setLevel(logging.INFO)
    handler = StreamlitLogHandler()
    handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
    logger.addHandler(handler)


# --- 4. (â˜…) AIãƒ¢ãƒ‡ãƒ«ãƒ»NLPãƒ¢ãƒ‡ãƒ«ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç† ---

# (â˜…) è¦ä»¶ã«åŸºã¥ãã€ç•°ãªã‚‹ãƒ¢ãƒ‡ãƒ«åã‚’æŒ‡å®šã—ã¦LLMã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹é–¢æ•°ã«åˆ·æ–°
@st.cache_resource(ttl=3600)
def get_llm(
    model_name: str, 
    temperature: float = 0.0,
    timeout_seconds: int = 120  # (â˜…) --- ä¿®æ­£: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå¼•æ•°ã‚’è¿½åŠ  (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ120ç§’) ---
) -> Optional[ChatGoogleGenerativeAI]:
    """
    æŒ‡å®šã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«åã€æ¸©åº¦ã€ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã§LLM (Google Gemini) ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ãƒ»ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚‹ã€‚
    """
    try:
        api_key = os.getenv("GOOGLE_API_KEY")
        if not api_key:
            logger.error(f"get_llm: GOOGLE_API_KEY ãŒã‚ã‚Šã¾ã›ã‚“ (Model: {model_name})")
            return None

        llm = ChatGoogleGenerativeAI(
            model=model_name,
            temperature=temperature,
            convert_system_message_to_human=True,
            api_key=api_key,
            request_timeout=timeout_seconds # (â˜…) --- ä¿®æ­£: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’æ¸¡ã™ ---
        )
        logger.info(f"LLM Model ({model_name}) loaded successfully (Timeout: {timeout_seconds}s).")
        return llm
    except Exception as e:
        logger.error(f"LLM ({model_name}) ã®åˆæœŸåŒ–ã«å¤±æ•—: {e}", exc_info=True)
        st.error(f"AIãƒ¢ãƒ‡ãƒ« ({model_name}) ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return None

@st.cache_resource
def load_spacy_model() -> Optional[spacy.language.Language]:
    """spaCyã®æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«(ja_core_news_sm)ã‚’ãƒ­ãƒ¼ãƒ‰ãƒ»ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚‹"""
    try:
        logger.info("Loading spaCy model (ja_core_news_sm)...")
        nlp = spacy.load("ja_core_news_sm")
        logger.info("spaCy model loaded successfully.")
        return nlp
    except Exception as e:
        logger.error(f"Failed to load spaCy model: {e}", exc_info=True)
        return None

@st.cache_data(ttl=3600) # 1æ™‚é–“ã‚­ãƒ£ãƒƒã‚·ãƒ¥
def get_analysis_tips_list_from_ai() -> List[str]:
    """
    (â˜…) å¾…æ©Ÿæ™‚é–“ä¸­ã«è¡¨ç¤ºã™ã‚‹ã€Œãƒ‡ãƒ¼ã‚¿åˆ†æã«é–¢ã™ã‚‹Tipsã€ã‚’AIã§ç”Ÿæˆã™ã‚‹ã€‚
    ãƒ¢ãƒ‡ãƒ«: MODEL_FLASH_LITE (gemini-2.5-flash-lite)
    """
    logger.info("get_analysis_tips_list_from_ai: AI (Flash Lite) ã§TIPSã‚’ç”Ÿæˆã—ã¾ã™ã€‚")
    llm = get_llm(model_name=MODEL_FLASH_LITE, temperature=0.5)
    if llm is None:
        return ["AIãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚"]

    prompt = PromptTemplate.from_template(
        """
        ã‚ãªãŸã¯ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ãƒ†ã‚£ã‚¹ãƒˆã®ãƒ¡ãƒ³ã‚¿ãƒ¼ã§ã™ã€‚
        ãƒ‡ãƒ¼ã‚¿åˆ†æã®åˆå¿ƒè€…ã‹ã‚‰ä¸­ç´šè€…ã«å‘ã‘ã¦ã€å½¹ç«‹ã¤ã€Œãƒ’ãƒ³ãƒˆã‚„TIPSã€ã‚’ã€10å€‹ã€‘ã€JSONã®ãƒªã‚¹ãƒˆå½¢å¼ã§ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
        
        # æŒ‡ç¤º:
        1. å„TIPSã¯ã€1ã€œ2æ–‡ã®ç°¡æ½”ãªæ—¥æœ¬èªã®æ–‡å­—åˆ—ã«ã™ã‚‹ã“ã¨ã€‚
        2. ä¾‹: ã€Œ'å¹³å‡å€¤'ã ã‘ã§ãªã'ä¸­å¤®å€¤'ã‚‚è¦‹ã‚‹ã“ã¨ã§ã€å¤–ã‚Œå€¤ã®å½±éŸ¿ã‚’æŠŠæ¡ã§ãã¾ã™ã€‚ã€
        3. ä¾‹: ã€Œãƒ‡ãƒ¼ã‚¿ã‚’å¯è¦–åŒ–ã™ã‚‹å‰ã«ã€ã¾ãšãƒ‡ãƒ¼ã‚¿ã®'æ¬ æå€¤'ã¨'å‹'ã‚’ç¢ºèªã—ã¾ã—ã‚‡ã†ã€‚ã€
        4. å‡ºåŠ›ã¯JSONãƒªã‚¹ãƒˆå½¢å¼ï¼ˆ ["TIPS1", "TIPS2", ...] ï¼‰ã®ã¿ã€‚
        
        # å›ç­” (JSONãƒªã‚¹ãƒˆå½¢å¼ã®ã¿):
        """
    )
    chain = prompt | llm | StrOutputParser()
    
    try:
        response_str = chain.invoke({})
        logger.debug(f"AI TIPSç”Ÿæˆ(ç”Ÿ): {response_str}")
        
        # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚„ä¸è¦ãªãƒ†ã‚­ã‚¹ãƒˆã‚’é™¤å»ã—ã€JSONã®ã¿ã‚’æŠ½å‡º
        match = re.search(r'\[.*\]', response_str, re.DOTALL)
        if not match:
            logger.warning("AIãŒTIPSãƒªã‚¹ãƒˆ(JSON)ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
            return ["åˆ†æTIPSã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚"]
        
        json_str = match.group(0)
        tips_list = json.loads(json_str)
        
        if isinstance(tips_list, list) and all(isinstance(tip, str) for tip in tips_list):
            logger.info(f"AI TIPS {len(tips_list)}ä»¶ã®ç”Ÿæˆã«æˆåŠŸã€‚")
            return tips_list
        else:
            raise Exception("AIã®å›ç­”ãŒæ–‡å­—åˆ—ã®ãƒªã‚¹ãƒˆå½¢å¼ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
            
    except Exception as e:
        logger.error(f"AI TIPSç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        return [
            "TIPSã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚",
            "ãƒ‡ãƒ¼ã‚¿åˆ†æã¯ã€ã¾ãšç›®çš„ï¼ˆKGI/KPIï¼‰ã‚’æ˜ç¢ºã«ã™ã‚‹ã“ã¨ã‹ã‚‰å§‹ã¾ã‚Šã¾ã™ã€‚",
            "ã€Œãªãœï¼Ÿã€ã‚’5å›ç¹°ã‚Šè¿”ã™ã“ã¨ã§ã€åˆ†æã®çœŸã®ç›®çš„ã«ãŸã©ã‚Šç€ãã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚",
            "è‰¯ã„åˆ†æã¯ã€è‰¯ã„ã€Œå•ã„ã€ã‹ã‚‰ç”Ÿã¾ã‚Œã¾ã™ã€‚",
            "ãƒ‡ãƒ¼ã‚¿ã¯ã€Œé›†ã‚ã‚‹ã€ã“ã¨ã‚ˆã‚Šã€Œã©ã†ä½¿ã†ã‹ã€ãŒé‡è¦ã§ã™ã€‚"
        ]

# --- 5. ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ãƒ˜ãƒ«ãƒ‘ãƒ¼ ---
def read_file(file: st.runtime.uploaded_file_manager.UploadedFile) -> (Optional[pd.DataFrame], Optional[str]):
    """ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«(Excel/CSV)ã‚’Pandas DataFrameã¨ã—ã¦èª­ã¿è¾¼ã‚€"""
    file_name = file.name
    logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿é–‹å§‹: {file_name}")
    try:
        if file_name.endswith('.csv'):
            # æ–‡å­—ã‚³ãƒ¼ãƒ‰ã‚’è‡ªå‹•åˆ¤åˆ¥
            try:
                content = file.getvalue().decode('utf-8-sig')
            except UnicodeDecodeError:
                logger.warning(f"UTF-8-SIGãƒ‡ã‚³ãƒ¼ãƒ‰å¤±æ•—ã€‚CP932ã§å†è©¦è¡Œ: {file_name}")
                content = file.getvalue().decode('cp932')
            df = pd.read_csv(StringIO(content))

        elif file_name.endswith(('.xlsx', '.xls')):
            df = pd.read_excel(BytesIO(file.getvalue()), engine='openpyxl')
        else:
            msg = f"ã‚µãƒãƒ¼ãƒˆå¤–ã®ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼: {file_name}"
            logger.warning(msg)
            return None, msg

        logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿æˆåŠŸ: {file_name}")
        return df, None
    except Exception as e:
        logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ ({file_name}): {e}", exc_info=True)
        st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ã€Œ{file_name}ã€ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        return None, f"èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}"


# --- 6. (â˜…) Step A: AIã‚¿ã‚°ä»˜ã‘é–¢é€£é–¢æ•° ---
# (è¦ä»¶: Step Aã¯ gemini-2.5-flash-lite ã‚’ä½¿ç”¨)

def get_dynamic_categories(analysis_prompt: str) -> Optional[Dict[str, str]]:
    """
    (Step A) ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®åˆ†ææŒ‡é‡ã«åŸºã¥ãã€AIãŒå‹•çš„ãªã‚«ãƒ†ã‚´ãƒªã‚’JSONå½¢å¼ã§ç”Ÿæˆã™ã‚‹ã€‚
    (â˜…) ãƒ¢ãƒ‡ãƒ«: MODEL_FLASH_LITE
    """
    llm = get_llm(model_name=MODEL_FLASH_LITE, temperature=0.0)
    if llm is None:
        logger.error("get_dynamic_categories: LLM (Flash Lite) ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")
        st.error("AIãƒ¢ãƒ‡ãƒ«(Flash Lite)ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§APIã‚­ãƒ¼ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
        return None

    logger.info("å‹•çš„ã‚«ãƒ†ã‚´ãƒªç”ŸæˆAI (Flash Lite) ã‚’å‘¼ã³å‡ºã—...")
    
    prompt = PromptTemplate.from_template(
        """
        ã‚ãªãŸã¯ãƒ‡ãƒ¼ã‚¿åˆ†æã®ã‚¹ã‚­ãƒ¼ãƒè¨­è¨ˆè€…ã§ã™ã€‚ã€Œåˆ†ææŒ‡é‡ã€ã‚’èª­ã¿ã€ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æŠ½å‡ºã™ã‚‹ã¹ãã€Œã‚«ãƒ†ã‚´ãƒªã€ã‚’è€ƒæ¡ˆã—ã¦ãã ã•ã„ã€‚
        
        # åˆ†ææŒ‡é‡:
        {user_prompt}

        # æŒ‡ç¤º:
        1.  ã€Œåˆ†ææŒ‡é‡ã€ã‚’æ³¨æ„æ·±ãèª­ã¿ã¾ã™ã€‚
        2.  ã‚‚ã—ã€Œåˆ†ææŒ‡é‡ã€ãŒã€ãƒ¦ãƒ¼ã‚¶ãƒ¼å®šç¾©ã®ã‚«ãƒ†ã‚´ãƒªåã¨èª¬æ˜ã€‘ï¼ˆä¾‹: ã€Œâ‘ è©±é¡Œã‚«ãƒ†ã‚´ãƒªï¼š...ã€ã‚„ã€Œâ‘¡è¦³å…‰åœ°ï¼š...ã€ï¼‰ã‚’æ˜ç¤ºçš„ã«æŒ‡å®šã—ã¦ã„ã‚‹å ´åˆã€**ãã®æŒ‡ç¤ºã«å³å¯†ã«å¾“ã„**ã€æŒ‡å®šã•ã‚ŒãŸã‚«ãƒ†ã‚´ãƒªåï¼ˆä¾‹: "è©±é¡Œã‚«ãƒ†ã‚´ãƒª"ï¼‰ã¨èª¬æ˜ï¼ˆä¾‹: "ã©ã®è©±é¡Œã«é–¢ã™ã‚‹è¨€åŠã‹..."ï¼‰ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚
        3.  ã‚‚ã—ã€Œåˆ†ææŒ‡é‡ã€ãŒã‚«ãƒ†ã‚´ãƒªã‚’æ˜ç¤ºçš„ã«æŒ‡å®šã—ã¦ã„ãªã„å ´åˆï¼ˆä¾‹: ã€Œåºƒå³¶ã®è¦³å…‰ã«ã¤ã„ã¦åˆ†æã—ãŸã„ã€ï¼‰ã€åˆ†ææŒ‡é‡ã®ãƒˆãƒ”ãƒƒã‚¯ã‚’å…ƒã«ã€æŠ½å‡ºã™ã‚‹ã¹ãã‚«ãƒ†ã‚´ãƒªï¼ˆã‚­ãƒ¼ï¼‰ã¨ã
            ã®èª¬æ˜ï¼ˆå€¤ï¼‰ã‚’ã€ã‚ãªãŸè‡ªèº«ã§è€ƒæ¡ˆã€‘ã—ã¦ãã ã•ã„ã€‚
        4.  ã€Œå¸‚åŒºç”ºæ‘ã€ã‚„ã€Œåœ°åã€ã«é–¢ã™ã‚‹ã‚«ãƒ†ã‚´ãƒªã¯ã€å¿…é ˆã‚«ãƒ†ã‚´ãƒªã¨ã—ã¦è‡ªå‹•ã§è¿½åŠ ã•ã‚Œã‚‹ãŸã‚ã€**çµ¶å¯¾ã«è€ƒæ¡ˆãƒ»æŠ½å‡ºã—ãªã„ã§ãã ã•ã„**ã€‚
        5.  å‡ºåŠ›ã¯ã€å³æ ¼ãªJSONè¾æ›¸å½¢å¼ã€‘ `{{ "ã‚«ãƒ†ã‚´ãƒªå1": "ã‚«ãƒ†ã‚´ãƒªã®èª¬æ˜1", "ã‚«ãƒ†ã‚´ãƒªå2": "ã‚«ãƒ†ã‚´ãƒªã®èª¬æ˜2" }}` ã®ã¿ã¨ã—ã¾ã™ã€‚
        6.  è©²å½“ã™ã‚‹ã‚«ãƒ†ã‚´ãƒªãŒï¼ˆåœ°åä»¥å¤–ã«ï¼‰ç„¡ã„å ´åˆã¯ã€ç©ºã®JSON `{{}}` ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚

        # å›ç­” (JSONè¾æ›¸å½¢å¼ã®ã¿):
        """
    )
    
    chain = prompt | llm | StrOutputParser()
    try:
        response_str = chain.invoke({"user_prompt": analysis_prompt})
        logger.debug(f"AIã‚«ãƒ†ã‚´ãƒªå®šç¾©(ç”Ÿ): {response_str}")

        # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚„ä¸è¦ãªãƒ†ã‚­ã‚¹ãƒˆã‚’é™¤å»ã—ã€JSONã®ã¿ã‚’æŠ½å‡º
        match = re.search(r'\{.*\}', response_str, re.DOTALL)
        if not match:
            logger.warning("AIãŒJSONå½¢å¼ã§å¿œç­”ã—ã¾ã›ã‚“ã§ã—ãŸã€‚")
            return None
        
        json_str = match.group(0).replace("'", '"')
        try:
            categories = json.loads(json_str)
            return categories
        except json.JSONDecodeError as json_e:
            logger.error(f"AIå¿œç­”ã®JSONãƒ‘ãƒ¼ã‚¹å¤±æ•—: {json_e} - Raw: {json_str}")
            return None
            
    except Exception as e:
        logger.error(f"AIã‚«ãƒ†ã‚´ãƒªç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        st.error(f"AIã‚«ãƒ†ã‚´ãƒªç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return None

def filter_relevant_data_by_ai(df_batch: pd.DataFrame, analysis_prompt: str) -> pd.DataFrame:
    """
    (Step A) AIã‚’ä½¿ã„ã€åˆ†ææŒ‡é‡ã¨ç„¡é–¢ä¿‚ãªè¡Œã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹ (relevant: true/false)ã€‚
    (â˜…) ãƒ¢ãƒ‡ãƒ«: MODEL_FLASH_LITE (gemini-2.5-flash-lite)
    (â˜…) è¦ä»¶: é€²æ—è¡¨ç¤º (ã“ã®é–¢æ•°ã¯ãƒãƒƒãƒå‡¦ç†ã®ä¸€éƒ¨ã¨ã—ã¦å‘¼ã°ã‚Œã€å‘¼ã³å‡ºã—å…ƒã®
          `render_step_a` å†…ã® `update_progress_ui` ã§é€²æ—ãŒè¡¨ç¤ºã•ã‚Œã‚‹)
    """
    # (â˜…) Step A ã®è¦ä»¶ã«åŸºã¥ãã€FLASH_LITE ãƒ¢ãƒ‡ãƒ«ã‚’æ˜ç¤ºçš„ã«æŒ‡å®š
    llm = get_llm(model_name=MODEL_FLASH_LITE, temperature=0.0)
    if llm is None:
        logger.error("filter_relevant_data_by_ai: LLM (Flash Lite) ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")
        st.error("AIãƒ¢ãƒ‡ãƒ«(Flash Lite)ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚APIã‚­ãƒ¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        return pd.DataFrame()  # ç©ºã®DF

    logger.debug(f"{len(df_batch)}ä»¶ AIé–¢é€£æ€§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° (Flash Lite) é–‹å§‹...")

    # ãƒ†ã‚­ã‚¹ãƒˆãŒé•·ã™ãã‚‹å ´åˆã€å…ˆé ­500æ–‡å­—ã«åˆ‡ã‚Šè©°ã‚ã‚‹
    input_texts_jsonl = df_batch.apply(
        lambda row: json.dumps(
            {"id": row['id'], "text": str(row['ANALYSIS_TEXT_COLUMN'])[:500]},
            ensure_ascii=False
        ),
        axis=1
    ).tolist()

    prompt = PromptTemplate.from_template(
        """
        ã‚ãªãŸã¯ãƒ‡ãƒ¼ã‚¿åˆ†æã®ã‚­ãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼ã§ã™ã€‚ã€Œåˆ†ææŒ‡é‡ã€ã«åŸºã¥ãã€ã€Œãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿(JSONL)ã€ã®å„è¡ŒãŒåˆ†æå¯¾è±¡ã¨ã—ã¦ã€é–¢é€£ã—ã¦ã„ã‚‹ã‹ (relevant: true)ã€‘ã€ã€ç„¡é–¢ä¿‚ã‹ (relevant: false)ã€‘ã‚’åˆ¤å®šã—ã¦ãã ã•ã„ã€‚
        # åˆ†ææŒ‡é‡ (Analysis Scope):
        {analysis_prompt}
        # æŒ‡ç¤º:
        1. ã€Œåˆ†ææŒ‡é‡ã€ã¨ã€å¼·ãé–¢é€£ã€‘ã™ã‚‹æŠ•ç¨¿ã®ã¿ã‚’ `true` ã¨ã™ã‚‹ã€‚
        2. å˜ãªã‚‹å®£ä¼ã€æŒ¨æ‹¶ã®ã¿ã€æŒ‡é‡ã¨ç„¡é–¢ä¿‚ãªåœ°åŸŸã®è¨€åŠã¯ `false` ã¨ã™ã‚‹ã€‚
        3. å‡ºåŠ›ã¯ã€JSONLå½¢å¼ã®ã¿ã€‘ï¼ˆid ã¨ relevant (boolean) ã‚’å«ã‚€è¾æ›¸ï¼‰ã€‚
        # ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ (JSONL):
        {text_data_jsonl}
        # å›ç­” (JSONLå½¢å¼ã®ã¿):
        """
    )
    chain = prompt | llm | StrOutputParser()
    
    try:
        invoke_params = {
            "analysis_prompt": analysis_prompt,
            "text_data_jsonl": "\n".join(input_texts_jsonl)
        }
        response_str = chain.invoke(invoke_params)
        
        results = []
        # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ ````jsonl ... ``` ã‚’é™¤å»
        match = re.search(r'```(?:jsonl|json)?\s*([\s\S]*?)\s*```', response_str, re.DOTALL)
        jsonl_content = match.group(1).strip() if match else response_str.strip()

        for line in jsonl_content.strip().split('\n'):
            cleaned_line = line.strip()
            if not cleaned_line: continue
            try:
                data = json.loads(cleaned_line)
                # relevant ãŒ "true" (str) ã‚„ true (bool) ãªã©æºã‚‰ããŒã‚ã‚‹ãŸã‚å …ç‰¢ã«å‡¦ç†
                is_relevant = False
                if isinstance(data.get("relevant"), bool):
                    is_relevant = data.get("relevant")
                elif isinstance(data.get("relevant"), str):
                    is_relevant = data.get("relevant").lower() == 'true'
                
                results.append({"id": data.get("id"), "relevant": is_relevant})
            except (json.JSONDecodeError, AttributeError) as json_e:
                logger.warning(f"AIãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å›ç­”ãƒ‘ãƒ¼ã‚¹å¤±æ•—: {cleaned_line} - Error: {json_e}")
                # ãƒ‘ãƒ¼ã‚¹å¤±æ•—æ™‚ã¯ã€IDãŒç‰¹å®šã§ãã‚Œã°é–¢é€£ã‚ã‚Š(True)ã¨ã—ã¦ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                id_match = re.search(r'"id":\s*(\d+)', cleaned_line)
                if id_match:
                    results.append({"id": int(id_match.group(1)), "relevant": True})

        return pd.DataFrame(results) if results else pd.DataFrame(columns=['id', 'relevant'])
        
    except Exception as e:
        logger.error(f"AIãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãƒãƒƒãƒå‡¦ç†ä¸­ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        st.error(f"AIãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å®‰å…¨å´ã«å€’ã—ã€ã™ã¹ã¦é–¢é€£ã‚ã‚Š(True)ã¨ã—ã¦è¿”ã™
        return df_batch[['id']].copy().assign(relevant=True)
@st.cache_data(ttl=3600)
def get_location_normalization_maps(
    db: Dict[str, List[str]], 
    analysis_prompt_str: str
) -> (Dict[str, str], Set[str], Set[str]): # (â˜…) å‹ãƒ’ãƒ³ãƒˆã« Set[str] ã‚’è¿½åŠ 
    """
    (â˜…) Step A æ”¹å–„: åœ°åæ­£è¦åŒ–ç”¨ã®è¾æ›¸ã‚’å‹•çš„ç”Ÿæˆã™ã‚‹
    JAPAN_GEOGRAPHY_DB å…¨ä½“ã‚’ã‚¹ã‚­ãƒ£ãƒ³ã—ã€ã‚¨ã‚¤ãƒªã‚¢ã‚¹è¾æ›¸ã¨æ›–æ˜§ãªå˜èªã‚»ãƒƒãƒˆã‚’ä½œæˆã™ã‚‹
    """
    if not db:
        return {}, set(), set() # (â˜…) 3ã¤ã®å€¤ã‚’è¿”ã™

    logger.info("åœ°åæ­£è¦åŒ–ãƒãƒƒãƒ—ã®å‹•çš„ç”Ÿæˆé–‹å§‹...")
    alias_to_city_map = {} # {"æ—¥å…‰": "æ—¥å…‰å¸‚", "å°¾é“": "å°¾é“å¸‚"}
    ambiguous_keys = set() # {"åºƒå³¶", "æ±äº¬", "æœ­å¹Œ"}
    prefectures = set() # {"åºƒå³¶çœŒ", "æ±äº¬éƒ½"}
    all_cities_wards = set() # {"åºƒå³¶å¸‚", "ä¸­åŒº", "æ—¥å…‰å¸‚"}
    
    # 1. DBå…¨ä½“ã‚’ã‚¹ã‚­ãƒ£ãƒ³
    for key, values in db.items():
        if not isinstance(values, list): continue

        key_normalized = key.replace("çœŒ", "").replace("éƒ½", "").replace("åºœ", "").replace("å¸‚", "")
        
        # 1a. éƒ½é“åºœçœŒ/æ”¿ä»¤å¸‚ã‚­ãƒ¼ã®å‡¦ç†
        if "çœŒ" in key or "éƒ½" in key or "åºœ" in key:
            prefectures.add(key)
            ambiguous_keys.add(key_normalized) # "åŒ—æµ·é“", "æ±äº¬", "å¤§é˜ª"
        elif "å¸‚" in key and values and "åŒº" in values[0]: # æ”¿ä»¤å¸‚
            ambiguous_keys.add(key_normalized) # "æœ­å¹Œ"
            all_cities_wards.add(key) # "æœ­å¹Œå¸‚"
        
        # 1b. å€¤ãƒªã‚¹ãƒˆ (å¸‚åŒºç”ºæ‘) ã®å‡¦ç†
        for city_or_ward in values:
            all_cities_wards.add(city_or_ward) # "å‡½é¤¨å¸‚", "ä¸­å¤®åŒº"
            
            # (â˜…) "æ—¥å…‰å¸‚" -> "æ—¥å…‰" ã®ã‚ˆã†ãªã‚¨ã‚¤ãƒªã‚¢ã‚¹ã‚’å‹•çš„ç”Ÿæˆ
            alias = city_or_ward.replace("å¸‚", "").replace("åŒº", "").replace("ç”º", "").replace("æ‘", "")
            
            if alias != city_or_ward:
                # "ä¸­å¤®" ã‚„ "å—" ã®ã‚ˆã†ãªæ±ç”¨çš„ãªåŒºåã¯ã€æ›–æ˜§ã‚­ãƒ¼ã¨ã—ã¦å‡¦ç†
                if "åŒº" in city_or_ward and len(alias) <= 2: 
                     ambiguous_keys.add(alias)
                # "æ—¥å…‰" -> "æ—¥å…‰å¸‚" ã®ãƒãƒƒãƒ”ãƒ³ã‚°
                elif alias not in alias_to_city_map:
                    alias_to_city_map[alias] = city_or_ward
                else:
                    # (â˜…) "åºœä¸­" (æ±äº¬éƒ½/åºƒå³¶çœŒ) ã®ã‚ˆã†ãªé‡è¤‡ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã¯æ›–æ˜§ã‚­ãƒ¼ã«
                    if alias in alias_to_city_map: # (â˜…) ä¿®æ­£: del ã®å‰ã«å­˜åœ¨ç¢ºèª
                        del alias_to_city_map[alias]
                    ambiguous_keys.add(alias)

    # 2. æ›–æ˜§ã‚­ãƒ¼ã‹ã‚‰ã€åˆ†ææŒ‡é‡ã§ç‰¹å®šã§ãã‚‹ã‚‚ã®ã‚’æ•‘å‡º
    prompt_lower = analysis_prompt_str.lower()
    relevant_cities = []
    for city_key in db.keys():
        if "å¸‚" in city_key and db[city_key] and "åŒº" in db[city_key][0]:
             city_name_only = city_key.replace("å¸‚", "") # "åºƒå³¶"
             if city_name_only in prompt_lower:
                 relevant_cities.append(city_key) # "åºƒå³¶å¸‚"

    if relevant_cities:
        logger.info(f"åˆ†ææŒ‡é‡ã‹ã‚‰é–¢é€£éƒ½å¸‚ã‚’ç‰¹å®š: {relevant_cities}")
        for city in relevant_cities:
            for ward in db[city]: # "ä¸­åŒº", "å—åŒº" ...
                # (â˜…) "ä¸­åŒº" -> "åºƒå³¶å¸‚ ä¸­åŒº" ã¨ã„ã†ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆ
                alias_to_city_map[ward] = f"{city} {ward}"
                
                # "ä¸­" ã®ã‚ˆã†ãªã‚¨ã‚¤ãƒªã‚¢ã‚¹ã‚‚ "åºƒå³¶å¸‚ ä¸­åŒº" ã«
                ward_alias = ward.replace("åŒº", "")
                if ward_alias in ambiguous_keys:
                    alias_to_city_map[ward_alias] = f"{city} {ward}"

    # 3. æ›–æ˜§ãªã‚­ãƒ¼ (å¸‚/çœŒ/éƒ½/åºœ ã‚’å–ã£ãŸã‚‚ã®) ã¨éƒ½é“åºœçœŒåã¯é™¤å¤–å¯¾è±¡
    final_ambiguous_set = ambiguous_keys.union(prefectures)
    
    logger.info(f"åœ°åæ­£è¦åŒ–ãƒãƒƒãƒ—å‹•çš„ç”Ÿæˆå®Œäº†ã€‚ã‚¨ã‚¤ãƒªã‚¢ã‚¹: {len(alias_to_city_map)}ä»¶, æ›–æ˜§ã‚­ãƒ¼: {len(final_ambiguous_set)}ä»¶")
    
    # (â˜…) --- [ä¿®æ­£] 3ã¤ã®å€¤ã‚’è¿”ã™ ---
    return alias_to_city_map, final_ambiguous_set, all_cities_wards

def perform_ai_tagging(
    df_batch: pd.DataFrame,
    categories_to_tag: Dict[str, str],
    analysis_prompt: str = ""
) -> pd.DataFrame:
    """
    (Step A) ãƒ†ã‚­ã‚¹ãƒˆã®ãƒãƒƒãƒã‚’å—ã‘å–ã‚Šã€AIãŒã€æŒ‡å®šã•ã‚ŒãŸã‚«ãƒ†ã‚´ãƒªå®šç¾©ã€‘ã«åŸºã¥ã„ã¦ç›´æ¥ã‚¿ã‚°ä»˜ã‘ã‚’è¡Œã†
    (â˜…) ãƒ¢ãƒ‡ãƒ«: MODEL_FLASH_LITE
    (â˜…) æ”¹å–„: AIã¯ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºã«å°‚å¿µã—ã€Pythonå´ã§åœ°åã‚’æ­£è¦åŒ–ã™ã‚‹
    """
    llm = get_llm(model_name=MODEL_FLASH_LITE, temperature=0.0)
    if llm is None:
        logger.error("perform_ai_tagging: LLM (Flash Lite) ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")
        st.error("AIãƒ¢ãƒ‡ãƒ«(Flash Lite)ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚APIã‚­ãƒ¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        return pd.DataFrame()

    logger.info(f"{len(df_batch)}ä»¶ AIã‚¿ã‚°ä»˜ã‘ (Flash Lite) é–‹å§‹ (ã‚«ãƒ†ã‚´ãƒª: {list(categories_to_tag.keys())})")

    # åœ°åè¾æ›¸ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æº–å‚™ (åˆ†ææŒ‡é‡ã«é–¢é€£ã™ã‚‹åœ°åã®ã¿ã‚’AIã«æ¸¡ã™)
    geo_context_str = "{}"
    if JAPAN_GEOGRAPHY_DB and "å¸‚åŒºç”ºæ‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰" in categories_to_tag:
        try:
            relevant_geo_db = {}
            prompt_lower = analysis_prompt.lower()
            
            hints = ["åºƒå³¶", "ç¦å²¡", "å¤§é˜ª", "æ±äº¬", "åŒ—æµ·é“", "æ„›çŸ¥", "å®®åŸ", "æœ­å¹Œ", "æ¨ªæµœ", "åå¤å±‹", "äº¬éƒ½", "ç¥æˆ¸", "ä»™å°"]
            keys_found = [
                key for key in JAPAN_GEOGRAPHY_DB.keys()
                if any(h in key.lower() for h in hints) and any(h in prompt_lower for h in hints)
            ]
            if "åºƒå³¶" in prompt_lower: keys_found.extend(["åºƒå³¶çœŒ", "åºƒå³¶å¸‚"])
            if "æ±äº¬" in prompt_lower: keys_found.extend(["æ±äº¬éƒ½", "æ±äº¬23åŒº"])
            if "å¤§é˜ª" in prompt_lower: keys_found.extend(["å¤§é˜ªåºœ", "å¤§é˜ªå¸‚"])

            for key in set(keys_found): 
                if key in JAPAN_GEOGRAPHY_DB:
                    relevant_geo_db[key] = JAPAN_GEOGRAPHY_DB[key]
            
            if not relevant_geo_db:
                logger.warning("åœ°åè¾æ›¸ã®çµã‚Šè¾¼ã¿ãƒ’ãƒ³ãƒˆãªã—ã€‚ä¸»è¦éƒ½å¸‚ã®ã¿æ¸¡ã—ã¾ã™ã€‚")
                default_keys = ["æ±äº¬éƒ½", "æ±äº¬23åŒº", "å¤§é˜ªåºœ", "å¤§é˜ªå¸‚", "åºƒå³¶çœŒ", "åºƒå³¶å¸‚", "ç¦å²¡çœŒ", "ç¦å²¡å¸‚"]
                for key in default_keys:
                    if key in JAPAN_GEOGRAPHY_DB:
                        relevant_geo_db[key] = JAPAN_GEOGRAPHY_DB[key]

            geo_context_str = json.dumps(relevant_geo_db, ensure_ascii=False, indent=2)
            
            if len(geo_context_str) > 5000:
                logger.warning(f"åœ°åè¾æ›¸ãŒå¤§ãã™ã ({len(geo_context_str)}B)ã€‚ã‚­ãƒ¼ã®ã¿ã«ç¸®å°ã€‚")
                geo_context_str = json.dumps(list(relevant_geo_db.keys()), ensure_ascii=False)
                
            logger.info(f"AIã«æ¸¡ã™åœ°åè¾æ›¸(çµè¾¼æ¸ˆ): {list(relevant_geo_db.keys())}")
            
            # (â˜…) --- [ä¿®æ­£] 3ã¤ã®å€¤ã‚’å—ã‘å–ã‚‹ ---
            alias_map, ambiguous_set, all_cities_wards = get_location_normalization_maps(JAPAN_GEOGRAPHY_DB, analysis_prompt)
            # (â˜…) --- ã“ã“ã¾ã§ ---

        except Exception as e:
            logger.error(f"åœ°åè¾æ›¸ã®æº–å‚™ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            geo_context_str = "{}" 
            # (â˜…) --- [ä¿®æ­£] 3ã¤ã®å¤‰æ•°ã‚’åˆæœŸåŒ– ---
            alias_map, ambiguous_set, all_cities_wards = {}, set(), set()
            
    else:
        # (â˜…) --- [ä¿®æ­£] 3ã¤ã®å¤‰æ•°ã‚’åˆæœŸåŒ– ---
        alias_map, ambiguous_set, all_cities_wards = {}, set(), set() 

    # ãƒ†ã‚­ã‚¹ãƒˆãŒé•·ã™ãã‚‹å ´åˆã€å…ˆé ­500æ–‡å­—ã«åˆ‡ã‚Šè©°ã‚ã‚‹
    input_texts_jsonl = df_batch.apply(
        lambda row: json.dumps(
            {"id": row['id'], "text": str(row['ANALYSIS_TEXT_COLUMN'])[:500]},
            ensure_ascii=False
        ),
        axis=1
    ).tolist()

    # (â˜…) ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯å¤‰æ›´ãªã— (AIã¯ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºã™ã‚‹ã ã‘)
    prompt = PromptTemplate.from_template(
        """
        ã‚ãªãŸã¯é«˜ç²¾åº¦ãƒ‡ãƒ¼ã‚¿åˆ†æã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ã€Œã‚«ãƒ†ã‚´ãƒªå®šç¾©ã€ã€Œåœ°åè¾æ›¸ã€ã€Œåˆ†ææŒ‡é‡ã€ã«åŸºã¥ãã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºã—ã¾ã™ã€‚
        # åˆ†ææŒ‡é‡ (Analysis Scope): {analysis_prompt}
        # åœ°åè¾æ›¸ (JAPAN_GEOGRAPHY_DB): {geo_context}
        # ã‚«ãƒ†ã‚´ãƒªå®šç¾© (categories): {categories}
        # ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ (JSONL): {text_data_jsonl}

        # æŒ‡ç¤º:
        1. ã€Œãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿(JSONL)ã€ã®å„è¡Œã‚’å‡¦ç†ã™ã‚‹ã€‚
        2. ã€Œã‚«ãƒ†ã‚´ãƒªå®šç¾©ã€ã®ã‚­ãƒ¼åã‚’ã€å³æ ¼ã«ã€‘ä½¿ç”¨ã—ã€å…¨ã‚«ãƒ†ã‚´ãƒªã‚’æŠ½å‡ºã™ã‚‹ã€‚
        3. ã€ã™ã¹ã¦ã®ã‚«ãƒ†ã‚´ãƒªã€‘ ( "å¸‚åŒºç”ºæ‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰" ã‚’å«ã‚€ ):
           - å€¤ã¯ã€å˜ä¸€ã®æ–‡å­—åˆ—ã€‘ã§å‡ºåŠ›ã™ã‚‹ (è©²å½“ãªã‘ã‚Œã°ç©ºæ–‡å­—åˆ— "")ã€‚ãƒªã‚¹ãƒˆå½¢å¼ã¯ã€å³ç¦ã€‘ã€‚
           - æ–‡è„ˆã‹ã‚‰æœ€ã‚‚é–¢é€£æ€§ã®é«˜ã„ã‚‚ã®ã‚’ã€1ã¤ã ã‘ã€‘é¸ã¶ã€‚
           - åˆ†ææŒ‡é‡ã§ã‚«ãƒ†ã‚´ãƒªã¨ãã®å†…å®¹ã®é¸æŠè‚¢ãŒæç¤ºã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€ãã‚Œã«å¾“ã„ãƒ©ãƒ™ãƒ«ä»˜ã‘ã‚’è¡Œã†ã€‚
        4. ã€"å¸‚åŒºç”ºæ‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰" ã®ç‰¹åˆ¥ãƒ«ãƒ¼ãƒ«ã€‘(â˜… å¤‰æ›´ç‚¹):
           - ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã€æœ€ã‚‚é–¢é€£æ€§ãŒé«˜ã„åœ°åã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆä¾‹ï¼šã€Œå®®å³¶ã€ã€Œæ—¥å…‰ã€ã€Œå°¾é“ã€ã€Œä¸­åŒºã€ã€Œåºƒå³¶å¸‚ã€ï¼‰ã‚’ã€1ã¤ã ã‘ãã®ã¾ã¾ã€‘æŠ½å‡ºã™ã‚‹ã€‚
           - ã€å¤‰æ›å‡¦ç†ã¯ä¸è¦ã€‘ã§ã™ï¼ˆä¾‹ï¼šã€Œå®®å³¶ã€ã‚’ã€Œå»¿æ—¥å¸‚å¸‚ã€ã«å¤‰æ›ã—ãªã„ã§ãã ã•ã„ï¼‰ã€‚
           - æ›–æ˜§ãªè¡¨ç¾ï¼ˆä¾‹ï¼šã€Œåºƒå³¶ã€ï¼‰ã‚„éƒ½é“åºœçœŒåï¼ˆä¾‹ï¼šã€Œåºƒå³¶çœŒã€ï¼‰ã‚‚ã€ã‚‚ã—ãã‚ŒãŒæœ€ã‚‚é–¢é€£æ€§ãŒé«˜ã„ã¨åˆ¤æ–­ã—ãŸå ´åˆã¯ã€ãã®ã¾ã¾æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚
           - ã€Œåˆ†ææŒ‡é‡ã€ã¨ç„¡é–¢ä¿‚ãªåœ°åŸŸã®åœ°åã¯ã€æŠ½å‡ºã—ãªã„ã€‘ã€‚
        5. ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆæƒ…å ±ã®æé€ ï¼‰ç¦æ­¢ã€‚
        6. å‡ºåŠ›ã¯ã€JSONLå½¢å¼ã®ã¿ã€‘ï¼ˆid ã¨ categories ã‚’å«ã‚€è¾æ›¸ï¼‰ã€‚

        # å›ç­” (JSONLå½¢å¼ã®ã¿):
        """
    )
    chain = prompt | llm | StrOutputParser()
    
    try:
        invoke_params = {
            "categories": json.dumps(categories_to_tag, ensure_ascii=False),
            "geo_context": geo_context_str,
            "text_data_jsonl": "\n".join(input_texts_jsonl),
            "analysis_prompt": analysis_prompt
        }
        response_str = chain.invoke(invoke_params)
        logger.debug(f"AI Tagging - Raw response received.")

        results = []
        expected_keys = list(categories_to_tag.keys())
        
        match = re.search(r'```(?:jsonl|json)?\s*([\s\S]*?)\s*```', response_str, re.DOTALL)
        jsonl_content = match.group(1).strip() if match else response_str.strip()

        for line in jsonl_content.strip().split('\n'):
            cleaned_line = line.strip()
            if not cleaned_line: continue
            try:
                data = json.loads(cleaned_line)
                row_result = {"id": data.get("id")}
                tag_source = data.get('categories', data)
                
                if not isinstance(tag_source, dict):
                    raise json.JSONDecodeError(f"tag_source is not a dict: {tag_source}", "", 0)

                for key in expected_keys:
                    found_key = None
                    for resp_key in tag_source.keys():
                        if str(resp_key).strip() == key:
                            found_key = resp_key
                            break
                    
                    raw_value = tag_source.get(found_key) if found_key else None
                    processed_value = ""
                    if isinstance(raw_value, list) and raw_value:
                        processed_value = str(raw_value[0]).strip()
                    elif raw_value is not None and str(raw_value).strip():
                        processed_value = str(raw_value).strip()
                    
                    if processed_value.lower() in ["è©²å½“ãªã—", "none", "null", "", "n/a"]:
                        processed_value = ""
                    
                    # (â˜…) --- [ä¿®æ­£] Python åœ°åæ­£è¦åŒ–ãƒ­ã‚¸ãƒƒã‚¯ (all_cities_wards ãŒä½¿ãˆã‚‹) ---
                    if key == "å¸‚åŒºç”ºæ‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰" and processed_value:
                        
                        # 1. ã‚¨ã‚¤ãƒªã‚¢ã‚¹ãƒãƒƒãƒ—ã§å¤‰æ› (ä¾‹: "æ—¥å…‰" -> "æ—¥å…‰å¸‚", "ä¸­åŒº" -> "åºƒå³¶å¸‚ ä¸­åŒº")
                        if processed_value in alias_map:
                            processed_value = alias_map[processed_value]
                        
                        # 2. æ›–æ˜§ãªã‚­ãƒ¼ (ä¾‹: "åºƒå³¶", "æ±äº¬", "æœ­å¹Œ") ã¯ç ´æ£„
                        elif processed_value in ambiguous_set:
                            logger.debug(f"åœ°åæ­£è¦åŒ–: æ›–æ˜§ãªã‚­ãƒ¼ '{processed_value}' ã‚’ç ´æ£„ã—ã¾ã—ãŸã€‚")
                            processed_value = ""
                        
                        # 3. éƒ½é“åºœçœŒå (ä¾‹: "åºƒå³¶çœŒ") ã¯ç ´æ£„ (ambiguous_set ã«å«ã¾ã‚Œã‚‹)
                        
                        # 4. DBã«å­˜åœ¨ã™ã‚‹æ­£å¼åç§° (ä¾‹: "åºƒå³¶å¸‚") ã‹ç¢ºèª
                        elif processed_value in all_cities_wards:
                            pass # (ä¾‹: "åºƒå³¶å¸‚" ã¯ãã®ã¾ã¾é€šã™)
                        
                        # 5. ãã‚Œä»¥å¤– (ä¾‹: "ã‚¢ãƒ¡ãƒªã‚«") ã¯ç ´æ£„
                        else:
                            # (â˜…) ãŸã ã—ã€"åºƒå³¶å¸‚ ä¸­åŒº" ã®ã‚ˆã†ãªã€Œå¸‚ åŒºã€å½¢å¼ã¯è¨±å¯
                            if " " in processed_value and any(s in processed_value for s in ["å¸‚", "åŒº"]):
                                pass
                            else:
                                logger.debug(f"åœ°åæ­£è¦åŒ–: ä¸æ˜ãªã‚­ãƒ¼ '{processed_value}' ã‚’ç ´æ£„ã—ã¾ã—ãŸã€‚")
                                processed_value = ""
                    # (â˜…) --- æ­£è¦åŒ–ãƒ­ã‚¸ãƒƒã‚¯ã“ã“ã¾ã§ ---

                    row_result[key] = processed_value
                
                results.append(row_result)
                
            except (json.JSONDecodeError, AttributeError) as json_e:
                logger.warning(f"AIã‚¿ã‚°ä»˜ã‘å›ç­”ãƒ‘ãƒ¼ã‚¹å¤±æ•—: {cleaned_line} - Error: {json_e}")
                id_match = re.search(r'"id":\s*(\d+)', cleaned_line)
                if id_match:
                    results.append({"id": int(id_match.group(1))})
                    
        return pd.DataFrame(results) if results else pd.DataFrame(columns=['id'] + list(expected_keys))

    except Exception as e:
        logger.error(f"AIã‚¿ã‚°ä»˜ã‘ãƒãƒƒãƒå‡¦ç†ä¸­ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        st.error(f"AIã‚¿ã‚°ä»˜ã‘å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        return pd.DataFrame()

def perform_ai_location_inference(
    df_batch: pd.DataFrame,
    analysis_prompt: str,
    normalization_maps: tuple
) -> pd.DataFrame:
    """
    (â˜… Pass 2) AIã‚’ä½¿ã„ã€æŠ•ç¨¿å†…å®¹ã‚„ä»–ã‚«ãƒ†ã‚´ãƒªã‹ã‚‰é–“æ¥çš„ã«åœ°åã‚’ã€Œæ¨è«–ã€ã™ã‚‹
    (â˜…) ãƒ¢ãƒ‡ãƒ«: MODEL_FLASH_LITE
    """
    llm = get_llm(model_name=MODEL_FLASH_LITE, temperature=0.0)
    if llm is None:
        logger.error("perform_ai_location_inference: LLM (Flash Lite) ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")
        return pd.DataFrame()

    logger.info(f"{len(df_batch)}ä»¶ AIåœ°åæ¨è«– (Flash Lite) é–‹å§‹...")
    
    # (â˜…) æ­£è¦åŒ–ãƒãƒƒãƒ—ã‚’ã‚¢ãƒ³ãƒ‘ãƒƒã‚¯ (AIã®å›ç­”ã‚’æ¤œè¨¼ã™ã‚‹ãŸã‚)
    alias_map, ambiguous_set, all_cities_wards = normalization_maps

    # AIã«æ¸¡ã™ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’JSONLå½¢å¼ã§ä½œæˆ
    # (â˜…) æŠ•ç¨¿æœ¬æ–‡ + Pass 1 ã§ã‚¿ã‚°ä»˜ã‘ã•ã‚ŒãŸå…¨ã‚«ãƒ†ã‚´ãƒª ã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«ã™ã‚‹
    def create_context(row):
        context_data = {
            "id": row['id'],
            "text": str(row['ANALYSIS_TEXT_COLUMN'])[:500]
        }
        # ä»–ã®ã‚«ãƒ†ã‚´ãƒªï¼ˆè¦³å…‰åœ°ã€è¾²ç”£å“ãªã©ï¼‰ã‚’ãƒ’ãƒ³ãƒˆã¨ã—ã¦è¿½åŠ 
        other_tags = {
            k: v for k, v in row.items() 
            if k not in ['id', 'ANALYSIS_TEXT_COLUMN', 'å¸‚åŒºç”ºæ‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰'] and pd.notna(v) and str(v).strip()
        }
        if other_tags:
            context_data["other_tags_context"] = other_tags
        return json.dumps(context_data, ensure_ascii=False)

    input_contexts_jsonl = df_batch.apply(create_context, axis=1).tolist()

    prompt = PromptTemplate.from_template(
        """
        ã‚ãªãŸã¯æ—¥æœ¬ã®åœ°ç†ã«ç²¾é€šã—ãŸåœ°åæ¨è«–AIã§ã™ã€‚
        ã€Œåˆ†ææŒ‡é‡ã€ã¨ã€Œã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿(JSONL)ã€ã‚’èª­ã¿ã€å„ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æœ€ã‚‚é–¢é€£æ€§ã®é«˜ã„ã€Œå¸‚åŒºç”ºæ‘åã€ã‚’ã€1ã¤ã ã‘ã€‘æ¨è«–ã—ã¦ãã ã•ã„ã€‚

        # åˆ†ææŒ‡é‡ (Analysis Scope): {analysis_prompt}
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ (JSONL):
        {text_data_jsonl}

        # æŒ‡ç¤º:
        1.  `text`ï¼ˆæŠ•ç¨¿æœ¬æ–‡ï¼‰ã‚„ `other_tags_context`ï¼ˆä»–ã®ã‚«ãƒ†ã‚´ãƒªã®ãƒ’ãƒ³ãƒˆï¼‰ã‚’æ³¨æ„æ·±ãèª­ã¿ã¾ã™ã€‚
        2.  ãƒ’ãƒ³ãƒˆã‹ã‚‰ã€æœ€ã‚‚å¯èƒ½æ€§ã®é«˜ã„ã€Œå¸‚åŒºç”ºæ‘åã€ã‚’ã€1ã¤ã ã‘ã€‘æ¨è«–ã—ã¾ã™ã€‚
            (ä¾‹: "å³å³¶ç¥ç¤¾" -> "å»¿æ—¥å¸‚å¸‚")
            (ä¾‹: "é‚£é ˆã®ç‰›ä¹³" -> "é‚£é ˆå¡©åŸå¸‚" ã¾ãŸã¯ "é‚£é ˆç”º")
            (ä¾‹: "ãƒ†ã‚­ã‚¹ãƒˆã«ã€Œæœ­å¹Œã€ã¨ã‚ã‚Šã€other_tagsã«ã€Œä¸­å¤®ã€ã¨ã‚ã‚Œã°" -> "æœ­å¹Œå¸‚ ä¸­å¤®åŒº")
        3.  æ¨è«–ã—ãŸåœ°åã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆä¾‹: "å»¿æ—¥å¸‚å¸‚"ï¼‰ã‚’ `inferred_location` ã¨ã—ã¦è¿”ã—ã¾ã™ã€‚
        4.  åœ°åã®å¤‰æ›ã¯ä¸è¦ã§ã™ï¼ˆä¾‹ï¼šã€Œå»¿æ—¥å¸‚å¸‚ã€ã‚’ã€Œå»¿æ—¥å¸‚ã€ã«ã—ãªã„ã§ãã ã•ã„ï¼‰ã€‚
        5.  æ¨è«–ã§ããªã„ã€ã¾ãŸã¯ã€Œåˆ†ææŒ‡é‡ã€ã¨ç„¡é–¢ä¿‚ãªå ´åˆã¯ `null` ã‚’è¿”ã—ã¾ã™ã€‚
        6.  å‡ºåŠ›ã¯ã€JSONLå½¢å¼ã®ã¿ã€‘ï¼ˆid ã¨ inferred_location (string or null) ã‚’å«ã‚€è¾æ›¸ï¼‰ã€‚

        # å›ç­” (JSONLå½¢å¼ã®ã¿):
        """
    )
    chain = prompt | llm | StrOutputParser()
    
    try:
        invoke_params = {
            "analysis_prompt": analysis_prompt,
            "text_data_jsonl": "\n".join(input_contexts_jsonl)
        }
        response_str = chain.invoke(invoke_params)
        logger.debug(f"AI Location Inference - Raw response received.")

        results = []
        
        match = re.search(r'```(?:jsonl|json)?\s*([\s\S]*?)\s*```', response_str, re.DOTALL)
        jsonl_content = match.group(1).strip() if match else response_str.strip()

        for line in jsonl_content.strip().split('\n'):
            cleaned_line = line.strip()
            if not cleaned_line: continue
            try:
                data = json.loads(cleaned_line)
                inferred_value = data.get("inferred_location")
                
                processed_value = ""
                if inferred_value and isinstance(inferred_value, str):
                    processed_value = inferred_value.strip()
                
                if processed_value.lower() in ["è©²å½“ãªã—", "none", "null", "", "n/a"]:
                    processed_value = ""
                
                # (â˜…) --- æ¨è«–çµæœã‚‚Pythonå´ã§å³å¯†ã«æ­£è¦åŒ–ãƒ»æ¤œè¨¼ ---
                if processed_value:
                    # 1. ã‚¨ã‚¤ãƒªã‚¢ã‚¹ãƒãƒƒãƒ—ã§å¤‰æ› (ä¾‹: AIãŒ "æ—¥å…‰" ã¨è¿”ã—ãŸå ´åˆ)
                    if processed_value in alias_map:
                        processed_value = alias_map[processed_value]
                    
                    # 2. æ›–æ˜§ãªã‚­ãƒ¼ (ä¾‹: "åºƒå³¶") ã¯ç ´æ£„
                    elif processed_value in ambiguous_set:
                        processed_value = ""
                    
                    # 3. DBã«å­˜åœ¨ã™ã‚‹æ­£å¼åç§° (ä¾‹: "å»¿æ—¥å¸‚å¸‚") ã‹ç¢ºèª
                    elif processed_value in all_cities_wards:
                        pass # OK
                    
                    # 4. ãã‚Œä»¥å¤– (ä¾‹: "ã‚¢ãƒ¡ãƒªã‚«") ã¯ç ´æ£„
                    else:
                        if " " in processed_value and any(s in processed_value for s in ["å¸‚", "åŒº"]):
                            pass # "æœ­å¹Œå¸‚ ä¸­å¤®åŒº" ã¯ OK
                        else:
                            processed_value = "" # ä¸æ˜ãªåœ°åã¨ã—ã¦ç ´æ£„
                # (â˜…) --- æ­£è¦åŒ–ãƒ­ã‚¸ãƒƒã‚¯ã“ã“ã¾ã§ ---
                
                results.append({
                    "id": data.get("id"),
                    "inferred_location": processed_value
                })
                
            except (json.JSONDecodeError, AttributeError) as json_e:
                logger.warning(f"AIåœ°åæ¨è«– å›ç­”ãƒ‘ãƒ¼ã‚¹å¤±æ•—: {cleaned_line} - Error: {json_e}")
                id_match = re.search(r'"id":\s*(\d+)', cleaned_line)
                if id_match:
                    results.append({"id": int(id_match.group(1)), "inferred_location": ""})
                    
        return pd.DataFrame(results) if results else pd.DataFrame(columns=['id', 'inferred_location'])

    except Exception as e:
        logger.error(f"AIåœ°åæ¨è«– ãƒãƒƒãƒå‡¦ç†ä¸­ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        st.error(f"AIåœ°åæ¨è«–å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        return pd.DataFrame() # å¤±æ•—æ™‚ã¯ç©ºã®DF

# --- 7. (â˜…) Step A: UIæç”»é–¢æ•° ---

def update_progress_ui(
    progress_placeholder: st.delta_generator.DeltaGenerator,
    log_placeholder: st.delta_generator.DeltaGenerator,
    tip_placeholder: st.delta_generator.DeltaGenerator,  # (â˜…) Tipsç”¨ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€
    processed_rows: int,
    total_rows: int,
    message_prefix: str
):
    """
    (Step A) ã®é€²æ—ãƒãƒ¼ã¨ãƒ­ã‚°ã‚¨ãƒªã‚¢ã‚’æ›´æ–°ã™ã‚‹ (DRY)
    (â˜…) è¦ä»¶: AIèª­ã¿è¾¼ã¿æ™‚é–“ã®é€²æ—ã‚’0ï½100ï¼…ã§è¡¨ç¤º
    (â˜…) è¦ä»¶: AI Tipsã‚’ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³è¡¨ç¤º
    """
    try:
        # total_rows ãŒ 0 ã®å ´åˆ DivisionByZero ã‚’é˜²ã
        if total_rows == 0:
            progress_percent = 1.0
        else:
            progress_percent = min(processed_rows / total_rows, 1.0)
            
        progress_text = f"[{message_prefix}] å‡¦ç†ä¸­: {processed_rows}/{total_rows} ä»¶ ({progress_percent:.0%})"
        progress_placeholder.progress(progress_percent, text=progress_text)

        # ãƒ­ã‚°è¡¨ç¤º (æœ€æ–°50ä»¶)
        log_text_for_ui = "\n".join(st.session_state.log_messages[-50:])
        log_placeholder.text_area(
            "å®Ÿè¡Œãƒ­ã‚° (æœ€æ–°50ä»¶):",
            log_text_for_ui,
            height=200,
            key=f"log_update_{message_prefix}_{processed_rows}", # é‡è¤‡ã‚­ãƒ¼ã‚’é¿ã‘ã‚‹
            disabled=True
        )
        
        # (â˜…) --- AI Tips ã®ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³è¡¨ç¤º ---
        if 'tips_list' not in st.session_state or not st.session_state.tips_list:
             # ä¸‡ãŒä¸€TipsãŒç©ºã®å ´åˆã¯AI Tipsé–¢æ•°ã‚’å‘¼ã³å‡ºã™
             st.session_state.tips_list = get_analysis_tips_list_from_ai()
             st.session_state.current_tip_index = 0
             st.session_state.last_tip_time = time.time()

        now = time.time()
        # 60ç§’ã”ã¨ï¼ˆã¾ãŸã¯TIPSãŒ1ä»¶ã—ã‹ãªã„å ´åˆï¼‰ã«TIPSã‚’æ›´æ–°
        if (now - st.session_state.last_tip_time > 60) or (len(st.session_state.tips_list) == 1):
            if len(st.session_state.tips_list) > 1:
                st.session_state.current_tip_index = (st.session_state.current_tip_index + 1) % len(st.session_state.tips_list)
            st.session_state.last_tip_time = now
        
        # (â˜…) ãƒªã‚¹ãƒˆãŒç©ºã§ãªã„ã‹ãƒã‚§ãƒƒã‚¯
        if st.session_state.tips_list:
            current_tip = st.session_state.tips_list[st.session_state.current_tip_index]
            tip_placeholder.info(f"ğŸ’¡ ãƒ‡ãƒ¼ã‚¿åˆ†æTIPS: {current_tip}")

    except Exception as e:
        # UIã®æ›´æ–°ã‚¨ãƒ©ãƒ¼ã¯ãƒ­ã‚°ã«è­¦å‘Šã®ã¿æ®‹ã—ã€å‡¦ç†ã¯ç¶šè¡Œ
        logger.warning(f"UI update failed: {e}")

def render_step_a():
    """(Step A) ã‚¿ã‚°ä»˜ã‘å‡¦ç†ã®UIã‚’æç”»ã™ã‚‹"""
    st.title("ğŸ·ï¸ Step A: AIã‚¿ã‚°ä»˜ã‘ & ã‚­ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³")

    # Step A å›ºæœ‰ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã‚’åˆæœŸåŒ–
    if 'cancel_analysis' not in st.session_state:
        st.session_state.cancel_analysis = False
    if 'generated_categories' not in st.session_state:
        st.session_state.generated_categories = {}
    if 'selected_categories' not in st.session_state:
        st.session_state.selected_categories = set()
    if 'analysis_prompt_A' not in st.session_state:
        st.session_state.analysis_prompt_A = ""
    if 'selected_text_col' not in st.session_state:
        st.session_state.selected_text_col = {}
    if 'tagged_df_A' not in st.session_state:
        st.session_state.tagged_df_A = pd.DataFrame()

    st.header("Step 1: åˆ†æå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
    uploaded_files = st.file_uploader(
        "åˆ†æã—ãŸã„ Excel / CSV ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆè¤‡æ•°å¯ï¼‰",
        type=['csv', 'xlsx', 'xls'],
        accept_multiple_files=True,
        key="uploader_A"
    )

    if not uploaded_files:
        st.info("åˆ†æã‚’é–‹å§‹ã™ã‚‹ã«ã¯ã€Excelã¾ãŸã¯CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
        return

    # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å‡¦ç†
    valid_files_data = {}
    error_messages = []
    for f in uploaded_files:
        df, err = read_file(f)
        if err:
            error_messages.append(f"**{f.name}**: {err}")
        else:
            valid_files_data[f.name] = df
            
    if error_messages:
        st.error("ä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯èª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸ:\n" + "\n".join(error_messages))
    if not valid_files_data:
        st.warning("èª­ã¿è¾¼ã¿å¯èƒ½ãªãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return

    st.header("Step 2: åˆ†ææŒ‡é‡ã®å…¥åŠ›ã¨ã‚«ãƒ†ã‚´ãƒªç”Ÿæˆ")
    analysis_prompt = st.text_area(
        "AIãŒã‚¿ã‚°ä»˜ã‘ã¨ã‚­ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¡Œã†éš›ã®æŒ‡é‡ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆå¿…é ˆï¼‰:",
        value=st.session_state.analysis_prompt_A,
        height=100,
        placeholder="ä¾‹: åºƒå³¶çœŒã®è¦³å…‰ã«é–¢ã™ã‚‹Instagramã®æŠ•ç¨¿ã€‚ç„¡é–¢ä¿‚ãªåœ°åŸŸã®æŠ•ç¨¿ã‚„ã€å˜ãªã‚‹æŒ¨æ‹¶ãƒ»å®£ä¼ã¯é™¤å¤–ã—ãŸã„ã€‚\nä¾‹: â‘ è¾²ç”£å“ã‚«ãƒ†ã‚´ãƒªï¼ˆç‰›ä¹³,ãƒãƒ¼ã‚º,ç±³ï¼‰ â‘¡è¾²ç”£å“ã®ã‚¤ãƒ¡ãƒ¼ã‚¸ï¼ˆæ¿ƒåš,æ–°é®®ï¼‰",
        key="analysis_prompt_input_A"
    )
    st.session_state.analysis_prompt_A = analysis_prompt
    
    st.markdown(f"ï¼ˆ(â˜…) ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: `{MODEL_FLASH_LITE}`ï¼‰")
    if st.button("AIã«ã‚«ãƒ†ã‚´ãƒªå€™è£œã‚’ç”Ÿæˆã•ã›ã‚‹ (Step 2)", key="gen_cat_button", type="primary"):
        if not analysis_prompt.strip():
            st.warning("åˆ†ææŒ‡é‡ã¯å¿…é ˆã§ã™ã€‚AIãŒãƒ‡ãƒ¼ã‚¿ã‚’ç†è§£ã™ã‚‹ãŸã‚ã«ç›®çš„ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        elif not os.getenv("GOOGLE_API_KEY"):
            st.error("Google APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ï¼ˆ.envãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„ï¼‰")
        else:
            with st.spinner(f"AI ({MODEL_FLASH_LITE}) ãŒåˆ†ææŒ‡é‡ã‚’èª­ã¿è§£ãã€ã‚«ãƒ†ã‚´ãƒªã‚’è€ƒæ¡ˆä¸­..."):
                logger.info("AIã‚«ãƒ†ã‚´ãƒªç”Ÿæˆãƒœã‚¿ãƒ³ã‚¯ãƒªãƒƒã‚¯")
                st.session_state.generated_categories = {"å¸‚åŒºç”ºæ‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰": "åœ°åè¾æ›¸(JAPAN_GEOGRAPHY_DB)ã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸå¸‚åŒºç”ºæ‘å"}
                
                ai_categories = get_dynamic_categories(analysis_prompt)
                
                if ai_categories:
                    st.session_state.generated_categories.update(ai_categories)
                    logger.info(f"AIã‚«ãƒ†ã‚´ãƒªç”ŸæˆæˆåŠŸ: {list(ai_categories.keys())}")
                    st.success("AIã«ã‚ˆã‚‹ã‚«ãƒ†ã‚´ãƒªå€™è£œã®ç”ŸæˆãŒå®Œäº†ã—ã¾ã—ãŸã€‚Step 3 ã«é€²ã‚“ã§ãã ã•ã„ã€‚")
                else:
                    st.error("AIã«ã‚ˆã‚‹ã‚«ãƒ†ã‚´ãƒªç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚AIã®å¿œç­”ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

    if not analysis_prompt.strip():
        st.warning("åˆ†ææŒ‡é‡ã¯å¿…é ˆã§ã™ã€‚AIãŒãƒ‡ãƒ¼ã‚¿ã‚’ç†è§£ã™ã‚‹ãŸã‚ã«ç›®çš„ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        return

    st.header("Step 3: åˆ†æã‚«ãƒ†ã‚´ãƒªã®é¸æŠ")
    if not st.session_state.generated_categories:
        st.info("Step 2 ã§ã€ŒAIã«ã‚«ãƒ†ã‚´ãƒªå€™è£œã‚’ç”Ÿæˆã•ã›ã‚‹ã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")
        return
        
    st.markdown("ã‚¿ã‚°ä»˜ã‘ã—ãŸã„ã‚«ãƒ†ã‚´ãƒªã‚’ä»¥ä¸‹ã‹ã‚‰é¸æŠã—ã¦ãã ã•ã„ï¼ˆã€Œå¸‚åŒºç”ºæ‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€ã¯å¿…é ˆã§ã™ï¼‰")
    
    selected_cats = []
    cols = st.columns(3)
    categories_to_show = st.session_state.generated_categories.items()
    
    for i, (cat, desc) in enumerate(categories_to_show):
        with cols[i % 3]:
            is_checked = st.checkbox(
                cat,
                value=(cat == "å¸‚åŒºç”ºæ‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰" or cat in st.session_state.selected_categories),
                help=str(desc), # (â˜… TypeError ä¿®æ­£)
                key=f"cat_cb_{cat}",
                disabled=(cat == "å¸‚åŒºç”ºæ‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰")
            )
            if is_checked:
                selected_cats.append(cat)
    st.session_state.selected_categories = set(selected_cats)

    st.header("Step 4: åˆ†æå¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆåˆ—ã®æŒ‡å®š")
    selected_text_col_map = {}
    st.markdown("ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã«ã€ã‚¿ã‚°ä»˜ã‘å¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆãŒå«ã¾ã‚Œã‚‹åˆ—ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚")
    for f_name, df in valid_files_data.items():
        cols_list = list(df.columns)
        default_index = 0
        
        if st.session_state.selected_text_col.get(f_name) in cols_list:
            default_index = cols_list.index(st.session_state.selected_text_col.get(f_name))
        elif any(c in cols_list for c in ['text', 'body', 'content', 'æŠ•ç¨¿', 'æœ¬æ–‡']):
            try:
                default_index = next(i for i, c in enumerate(cols_list) if c in ['text', 'body', 'content', 'æŠ•ç¨¿', 'æœ¬æ–‡'])
            except StopIteration:
                default_index = 0
                
        selected_col = st.selectbox(f"**{f_name}** ã®ãƒ†ã‚­ã‚¹ãƒˆåˆ—:", cols_list, index=default_index, key=f"col_select_{f_name}")
        selected_text_col_map[f_name] = selected_col
    st.session_state.selected_text_col = selected_text_col_map

    st.header("Step 5: åˆ†æå®Ÿè¡Œ")
    st.markdown(f"ï¼ˆ(â˜…) ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: `{MODEL_FLASH_LITE}`ï¼‰")
    
    col_run, col_cancel = st.columns([1, 1])
    with col_cancel:
        if st.button("ã‚­ãƒ£ãƒ³ã‚»ãƒ«", key="cancel_button_A", use_container_width=True):
            st.session_state.cancel_analysis = True
            logger.warning("åˆ†æã‚­ãƒ£ãƒ³ã‚»ãƒ«ãƒœã‚¿ãƒ³ãŒæŠ¼ã•ã‚Œã¾ã—ãŸã€‚")
            st.warning("æ¬¡ã®ãƒãƒƒãƒå‡¦ç†å¾Œã«åˆ†æã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã—ã¾ã™...")
    
    with col_run:
        if st.button("åˆ†æå®Ÿè¡Œ (Step 5)", type="primary", key="run_analysis_A", use_container_width=True):
            st.session_state.cancel_analysis = False
            st.session_state.log_messages = []
            st.session_state.tagged_df_A = pd.DataFrame()
            
            tip_placeholder = st.empty()
            try:
                with st.spinner("åˆ†æTIPSã‚’AIã§ç”Ÿæˆä¸­..."):
                    if 'tips_list' not in st.session_state or not st.session_state.tips_list:
                        st.session_state.tips_list = get_analysis_tips_list_from_ai()
                
                if not st.session_state.tips_list: # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                    st.session_state.tips_list = ["ãƒ‡ãƒ¼ã‚¿åˆ†æTIPSã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚"]

                st.session_state.current_tip_index = random.randint(0, len(st.session_state.tips_list) - 1)
                st.session_state.last_tip_time = time.time()
                tip_placeholder.info(f"ğŸ’¡ ãƒ‡ãƒ¼ã‚¿åˆ†æTIPS: {st.session_state.tips_list[st.session_state.current_tip_index]}")
            except Exception as e:
                logger.error(f"TipsåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")

            try:
                with st.spinner(f"Step A: AIåˆ†æå‡¦ç†ä¸­ ({MODEL_FLASH_LITE})..."):
                    logger.info("Step A åˆ†æå®Ÿè¡Œãƒœã‚¿ãƒ³ã‚¯ãƒªãƒƒã‚¯")
                    progress_placeholder = st.progress(0.0, text="å‡¦ç†å¾…æ©Ÿä¸­...")
                    log_placeholder = st.empty()

                    # --- 1. ãƒ•ã‚¡ã‚¤ãƒ«çµåˆ ---
                    update_progress_ui(progress_placeholder, log_placeholder, tip_placeholder, 0, 100, "ãƒ•ã‚¡ã‚¤ãƒ«çµåˆ")
                    temp_dfs = []
                    for f_name, df in valid_files_data.items():
                        col_name = selected_text_col_map[f_name]
                        temp_df = df.rename(columns={col_name: 'ANALYSIS_TEXT_COLUMN'})
                        temp_dfs.append(temp_df)
                    
                    master_df = pd.concat(temp_dfs, ignore_index=True, sort=False)
                    master_df['id'] = master_df.index
                    if master_df.empty:
                        raise Exception("åˆ†æå¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

                    # --- 2. é‡è¤‡å‰Šé™¤ ---
                    initial_row_count = len(master_df)
                    master_df.drop_duplicates(subset=['ANALYSIS_TEXT_COLUMN'], keep='first', inplace=True)
                    deduped_row_count = len(master_df)
                    logger.info(f"é‡è¤‡å‰Šé™¤ å®Œäº†ã€‚ {initial_row_count}è¡Œ -> {deduped_row_count}è¡Œ")

                    # --- 3. (â˜…) AIé–¢é€£æ€§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° (ã‚­ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³) ---
                    total_filter_rows = len(master_df)
                    total_filter_batches = (total_filter_rows + FILTER_BATCH_SIZE - 1) // FILTER_BATCH_SIZE
                    all_filtered_results = []
                    
                    for i in range(0, total_filter_rows, FILTER_BATCH_SIZE):
                        if st.session_state.cancel_analysis:
                            raise Exception("åˆ†æãŒã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ")
                        
                        batch_df = master_df.iloc[i:i + FILTER_BATCH_SIZE]
                        current_batch_num = (i // FILTER_BATCH_SIZE) + 1
                        
                        update_progress_ui(
                            progress_placeholder, log_placeholder, tip_placeholder,
                            min(i + FILTER_BATCH_SIZE, total_filter_rows), total_filter_rows,
                            f"AIã‚­ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ (ãƒãƒƒãƒ {current_batch_num}/{total_filter_batches})"
                        )
                        
                        filtered_df = filter_relevant_data_by_ai(batch_df, analysis_prompt)
                        if filtered_df is not None and not filtered_df.empty:
                            all_filtered_results.append(filtered_df)
                        
                        time.sleep(FILTER_SLEEP_TIME)
                    
                    if not all_filtered_results:
                        raise Exception("AIãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")

                    filter_results_df = pd.concat(all_filtered_results, ignore_index=True)
                    relevant_ids = filter_results_df[filter_results_df['relevant'] == True]['id']
                    filtered_master_df = master_df[master_df['id'].isin(relevant_ids)].copy()
                    filtered_row_count = len(filtered_master_df)
                    logger.info(f"AIãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° å®Œäº†ã€‚ {deduped_row_count}è¡Œ -> {filtered_row_count}è¡Œ")

                    if filtered_master_df.empty:
                        st.warning("AIã‚­ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®çµæœã€åˆ†æå¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ãŒ0ä»¶ã«ãªã‚Šã¾ã—ãŸã€‚")
                        st.session_state.tagged_df_A = pd.DataFrame()
                        progress_placeholder.progress(1.0, text="å‡¦ç†å®Œäº† (å¯¾è±¡ãƒ‡ãƒ¼ã‚¿0ä»¶)")
                        return

                    # --- 4. (â˜…) AIã‚¿ã‚°ä»˜ã‘ (Pass 1) ---
                    selected_category_definitions = {
                        cat: desc for cat, desc in st.session_state.generated_categories.items()
                        if cat in st.session_state.selected_categories
                    }
                    
                    master_df_for_tagging = filtered_master_df
                    total_rows = len(master_df_for_tagging)
                    all_tagged_results = []
                    total_batches = (total_rows + TAGGING_BATCH_SIZE - 1) // TAGGING_BATCH_SIZE
                    
                    for i in range(0, total_rows, TAGGING_BATCH_SIZE):
                        if st.session_state.cancel_analysis:
                            raise Exception("åˆ†æãŒã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ")
                        
                        batch_df = master_df_for_tagging.iloc[i:i + TAGGING_BATCH_SIZE]
                        current_batch_num = (i // TAGGING_BATCH_SIZE) + 1
                        
                        update_progress_ui(
                            progress_placeholder, log_placeholder, tip_placeholder,
                            min(i + TAGGING_BATCH_SIZE, total_rows), total_rows,
                            f"AIã‚¿ã‚°ä»˜ã‘[1/2] (ãƒãƒƒãƒ {current_batch_num}/{total_batches})"
                        )

                        tagged_df = perform_ai_tagging(batch_df, selected_category_definitions, analysis_prompt)
                        if tagged_df is not None and not tagged_df.empty:
                            all_tagged_results.append(tagged_df)
                        
                        time.sleep(TAGGING_SLEEP_TIME)

                    if not all_tagged_results:
                        raise Exception("AIã‚¿ã‚°ä»˜ã‘å‡¦ç†(Pass 1)ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
                    
                    tagged_results_df = pd.concat(all_tagged_results, ignore_index=True)

                    # --- (â˜…) [æ”¹å–„ A-2] 5. AIåœ°åæ¨è«– (Pass 2) ---
                    
                    # (â˜…) Pass 1 ã®çµæœã‚’ä¸€æ™‚çš„ã«ãƒãƒ¼ã‚¸
                    temp_merged_df = pd.merge(master_df_for_tagging, tagged_results_df, on='id', how='left')
                    
                    # (â˜…) ã€Œå¸‚åŒºç”ºæ‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€ãŒç©ºã®è¡Œã‚’æŠ½å‡º
                    rows_needing_inference = temp_merged_df[
                        temp_merged_df['å¸‚åŒºç”ºæ‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰'].isnull() | (temp_merged_df['å¸‚åŒºç”ºæ‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰'] == '')
                    ]
                    
                    all_inferred_results = []
                    total_inference_rows = len(rows_needing_inference)
                    
                    if total_inference_rows > 0:
                        logger.info(f"AIåœ°åæ¨è«–(Pass 2) é–‹å§‹ã€‚å¯¾è±¡: {total_inference_rows}ä»¶")
                        # (â˜…) åœ°åæ­£è¦åŒ–ãƒãƒƒãƒ—ã‚’ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ï¼‰å–å¾—
                        norm_maps = get_location_normalization_maps(JAPAN_GEOGRAPHY_DB, analysis_prompt)
                        
                        total_inf_batches = (total_inference_rows + TAGGING_BATCH_SIZE - 1) // TAGGING_BATCH_SIZE
                        
                        for i in range(0, total_inference_rows, TAGGING_BATCH_SIZE):
                            if st.session_state.cancel_analysis:
                                raise Exception("åˆ†æãŒã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ")
                            
                            batch_df = rows_needing_inference.iloc[i:i + TAGGING_BATCH_SIZE]
                            current_batch_num = (i // TAGGING_BATCH_SIZE) + 1
                            
                            update_progress_ui(
                                progress_placeholder, log_placeholder, tip_placeholder,
                                min(i + TAGGING_BATCH_SIZE, total_inference_rows), total_inference_rows,
                                f"AIåœ°åæ¨è«–[2/2] (ãƒãƒƒãƒ {current_batch_num}/{total_inf_batches})"
                            )

                            inferred_df = perform_ai_location_inference(batch_df, analysis_prompt, norm_maps)
                            if inferred_df is not None and not inferred_df.empty:
                                all_inferred_results.append(inferred_df)
                            
                            time.sleep(TAGGING_SLEEP_TIME)
                        
                        if all_inferred_results:
                            inferred_results_df = pd.concat(all_inferred_results, ignore_index=True)
                            
                            # (â˜…) Pass 2 ã®çµæœã‚’ Pass 1 ã®çµæœã«ãƒãƒ¼ã‚¸
                            tagged_results_df = tagged_results_df.set_index('id')
                            inferred_results_df = inferred_results_df.set_index('id')
                            
                            # (â˜…) inferred_location ã®å€¤ã§ã€'å¸‚åŒºç”ºæ‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰' ã® null/ç©º ã‚’åŸ‹ã‚ã‚‹
                            tagged_results_df['å¸‚åŒºç”ºæ‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰'].fillna(inferred_results_df['inferred_location'], inplace=True)
                            tagged_results_df.loc[tagged_results_df['å¸‚åŒºç”ºæ‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰'] == '', 'å¸‚åŒºç”ºæ‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰'] = inferred_results_df['inferred_location']
                            
                            tagged_results_df = tagged_results_df.reset_index()
                            logger.info("AIåœ°åæ¨è«–(Pass 2)ã®çµæœã‚’ãƒãƒ¼ã‚¸ã—ã¾ã—ãŸã€‚")

                    # --- 6. æœ€çµ‚ãƒãƒ¼ã‚¸ ---
                    logger.info("å…¨AIã‚¿ã‚°ä»˜ã‘çµæœçµåˆ...");
                    
                    logger.info("æœ€çµ‚ãƒãƒ¼ã‚¸å‡¦ç†é–‹å§‹...");
                    final_df = pd.merge(master_df_for_tagging, tagged_results_df, on='id', how='right')
                    
                    final_cols = list(master_df_for_tagging.columns) + [col for col in tagged_results_df.columns if col not in master_df_for_tagging.columns]
                    final_df = final_df[final_cols]

                    st.session_state.tagged_df_A = final_df
                    logger.info("Step A åˆ†æå‡¦ç† æ­£å¸¸çµ‚äº†");
                    st.success("AIã«ã‚ˆã‚‹åˆ†æå‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚");
                    progress_placeholder.progress(1.0, text="å‡¦ç†å®Œäº†")
                    
                    update_progress_ui(
                        progress_placeholder, log_placeholder, tip_placeholder, 
                        total_rows, total_rows, "å‡¦ç†å®Œäº†"
                    )
                    
                    tip_placeholder.empty() # å‡¦ç†å®Œäº†å¾Œã€Tipsã‚’æ¶ˆã™

            except Exception as e:
                logger.error(f"Step A åˆ†æå®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
                st.error(f"åˆ†æå®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                if 'progress_placeholder' in locals():
                    progress_placeholder.progress(1.0, text="ã‚¨ãƒ©ãƒ¼ã«ã‚ˆã‚Šå‡¦ç†ä¸­æ–­")
                if 'tip_placeholder' in locals():
                    tip_placeholder.empty() # ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚Tipsã‚’æ¶ˆã™

    # (â˜…) è¦ä»¶â‘£: ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆãƒªãƒ³ã‚¯ã‚’è¡¨ç¤º
    if not st.session_state.tagged_df_A.empty:
        st.header("Step 6: åˆ†æçµæœã®ç¢ºèªã¨ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ")
        st.dataframe(st.session_state.tagged_df_A.head(50))

        @st.cache_data
        def convert_df_to_csv(df: pd.DataFrame) -> bytes:
            """DataFrameã‚’UTF-8-SIGã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã®CSV (bytes) ã«å¤‰æ›ã™ã‚‹"""
            return df.to_csv(encoding="utf-8-sig", index=False).encode("utf-8-sig")

        csv_data = convert_df_to_csv(st.session_state.tagged_df_A)
        st.download_button(
            label="åˆ†æçµæœCSV (Curated_Data.csv) ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            data=csv_data,
            file_name="Curated_Data.csv",
            mime="text/csv",
        )
        st.info("ã“ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã€Step B ã§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦åˆ†æã‚’ç¶šã‘ã¦ãã ã•ã„ã€‚")

import networkx as nx # (â˜…) Step B (å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯) ã§å¿…è¦
from itertools import combinations # (â˜…) Step B (å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯) ã§å¿…è¦

def find_col(df: pd.DataFrame, patterns: List[str]) -> Optional[str]:
    """DataFrameã‹ã‚‰ã€è¤‡æ•°ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã«æœ€åˆã«ä¸€è‡´ã™ã‚‹åˆ—å(str)ã‚’1ã¤è¿”ã™"""
    cols = df.columns
    for pattern in patterns:
        try:
            # 1. å®Œå…¨ä¸€è‡´ (å¤§æ–‡å­—å°æ–‡å­—ç„¡è¦–)
            for col in cols:
                if col.lower() == pattern.lower():
                    return col
            # 2. éƒ¨åˆ†ä¸€è‡´ (å¤§æ–‡å­—å°æ–‡å­—ç„¡è¦–)
            for col in cols:
                if re.search(pattern, col, re.IGNORECASE):
                    return col
        except re.error:
            continue # (e.g. invalid regex pattern)
    return None

def find_cols(df: pd.DataFrame, patterns: List[str]) -> List[str]:
    """DataFrameã‹ã‚‰ã€è¤‡æ•°ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ä¸€è‡´ã™ã‚‹åˆ—å(list)ã‚’ã™ã¹ã¦è¿”ã™"""
    cols = df.columns
    found_cols = set()
    for pattern in patterns:
        try:
            for col in cols:
                if re.search(pattern, col, re.IGNORECASE):
                    found_cols.add(col)
        except re.error:
            continue
    return sorted(list(found_cols))

def find_engagement_cols(df: pd.DataFrame, patterns: List[str]) -> List[str]:
    """DataFrameã‹ã‚‰ã€ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ä¸€è‡´ã™ã‚‹ã€Œæ•°å€¤ã€åˆ—å(list)ã‚’ã™ã¹ã¦è¿”ã™"""
    numeric_cols = df.select_dtypes(include=np.number).columns
    found_cols = set()
    for pattern in patterns:
        try:
            for col in numeric_cols: # æ•°å€¤åˆ—ã®ã¿ã‚’æ¤œç´¢
                if re.search(pattern, col, re.IGNORECASE):
                    found_cols.add(col)
        except re.error:
            continue
    return sorted(list(found_cols))

def suggest_analysis_techniques_py(df: pd.DataFrame) -> List[Dict[str, Any]]:
    """
    (Step B) ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’åˆ†æã—ã€Pythonã§å®Ÿè¡Œå¯èƒ½ãªåŸºæœ¬çš„ãªåˆ†ææ‰‹æ³•ã‚’ææ¡ˆã™ã‚‹ã€‚
    (åˆ—åã«ä¾å­˜ã—ãªã„ã€æ±ç”¨çš„ãªåˆ†ææ‰‹æ³•ã‚’ææ¡ˆã™ã‚‹ã‚ˆã†ä¿®æ­£)
    """
    suggestions = []
    if df is None or df.empty:
        logger.error("suggest_analysis_techniques_py: DFãŒç©ºã§ã™ã€‚")
        return suggestions
        
    try:
        # --- 1. æŸ”è»Ÿãªåˆ—åã®ç‰¹å®š ---
        all_cols = list(df.columns)
        
        text_col = find_col(df, ['ANALYSIS_TEXT_COLUMN', 'text', 'content', 'æœ¬æ–‡'])
        location_col = find_col(df, ['å¸‚åŒºç”ºæ‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰', 'location', 'city', 'åœ°åŸŸ'])
        sentiment_col = find_col(df, ['sent', 'ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆ'])
        
        # æ—¥ä»˜åˆ—ã®å …ç‰¢ãªæ¤œç´¢
        date_col = None
        object_cols_for_date = df.select_dtypes(include='object').columns.tolist()
        date_patterns = ['date', 'time', 'æ—¥ä»˜', 'æ—¥æ™‚']
        for col in object_cols_for_date:
            if any(re.search(p, col, re.IGNORECASE) for p in date_patterns):
                 if df[col].isnull().all(): continue
                 try:
                     if pd.to_datetime(df[col].dropna().sample(n=min(5, df[col].count())), errors='coerce').notna().any():
                         date_col = col
                         break
                 except Exception:
                     pass
            if date_col:
                break
        
        engagement_cols = find_engagement_cols(df, ['eng', 'like', 'ã„ã„ã­', 'ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ'])
        
        # æ±ç”¨ã‚«ãƒ†ã‚´ãƒªåˆ— (flag_cols) ã®ç‰¹å®š
        base_flag_cols = find_cols(df, ['key', 'keyword', 'ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰', 'ã‚«ãƒ†ã‚´ãƒª', 'topic', 'ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°'])
        flag_cols = sorted(list(set([c for c in base_flag_cols if c is not None and c != location_col])))

        # ãã®ä»–ã®ã‚«ãƒ†ã‚´ãƒªåˆ—
        other_categorical = [
            col for col in df.select_dtypes(include='object').columns
            if col not in flag_cols and col != text_col and col != date_col and col != location_col
        ]
        
        # å…¨ã¦ã®ã‚«ãƒ†ã‚´ãƒªåˆ— (æ±ç”¨ + åœ°åŸŸ + ãã®ä»–)
        all_categorical = flag_cols + ([location_col] if location_col else []) + other_categorical
        
        logger.info(f"ææ¡ˆåˆ†æ(PY) - Text:{text_col}, Location:{location_col}")
        logger.info(f"ææ¡ˆåˆ†æ(PY) - FlagCols(æ±ç”¨ã‚«ãƒ†ã‚´ãƒª):{flag_cols}")
        logger.info(f"ææ¡ˆåˆ†æ(PY) - Engagement:{engagement_cols}, Sentiment:{sentiment_col}, Date:{date_col}")

        potential_suggestions = []

        # --- 2. ææ¡ˆãƒ­ã‚¸ãƒƒã‚¯ (æ±ç”¨åŒ–ç‰ˆ) ---

        # 1. å…¨ä½“ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        overall_metric_cols = [c for c in [sentiment_col] + engagement_cols if c is not None]
        potential_suggestions.append({
            "priority": 1, "name": "å…¨ä½“ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹",
            "description": "æŠ•ç¨¿æ•°ã€ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆã€ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆå‚¾å‘ãªã©ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã®æ¦‚è¦ã‚’è¨ˆç®—ã—ã¾ã™ã€‚",
            "reason": "ãƒ‡ãƒ¼ã‚¿å…¨ä½“ã®çŠ¶æ³æŠŠæ¡ã«å¿…é ˆã§ã™ã€‚",
            "suitable_cols": overall_metric_cols,
            "type": "python"
        })

        # 3. å˜ç´”é›†è¨ˆï¼ˆé »åº¦åˆ†æï¼‰
        for col in flag_cols + ([location_col] if location_col else []):
            potential_suggestions.append({
                "priority": 1, 
                "name": f"å˜ç´”é›†è¨ˆ: {col}",
                "description": f"ã€Œ{col}ã€åˆ—ã®å‡ºç¾é »åº¦ï¼ˆTOP50ï¼‰ã‚’åˆ†æã—ã¾ã™ã€‚",
                "reason": f"ã‚«ãƒ†ã‚´ãƒªåˆ—({col})ã®åŸºæœ¬æŒ‡æ¨™ã§ã™ã€‚",
                "suitable_cols": [col],
                "type": "python"
            })

        # 2. ã‚¯ãƒ­ã‚¹é›†è¨ˆ
        if len(all_categorical) >= 2:
            potential_suggestions.append({
                "priority": 2, "name": "ã‚¯ãƒ­ã‚¹é›†è¨ˆï¼ˆã‚«ãƒ†ã‚´ãƒªé–“ï¼‰",
                "description": "2ã¤ã®ã‚«ãƒ†ã‚´ãƒªåˆ—ï¼ˆä¾‹: 'è©±é¡Œã‚«ãƒ†ã‚´ãƒª' vs 'å¸‚åŒºç”ºæ‘'ï¼‰ã‚’é¸æŠã—ã€ãã®çµ„ã¿åˆã‚ã›ã‚’åˆ†æã—ã¾ã™ã€‚",
                "reason": f"è¤‡æ•°ã‚«ãƒ†ã‚´ãƒªåˆ—({len(all_categorical)}å€‹)ã‚ã‚Šã€é–¢é€£æ€§ã®ç™ºè¦‹ã«ã€‚",
                "suitable_cols": all_categorical, 
                "type": "python"
            })

        # 3. æ™‚ç³»åˆ—åˆ†æ
        if date_col and all_categorical:
            potential_suggestions.append({
                "priority": 3, "name": "æ™‚ç³»åˆ—ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æ",
                "description": f"ç‰¹å®šã®ã‚«ãƒ†ã‚´ãƒªåˆ—ï¼ˆä¾‹: 'è©±é¡Œã‚«ãƒ†ã‚´ãƒª'ï¼‰ã®å‡ºç¾æ•°ãŒæ™‚é–“ï¼ˆ{date_col}ï¼‰ã¨ã¨ã‚‚ã«ã©ã†å¤‰åŒ–ã—ãŸã‹åˆ†æã—ã¾ã™ã€‚",
                "reason": f"ã‚«ãƒ†ã‚´ãƒªåˆ—ã¨æ—¥æ™‚åˆ—({date_col})ã‚ã‚Šã€‚",
                "suitable_cols": {"datetime": [date_col], "keywords": all_categorical},
                "type": "python"
            })
            
        # 3. å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
        if text_col:
            potential_suggestions.append({
                "priority": 3, "name": "å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯",
                "description": "æŠ•ç¨¿ãƒ†ã‚­ã‚¹ãƒˆå†…ã®å˜èªã®å‡ºç¾ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æã—ã€é–¢é€£æ€§ã®é«˜ã„å˜èªã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚",
                "reason": "ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‹ã‚‰éš ã‚ŒãŸãƒˆãƒ”ãƒƒã‚¯ã‚„é–¢é€£æ€§ã‚’ç™ºè¦‹ã—ã¾ã™ã€‚",
                "suitable_cols": [text_col],
                "type": "python"
            })
            
        # 4. ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°
        if text_col:
            potential_suggestions.append({
                "priority": 4, "name": "ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°ï¼ˆé »å‡ºå˜èªï¼‰",
                "description": "åŸæ–‡ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰é »å‡ºã™ã‚‹å˜èªã‚’æŠ½å‡ºã—ã€ã©ã®ã‚ˆã†ãªè¨€è‘‰ãŒå¤šãä½¿ã‚ã‚Œã¦ã„ã‚‹ã‹å…¨ä½“åƒã‚’æŠŠæ¡ã—ã¾ã™ã€‚",
                "reason": "åŸæ–‡ãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚Šã€ã‚¿ã‚°ä»˜ã‘ä»¥å¤–ã®ã‚¤ãƒ³ã‚µã‚¤ãƒˆç™ºè¦‹ã«ã€‚",
                "suitable_cols": [text_col],
                "type": "python"
            })

        # 4. ã‚«ãƒ†ã‚´ãƒªåˆ—ã®é›†è¨ˆã¨æ·±æ˜ã‚Š (Python + AI)
        if flag_cols and text_col:
            potential_suggestions.append({
                "priority": 4, "name": "ã‚«ãƒ†ã‚´ãƒªåˆ—ã®é›†è¨ˆã¨æ·±æ˜ã‚Š",
                "description": "æŒ‡å®šã—ãŸã‚«ãƒ†ã‚´ãƒªåˆ—ï¼ˆä¾‹: 'è©±é¡Œã‚«ãƒ†ã‚´ãƒª'ï¼‰ã”ã¨ã«æŠ•ç¨¿æ•°ã‚’é›†è¨ˆã—ã€AIãŒæŠ•ç¨¿å†…å®¹ã®ã‚µãƒãƒªã‚’ç”Ÿæˆã—ã¾ã™ã€‚",
                "reason": "ã‚«ãƒ†ã‚´ãƒªã”ã¨ã®ä¸»è¦ãªè©±é¡Œã‚’æŠŠæ¡ã—ã¾ã™ã€‚",
                "suitable_cols": {'category_cols': flag_cols, 'text_col': [text_col]},
                "type": "python"
            })

        # 4. ã‚«ãƒ†ã‚´ãƒªåˆ¥ æ•°å€¤åˆ—TOP5åˆ†æ (Python + AI)
        if flag_cols and text_col and engagement_cols:
            potential_suggestions.append({
                "priority": 4, "name": "ã‚«ãƒ†ã‚´ãƒªåˆ¥ æ•°å€¤åˆ—TOP5åˆ†æ",
                "description": f"æŒ‡å®šã—ãŸã‚«ãƒ†ã‚´ãƒªåˆ—ã”ã¨ã«ã€æŒ‡å®šã—ãŸæ•°å€¤åˆ—ï¼ˆä¾‹: '{engagement_cols[0]}'ï¼‰ãŒé«˜ã„TOP5æŠ•ç¨¿ã‚’æŠ½å‡ºã—ã€AIãŒãã®æ¦‚è¦ã‚’ç”Ÿæˆã—ã¾ã™ã€‚",
                "reason": "ã‚«ãƒ†ã‚´ãƒªã”ã¨ã«ã€Œãƒã‚ºã£ãŸã€æŠ•ç¨¿ã®å†…å®¹ã‚’æŠŠæ¡ã—ã¾ã™ã€‚",
                "suitable_cols": {'category_cols': flag_cols, 'text_col': [text_col], 'numeric_cols': engagement_cols},
                "type": "python"
            })
        
        # 5. A/B æ¯”è¼ƒåˆ†æ
        if all_categorical and location_col:
             potential_suggestions.append({
                "priority": 5, "name": "A/B æ¯”è¼ƒåˆ†æ",
                "description": "2ã¤ã®ç•°ãªã‚‹æŠ•ç¨¿ã‚°ãƒ«ãƒ¼ãƒ—ï¼ˆä¾‹ï¼šã‚«ãƒ†ã‚´ãƒªA vs Bã€ã¾ãŸã¯ã‚¨ãƒªã‚¢A vs Bï¼‰ã‚’é¸æŠã—ã€æŠ•ç¨¿æ•°ã‚„äººæ°—è¦³å…‰åœ°ï¼ˆå¸‚åŒºç”ºæ‘ï¼‰ã®é †ä½å¤‰å‹•ã‚’æ¯”è¼ƒã—ã¾ã™ã€‚",
                "reason": "ã‚°ãƒ«ãƒ¼ãƒ—é–“ã®å‚¾å‘ã®é•ã„ã‚’æ˜ç¢ºã«ã—ã€æˆ¦ç•¥ç«‹æ¡ˆã«å½¹ç«‹ã¦ã¾ã™ã€‚",
                "suitable_cols": {'category_cols': all_categorical, 'location_col': [location_col]},
                "type": "python"
            })

        suggestions = sorted(potential_suggestions, key=lambda x: x['priority'])
        
        final_suggestions = []
        seen_names = set()
        for s in suggestions:
             if s['name'] not in seen_names:
                 final_suggestions.append(s)
                 seen_names.add(s['name'])
                 
        logger.info(f"Pythonãƒ™ãƒ¼ã‚¹ææ¡ˆ(ã‚½ãƒ¼ãƒˆå¾Œ): {[s['name'] for s in final_suggestions]}")
        return final_suggestions

    except Exception as e:
        logger.error(f"Pythonåˆ†ææ‰‹æ³•ææ¡ˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        st.warning(f"åˆ†ææ‰‹æ³•ææ¡ˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
    return suggestions

def suggest_analysis_techniques_ai(
    user_prompt: str,
    df: pd.DataFrame,
    existing_suggestions: List[Dict[str, Any]]
) -> List[Dict[str, Any]]:
    """
    (Step B) ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è‡ªç”±è¨˜è¿°ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«åŸºã¥ãã€AIãŒè¿½åŠ ã®åˆ†ææ‰‹æ³•ã‚’ææ¡ˆã™ã‚‹ã€‚
    """
    logger.info("AIãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ™ãƒ¼ã‚¹ã®åˆ†æææ¡ˆ (Flash Lite) ã‚’é–‹å§‹...")
    
    llm = get_llm(model_name=MODEL_FLASH_LITE, temperature=0.1)
    if llm is None:
        logger.error("suggest_analysis_techniques_ai: LLM (Flash Lite) ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")
        return []

    try:
        col_info = []
        for col in df.columns:
            col_info.append(f"- {col} (å‹: {df[col].dtype}, ä¾‹: {df[col].dropna().iloc[0] if not df[col].dropna().empty else 'N/A'})")
        column_info_str = "\n".join(col_info[:15])
        
        existing_names = [s['name'] for s in existing_suggestions]
        
        # (Bug 1.2 / å·®åˆ†å•é¡Œ) é‡è¤‡ã‚¿ã‚¹ã‚¯ã‚’AIã«å³æ ¼ã«ç¦æ­¢ã™ã‚‹
        forbidden_tasks = [
            "å…¨ä½“ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹", "å˜ç´”é›†è¨ˆ", "å¸‚åŒºç”ºæ‘åˆ¥æŠ•ç¨¿æ•°", "ã‚¯ãƒ­ã‚¹é›†è¨ˆ", 
            "æ™‚ç³»åˆ—ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æ", "å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯", "ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°",
            "ã‚«ãƒ†ã‚´ãƒªåˆ—ã®é›†è¨ˆã¨æ·±æ˜ã‚Š", "ã‚«ãƒ†ã‚´ãƒªåˆ¥ æ•°å€¤åˆ—TOP5åˆ†æ", 
            "A/B æ¯”è¼ƒåˆ†æ", "ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆ", "Sentiment",
            # AIãŒç”Ÿæˆã—ãŒã¡ãªé‡è¤‡ã‚¿ã‚¹ã‚¯åã‚‚æ˜ç¤ºçš„ã«ç¦æ­¢
            "è©±é¡Œã‚«ãƒ†ã‚´ãƒªåˆ¥ æŠ•ç¨¿æ•°ã¨ã‚µãƒãƒª", 
            "è©±é¡Œã‚«ãƒ†ã‚´ãƒªåˆ¥æŠ•ç¨¿æ•°ã¨ã‚µãƒãƒª",
            "è©±é¡Œã‚«ãƒ†ã‚´ãƒªåˆ¥ ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆTOP5ã¨æ¦‚è¦", 
            "è©±é¡Œã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆä¸Šä½æŠ•ç¨¿TOP5",
            "å¸‚åŒºç”ºæ‘åˆ¥æŠ•ç¨¿æ•°é›†è¨ˆ",
            "å…¨ä½“ã®ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆåˆ†æ"
        ]
        existing_names_str = ", ".join(list(set(existing_names + forbidden_tasks)))

        prompt = PromptTemplate.from_template(
            """
            ã‚ãªãŸã¯ãƒ‡ãƒ¼ã‚¿åˆ†æã®å°‚é–€å®¶ã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã€Œåˆ†ææŒ‡ç¤ºã€ã¨ã€Œãƒ‡ãƒ¼ã‚¿æ§‹é€ ã€ã‚’èª­ã¿ã€å®Ÿè¡Œå¯èƒ½ãªã€Œåˆ†æã‚¿ã‚¹ã‚¯ã€ã‚’JSONãƒªã‚¹ãƒˆå½¢å¼ã§ææ¡ˆã—ã¦ãã ã•ã„ã€‚
            
            # ãƒ‡ãƒ¼ã‚¿æ§‹é€  (åˆ©ç”¨å¯èƒ½ãªåˆ—å):
            {column_info}
            
            # æ—¢ã«ææ¡ˆæ¸ˆã¿ã®ã‚¿ã‚¹ã‚¯ (ã“ã‚Œã‚‰ã¯ææ¡ˆã—ãªã„ã§ãã ã•ã„):
            {existing_tasks}
            
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®åˆ†ææŒ‡ç¤º:
            {user_prompt}
            
            # æŒ‡ç¤º:
            1. ã€Œãƒ¦ãƒ¼ã‚¶ãƒ¼ã®åˆ†ææŒ‡ç¤ºã€ã‚’è§£é‡ˆã—ã€å…·ä½“çš„ãªåˆ†æã‚¿ã‚¹ã‚¯ï¼ˆä¾‹ï¼šã€Œåºƒå³¶å¸‚ã¨è¦³å…‰åœ°ã®ç›¸é–¢åˆ†æã€ï¼‰ã«åˆ†è§£ã™ã‚‹ã€‚
            2. ã€é‡è¦ã€‘ã€Œæ—¢ã«ææ¡ˆæ¸ˆã¿ã®ã‚¿ã‚¹ã‚¯ã€ãƒªã‚¹ãƒˆã«ã‚ã‚‹ã‚¿ã‚¹ã‚¯ã‚„ã€ãã‚Œã«é…·ä¼¼ã—ãŸã‚¿ã‚¹ã‚¯ï¼ˆä¾‹ï¼šã€Œå˜ç´”é›†è¨ˆã€ã‚„ã€Œã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚µãƒãƒªã€ãªã©ï¼‰ã¯ã€çµ¶å¯¾ã«ã€‘ææ¡ˆã—ãªã„ã§ãã ã•ã„ã€‚
            3. å„ã‚¿ã‚¹ã‚¯ã‚’ä»¥ä¸‹ã®JSONå½¢å¼ã§å®šç¾©ã™ã‚‹ã€‚
            4. `name`ã¯ã‚¿ã‚¹ã‚¯åã€`description`ã¯AIï¼ˆã‚ãªãŸè‡ªèº«ï¼‰ãŒã“ã®å¾Œå®Ÿè¡Œã™ã‚‹ã‚¿ã‚¹ã‚¯ã®å…·ä½“çš„ãªæŒ‡ç¤ºï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼‰ã¨ã™ã‚‹ã€‚
            5. `priority`ã¯ 5 å›ºå®šã€`type`ã¯ "ai" å›ºå®šã¨ã™ã‚‹ã€‚
            
            # å›ç­” (JSONãƒªã‚¹ãƒˆå½¢å¼ã®ã¿):
            [
              {{
                "priority": 5,
                "name": "ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡ç¤ºã«åŸºã¥ãã‚¿ã‚¹ã‚¯å1ï¼‰",
                "description": "ï¼ˆã“ã®ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã™ã‚‹ãŸã‚ã®AIã¸ã®å…·ä½“çš„ãªæŒ‡ç¤ºãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ1ï¼‰",
                "reason": "ãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡ç¤ºã«åŸºã¥ã",
                "suitable_cols": [],
                "type": "ai"
              }}
            ]
            """
        )
        chain = prompt | llm | StrOutputParser()
        response_str = chain.invoke({
            "column_info": column_info_str,
            "user_prompt": user_prompt,
            "existing_tasks": existing_names_str
        })

        logger.info(f"AIè¿½åŠ ææ¡ˆ(ç”Ÿ): {response_str}")
        match = re.search(r'\[.*\]', response_str, re.DOTALL)
        if not match:
            logger.warning("AIãŒJSONãƒªã‚¹ãƒˆå½¢å¼ã§å¿œç­”ã—ã¾ã›ã‚“ã§ã—ãŸã€‚")
            return []
            
        json_str = match.group(0)
        ai_suggestions = json.loads(json_str)
        
        for s in ai_suggestions:
            s['type'] = 'ai'
            if 'priority' not in s: s['priority'] = 5
            
        logger.info(f"AIè¿½åŠ ææ¡ˆ(ãƒ‘ãƒ¼ã‚¹æ¸ˆ): {len(ai_suggestions)}ä»¶")
        return ai_suggestions

    except Exception as e:
        logger.error(f"AIè¿½åŠ ææ¡ˆã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        st.warning(f"AIè¿½åŠ ææ¡ˆã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return []

# --- 8.0. ã‚°ãƒ©ãƒ•ç”Ÿæˆãƒ˜ãƒ«ãƒ‘ãƒ¼ ---
def generate_graph_image(
    df: pd.DataFrame,
    plot_type: str,
    x_col: Optional[str] = None,
    y_col: Optional[str] = None,
    title: str = "åˆ†æã‚°ãƒ©ãƒ•"
) -> Optional[str]:
    """
    DataFrameã‹ã‚‰matplotlibã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆã—ã€Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸç”»åƒæ–‡å­—åˆ—ã‚’è¿”ã™ã€‚
    (ã‚°ãƒ©ãƒ•ã‚µã‚¤ã‚ºã‚’å‹•çš„ã«å¤‰æ›´)
    """
    logger.info(f"ã‚°ãƒ©ãƒ•ç”Ÿæˆé–‹å§‹: {title} (ã‚¿ã‚¤ãƒ—: {plot_type})")
    if df is None or df.empty:
        logger.warning("ã‚°ãƒ©ãƒ•ç”Ÿæˆã‚¹ã‚­ãƒƒãƒ—: DataFrameãŒç©ºã§ã™ã€‚")
        return None

    # ãƒ—ãƒ­ãƒƒãƒˆã‚¿ã‚¤ãƒ—ã«å¿œã˜ã¦Figureã‚µã‚¤ã‚ºã‚’å¤‰æ›´
    if plot_type == 'network':
        plt.figure(figsize=(12, 12)) # å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯: æ­£æ–¹å½¢ (12x12)
    elif plot_type == 'timeseries':
        plt.figure(figsize=(15, 7)) # æ™‚ç³»åˆ—: æ¨ªé•· (15x7)
    else:
        plt.figure(figsize=(10, 7)) # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ (æ£’ã‚°ãƒ©ãƒ•ãªã©)
    
    plt.rcParams['font.size'] = 12
    
    try:
        if plot_type == 'bar' and x_col and y_col:
            df_plot = df.nlargest(20, y_col).sort_values(by=y_col, ascending=True)
            if df_plot.empty:
                raise ValueError("ã‚°ãƒ©ãƒ•æç”»å¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
                
            bars = plt.barh(df_plot[x_col], df_plot[y_col], color='#7280C1')
            plt.xlabel('ä»¶æ•°')
            plt.ylabel(x_col)
            plt.grid(axis='x', linestyle='--', alpha=0.6)
            
            for bar in bars:
                plt.text(
                    bar.get_width() + (df_plot[y_col].max() * 0.01),
                    bar.get_y() + bar.get_height() / 2,
                    f' {bar.get_width():.0f}',
                    va='center',
                    ha='left'
                )
        
        elif plot_type == 'timeseries' and x_col and y_col:
            try:
                df[x_col] = pd.to_datetime(df[x_col])
                df_pivot = df.pivot(index=x_col, columns='keyword', values=y_col).fillna(0)
                
                if len(df_pivot.columns) > 6:
                    top_5_keywords = df_pivot.sum().nlargest(5).index
                    df_pivot['ãã®ä»–'] = df_pivot.drop(columns=top_5_keywords).sum(axis=1)
                    df_pivot = df_pivot[list(top_5_keywords) + ['ãã®ä»–']]
                
                df_pivot.plot(kind='line', ax=plt.gca(), linewidth=2.5)
                plt.xlabel('æ—¥ä»˜')
                plt.ylabel('ä»¶æ•°')
                plt.legend(title='ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰', bbox_to_anchor=(1.05, 1), loc='upper left')
                plt.grid(axis='y', linestyle='--', alpha=0.6)
                
            except Exception as e:
                logger.error(f"æ™‚ç³»åˆ—ãƒ”ãƒœãƒƒãƒˆ/ãƒ—ãƒ­ãƒƒãƒˆã‚¨ãƒ©ãƒ¼: {e}")
                plt.plot(df[x_col], df[y_col])
                plt.xlabel(x_col)
                plt.ylabel(y_col)

        elif plot_type == 'network':
            df_plot = df.nlargest(100, 'weight')
            if df_plot.empty:
                raise ValueError("ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æç”»å¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

            G = nx.from_pandas_edgelist(df_plot, 'source', 'target', ['weight'])
            
            try:
                partition = community.best_partition(G)
                num_communities = len(set(partition.values()))
                colors = plt.cm.get_cmap('tab20', num_communities)
                node_colors = [colors(partition.get(node)) for node in G.nodes()]
            except Exception:
                node_colors = '#7280C1'
            
            # å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆèª¿æ•´
            k_val = 2.5 / math.sqrt(len(G.nodes())) # kå€¤ã‚’èª¿æ•´ (ãƒãƒ¼ãƒ‰ã‚’åºƒã’ã‚‹)
            pos = nx.spring_layout(G, k=max(k_val, 0.5), iterations=50, seed=42)
            
            node_sizes = []
            try:
                for node in G.nodes():
                    total_weight = sum(data['weight'] for _, _, data in G.edges(node, data=True))
                    node_sizes.append(total_weight * 20)
            except Exception:
                node_sizes = 500
            
            edge_weights = [d['weight'] / df_plot['weight'].max() * 8 for u, v, d in G.edges(data=True)]

            nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color=node_colors, alpha=0.9)
            nx.draw_networkx_edges(G, pos, width=edge_weights, alpha=0.1, edge_color='grey') # alphaã‚’èª¿æ•´
            nx.draw_networkx_labels(G, pos, font_size=10, font_family='IPAGothic')
            
            plt.axis('off')
        
        elif plot_type == 'wordcloud' and not df.empty:
            if 'word' not in df.columns or 'count' not in df.columns:
                 raise ValueError("ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã«ã¯ 'word' ã¨ 'count' åˆ—ãŒå¿…è¦ã§ã™ã€‚")
            
            frequencies = df.set_index('word')['count'].to_dict()
            
            if not frequencies:
                 raise ValueError("ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”¨ã®å˜èªãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

            wc = WordCloud(
                font_path=font_path,
                width=800,
                height=500,
                background_color='white',
                colormap='viridis',
                max_words=100
            ).generate_from_frequencies(frequencies)
            
            plt.imshow(wc, interpolation='bilinear')
            plt.axis('off')

        else:
            logger.warning(f"æœªå¯¾å¿œã®ãƒ—ãƒ­ãƒƒãƒˆã‚¿ã‚¤ãƒ—: {plot_type}")
            return None

        plt.title(title, fontsize=16, pad=20)
        plt.tight_layout()

        buf = BytesIO()
        plt.savefig(buf, format='png', dpi=96)
        buf.seek(0)
        
        image_base64 = base64.b64encode(buf.getvalue()).decode('utf-8')
        logger.info(f"ã‚°ãƒ©ãƒ•ç”ŸæˆæˆåŠŸ: {title}")
        return image_base64

    except Exception as e:
        logger.error(f"ã‚°ãƒ©ãƒ•ç”Ÿæˆ ({title}) å¤±æ•—: {e}", exc_info=True)
        return None
    finally:
        plt.clf()
        plt.close('all')

# --- 8.1. (â˜…) Step B: Pythonåˆ†æãƒ˜ãƒ«ãƒ‘ãƒ¼ ---

def run_simple_count(df: pd.DataFrame, suggestion: Dict[str, Any]) -> Dict[str, Any]:
    """(Step B) å˜ç´”é›†è¨ˆï¼ˆé »åº¦åˆ†æï¼‰ã‚’å®Ÿè¡Œã—ã€DataFrameã¨ã‚°ãƒ©ãƒ•(Base64)ã‚’è¿”ã™"""
    results = {"data": pd.DataFrame(), "image_base64": None, "summary": ""}
    
    flag_cols = suggestion.get('suitable_cols', [])
    if not flag_cols:
        msg = "é›†è¨ˆå¯¾è±¡ã®åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
        logger.warning(f"run_simple_count: {msg}")
        results["summary"] = msg
        return results
    
    # UIã§ç·¨é›†ã•ã‚ŒãŸåˆ—ã‚’å–å¾— (ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚ã‚Š)
    col_to_analyze = suggestion.get('ui_selected_col', flag_cols[0])
    
    if col_to_analyze not in df.columns:
        msg = f"åˆ— '{col_to_analyze}' ãŒDFã«å­˜åœ¨ã—ã¾ã›ã‚“ã€‚"
        logger.warning(f"run_simple_count: {msg}")
        results["summary"] = msg
        return results
        
    try:
        s = df[col_to_analyze].astype(str).str.split(', ').explode()
        s = s[s.str.strip().isin(['', 'nan', 'None', 'N/A', 'è©²å½“ãªã—']) == False]
        s = s.str.strip()
        
        if s.empty:
            msg = "é›†è¨ˆå¯¾è±¡ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
            logger.info(f"run_simple_count: {msg}")
            results["summary"] = msg
            return results
            
        counts = s.value_counts().head(50)
        counts_df = counts.reset_index()
        counts_df.columns = [col_to_analyze, 'count']
        
        results["data"] = counts_df
        results["summary"] = f"'{col_to_analyze}' ã®å˜ç´”é›†è¨ˆï¼ˆé »åº¦åˆ†æï¼‰ã‚’å®Ÿè¡Œã€‚ä¸Šä½ã¯ {counts_df.iloc[0,0]} ({counts_df.iloc[0,1]}ä»¶), {counts_df.iloc[1,0]} ({counts_df.iloc[1,1]}ä»¶) ã§ã—ãŸã€‚"
        
        results["image_base64"] = generate_graph_image(
            df=counts_df,
            plot_type='bar',
            x_col=col_to_analyze,
            y_col='count',
            title=f"ã€Œ{col_to_analyze}ã€ é »å‡ºTOP20"
        )
        return results
            
    except Exception as e:
        logger.error(f"run_simple_count error: {e}", exc_info=True)
        results["summary"] = f"ã‚¨ãƒ©ãƒ¼: {e}"
    return results

def run_crosstab(df: pd.DataFrame, suggestion: Dict[str, Any]) -> Dict[str, Any]:
    """(Step B) ã‚¯ãƒ­ã‚¹é›†è¨ˆã‚’å®Ÿè¡Œã—ã€DataFrameã‚’è¿”ã™"""
    results = {"data": pd.DataFrame(), "image_base64": None, "summary": ""}
    
    cols = suggestion.get('suitable_cols', [])
    if len(cols) < 2:
        msg = "ã‚¯ãƒ­ã‚¹é›†è¨ˆã«ã¯2åˆ—ä»¥ä¸Šå¿…è¦ã§ã™ã€‚"
        logger.warning(f"run_crosstab: {msg}")
        results["summary"] = msg
        return results

    # UIã§ç·¨é›†ã•ã‚ŒãŸåˆ—ã‚’å–å¾— (ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚ã‚Š)
    col1 = suggestion.get('ui_selected_col1', cols[0])
    col2 = suggestion.get('ui_selected_col2', cols[1])

    if col1 not in df.columns or col2 not in df.columns:
        msg = f"é¸æŠã•ã‚ŒãŸåˆ— ({col1}, {col2}) ãŒDFã«å­˜åœ¨ã—ã¾ã›ã‚“ã€‚"
        logger.warning(f"run_crosstab: {msg}")
        results["summary"] = msg
        return results
    
    try:
        df_exploded_1 = df.assign(**{col1: df[col1].astype(str).str.split(', ')}).explode(col1)
        df_exploded_2 = df_exploded_1.assign(**{col2: df_exploded_1[col2].astype(str).str.split(', ')}).explode(col2)

        df_exploded_2[col1] = df_exploded_2[col1].str.strip()
        df_exploded_2[col2] = df_exploded_2[col2].str.strip()
        df_exploded_2 = df_exploded_2.replace('', np.nan).replace('nan', np.nan).replace('None', np.nan).dropna(subset=[col1, col2])
        
        crosstab_df = pd.crosstab(df_exploded_2[col1], df_exploded_2[col2])
        
        crosstab_long = crosstab_df.stack().reset_index()
        crosstab_long.columns = [col1, col2, 'count']
        crosstab_long = crosstab_long[crosstab_long['count'] > 0].sort_values(by='count', ascending=False)
        
        results["data"] = crosstab_long.head(100)
        results["summary"] = f"'{col1}' ã¨ '{col2}' ã®ã‚¯ãƒ­ã‚¹é›†è¨ˆã‚’å®Ÿè¡Œã€‚æœ€å¼·ã®çµ„ã¿åˆã‚ã›ã¯ {crosstab_long.iloc[0,0]} x {crosstab_long.iloc[0,1]} ({crosstab_long.iloc[0,2]}ä»¶) ã§ã—ãŸã€‚"
        logger.info("run_crosstab: ã‚°ãƒ©ãƒ•ç”Ÿæˆã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã—ãŸã€‚")
        
        return results
        
    except Exception as e:
        logger.error(f"run_crosstab error: {e}", exc_info=True)
        results["summary"] = f"ã‚¨ãƒ©ãƒ¼: {e}"
    return results

def run_timeseries(df: pd.DataFrame, suggestion: Dict[str, Any]) -> Dict[str, Any]:
    """(Step B) æ™‚ç³»åˆ—åˆ†æã‚’å®Ÿè¡Œã—ã€DataFrameã¨ã‚°ãƒ©ãƒ•(Base64)ã‚’è¿”ã™"""
    results = {"data": pd.DataFrame(), "image_base64": None, "summary": ""}
    
    cols_dict = suggestion.get('suitable_cols', {})
    if not isinstance(cols_dict, dict) or 'datetime' not in cols_dict or 'keywords' not in cols_dict:
        msg = "åˆ—æƒ…å ±ï¼ˆdatetime, keywordsï¼‰ãŒä¸ååˆ†ã§ã™ã€‚"
        logger.warning(f"run_timeseries: {msg}")
        results["summary"] = msg
        return results
        
    # UIã§ç·¨é›†ã•ã‚ŒãŸåˆ—ã‚’å–å¾— (ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚ã‚Š)
    dt_col = suggestion.get('ui_selected_dt_col', cols_dict['datetime'][0])
    kw_col = suggestion.get('ui_selected_kw_col', cols_dict['keywords'][0])

    if dt_col not in df.columns:
        msg = f"æ—¥æ™‚åˆ— '{dt_col}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
        logger.warning(f"run_timeseries: {msg}"); results["summary"] = msg; return results
    if kw_col not in df.columns:
        msg = f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ— '{kw_col}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
        logger.warning(f"run_timeseries: {msg}"); results["summary"] = msg; return results

    try:
        df_copy = df[[dt_col, kw_col]].copy()
        df_copy[dt_col] = pd.to_datetime(df_copy[dt_col], errors='coerce')
        df_copy = df_copy.dropna(subset=[dt_col])
        
        df_exploded = df_copy.assign(**{kw_col: df_copy[kw_col].astype(str).str.split(', ')}).explode(kw_col)
        df_exploded[kw_col] = df_exploded[kw_col].str.strip()
        df_exploded = df_exploded[df_exploded[kw_col].isin(['', 'nan', 'None', 'N/A', 'è©²å½“ãªã—']) == False]
        
        if df_exploded.empty:
            msg = "æœ‰åŠ¹ãªæ—¥æ™‚/ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚"
            logger.info(f"run_timeseries: {msg}"); results["summary"] = msg; return results

        time_df = df_exploded.groupby([pd.Grouper(key=dt_col, freq='D'), kw_col]).size().rename("count").reset_index()
        
        time_df.columns = ['date', 'keyword', 'count']
        
        top_keywords = df_exploded[kw_col].value_counts().head(50).index
        time_df_filtered = time_df[time_df['keyword'].isin(top_keywords)]
        
        time_df_for_graph = time_df_filtered.copy()
        
        time_df_for_json = time_df_filtered.sort_values(by=['keyword', 'date'])
        time_df_for_json['date'] = time_df_for_json['date'].dt.strftime('%Y-%m-%d')
        
        results["data"] = time_df_for_json
        
        results["image_base64"] = generate_graph_image(
            df=time_df_for_graph,
            plot_type='timeseries',
            x_col='date',
            y_col='count',
            title=f"ã€Œ{kw_col}ã€åˆ¥ æ™‚ç³»åˆ—ãƒˆãƒ¬ãƒ³ãƒ‰ (TOP5)"
        )
        results["summary"] = f"'{dt_col}' ã¨ '{kw_col}' ã§æ™‚ç³»åˆ—åˆ†æã‚’å®Ÿè¡Œã€‚TOP5ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ãƒˆãƒ¬ãƒ³ãƒ‰ã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆã—ã¾ã—ãŸã€‚"
        return results
            
    except Exception as e:
        logger.error(f"run_timeseries error: {e}", exc_info=True)
        results["summary"] = f"ã‚¨ãƒ©ãƒ¼: {e}"
    return results

def run_text_mining(df: pd.DataFrame, suggestion: Dict[str, Any]) -> Dict[str, Any]:
    """(Step B) ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°ï¼ˆé »å‡ºå˜èªï¼‰ã‚’å®Ÿè¡Œã—ã€DataFrameã¨ã‚°ãƒ©ãƒ•(Base64)ã‚’è¿”ã™"""
    results = {"data": pd.DataFrame(), "image_base64": None, "summary": ""}
    
    # UIã§ç·¨é›†ã•ã‚ŒãŸåˆ—ã‚’å–å¾— (ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚ã‚Š)
    text_col = suggestion.get('ui_selected_text_col', suggestion.get('suitable_cols', ['ANALYSIS_TEXT_COLUMN'])[0])
    
    if text_col not in df.columns or df[text_col].empty:
        msg = f"ãƒ†ã‚­ã‚¹ãƒˆåˆ— '{text_col}' ãŒãªã„ã‹ã€ç©ºã§ã™ã€‚"
        logger.warning(f"run_text_mining: {msg}"); results["summary"] = msg; return results

    nlp = load_spacy_model()
    if nlp is None:
        st.error("spaCyæ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
        results["summary"] = "spaCyæ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸã€‚"
        return results
            
    try:
        texts = df[text_col].dropna().astype(str)
        if texts.empty:
            results["summary"] = "ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ã€‚"
            return results
            
        words = []
        target_pos = {'NOUN', 'PROPN', 'ADJ'}
        stop_words = {
            'ã®', 'ã«', 'ã¯', 'ã‚’', 'ãŒ', 'ã§', 'ã¦', 'ã§ã™', 'ã¾ã™', 'ã“ã¨', 'ã‚‚ã®', 'ãã‚Œ', 'ã‚ã‚Œ',
            'ã“ã‚Œ', 'ãŸã‚', 'ã„ã‚‹', 'ã™ã‚‹', 'ã‚ã‚‹', 'ãªã„', 'ã„ã†', 'ã‚ˆã†', 'ãã†', 'ãªã©', 'ã•ã‚“',
            'çš„', 'äºº', 'è‡ªåˆ†', 'ç§', 'åƒ•', 'ä½•', 'ãã®', 'ã“ã®', 'ã‚ã®'
        }
        
        custom_stop_words_str = suggestion.get('ui_custom_stop_words', '')
        if custom_stop_words_str:
            try:
                # ã‚«ãƒ³ãƒã€ç©ºç™½ã€æ”¹è¡Œã€èª­ç‚¹ï¼ˆã€ï¼‰ã§åŒºåˆ‡ã‚‰ã‚ŒãŸå˜èªã‚’ã‚»ãƒƒãƒˆã«è¿½åŠ 
                custom_set = set(
                    word.strip() for word in re.split(r'[\s,ã€\n]+', custom_stop_words_str) if word.strip()
                )
                stop_words.update(custom_set)
                logger.info(f"ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°: ã‚«ã‚¹ã‚¿ãƒ é™¤å¤–èª {len(custom_set)}ä»¶ ã‚’è¿½åŠ ã€‚")
            except Exception as e:
                logger.warning(f"ã‚«ã‚¹ã‚¿ãƒ é™¤å¤–èªã®è§£æå¤±æ•—: {e}")

        total_texts = len(texts)
        if 'progress_text' not in st.session_state:
             st.session_state.progress_text = ""
        st.session_state.progress_text = "ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚° (spaCy) å‡¦ç†ä¸­... 0%"

        for i, doc in enumerate(nlp.pipe(texts, disable=["parser", "ner"], batch_size=50)):
            for token in doc:
                if (token.pos_ in target_pos) and (not token.is_stop) and (token.lemma_ not in stop_words) and (len(token.lemma_) > 1):
                    words.append(token.lemma_)
            
            if (i + 1) % 100 == 0:
                percent = (i + 1) / total_texts
                st.session_state.progress_text = f"ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚° (spaCy) å‡¦ç†ä¸­... {percent:.0%}"

        if not words:
            msg = "æŠ½å‡ºå¯èƒ½ãªæœ‰åŠ¹ãªå˜èªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
            logger.warning(f"run_text_mining: {msg}"); results["summary"] = msg; return results

        word_counts = pd.Series(words).value_counts().head(100)
        word_counts_df = word_counts.reset_index()
        word_counts_df.columns = ['word', 'count']
        
        st.session_state.progress_text = "ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚° (spaCy) å®Œäº†ã€‚"
        
        results["data"] = word_counts_df
        results["summary"] = f"'{text_col}' ã«å¯¾ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œã€‚é »å‡ºå˜èªã¯ '{word_counts_df.iloc[0,0]}' ({word_counts_df.iloc[0,1]}ä»¶) ã§ã—ãŸã€‚"
        
        # ã‚°ãƒ©ãƒ•ç”Ÿæˆ (ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰)
        results["image_base64"] = generate_graph_image(
            df=word_counts_df,
            plot_type='wordcloud',
            title=f"ã€Œ{text_col}ã€ é »å‡ºå˜èª ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ (TOP100)"
        )
        return results
        
    except Exception as e:
        logger.error(f"run_text_mining error: {e}", exc_info=True)
        results["summary"] = f"ã‚¨ãƒ©ãƒ¼: {e}"
    return results

def run_overall_metrics(df: pd.DataFrame, suggestion: Dict[str, Any]) -> Dict[str, Any]:
    """(Step B) ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—ã™ã‚‹ (å˜ä½è¿½åŠ )"""
    logger.info("run_overall_metrics å®Ÿè¡Œ...")
    metrics = {}
    try:
        # (Bug 1.6) å˜ä½ã‚’æ–‡å­—åˆ—ã¨ã—ã¦è¿½åŠ 
        metrics["total_posts"] = f"{len(df):,}ä»¶"

        engagement_cols = [col for col in df.columns if any(c in col.lower() for c in ['ã„ã„ã­', 'like', 'ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ', 'engagement', 'retweet', 'ãƒªãƒ„ã‚¤ãƒ¼ãƒˆ'])]
        total_engagement = 0
        if engagement_cols:
            for col in engagement_cols:
                if pd.api.types.is_numeric_dtype(df[col]):
                    total_engagement += df[col].sum()
            metrics["total_engagement"] = f"{int(total_engagement):,}ä»¶"
        else:
            metrics["total_engagement"] = "N/A"

        sentiment_col = None
        if 'ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆ' in df.columns:
            sentiment_col = 'ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆ'
        elif find_col(df, ['sent', 'ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆ']):
            sentiment_col = find_col(df, ['sent', 'ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆ'])
            
        if sentiment_col:
            pos_count = int(df[df[sentiment_col].astype(str).str.contains('ãƒã‚¸ãƒ†ã‚£ãƒ–|Positive', case=False, na=False)].shape[0])
            neg_count = int(df[df[sentiment_col].astype(str).str.contains('ãƒã‚¬ãƒ†ã‚£ãƒ–|Negative', case=False, na=False)].shape[0])
            
            metrics["positive_posts"] = f"{pos_count:,}ä»¶"
            metrics["negative_posts"] = f"{neg_count:,}ä»¶"
            
            if (pos_count + neg_count) > 0:
                tendency = ((pos_count - neg_count) / (pos_count + neg_count)) * 100
                metrics["sentiment_tendency_percent"] = f"{int(np.floor(tendency))}%" # % ã‚’è¿½åŠ 
            else:
                metrics["sentiment_tendency_percent"] = "0%"
        else:
            logger.warning("åˆ— 'ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆ' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            metrics["positive_posts"] = "N/A"
            metrics["negative_posts"] = "N/A"
            metrics["sentiment_tendency_percent"] = "N/A"

        summary = f"å…¨ä½“ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—ã€‚ç·æŠ•ç¨¿æ•°: {metrics['total_posts']}, ç·ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ: {metrics['total_engagement']}ã€‚"
        
        return {"data": metrics, "image_base64": None, "summary": summary}

    except Exception as e:
        logger.error(f"run_overall_metrics error: {e}", exc_info=True)
        return {"data": {"error": str(e)}, "image_base64": None, "summary": f"ã‚¨ãƒ©ãƒ¼: {e}"}

def run_cooccurrence_network_pyvis(df: pd.DataFrame, suggestion: Dict[str, Any]) -> Dict[str, Any]:
    """
    (Step B) pyvis ã‚’ä½¿ç”¨ã—ã€è©³ç´°ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«åŸºã¥ãå…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æ§‹ç¯‰ã™ã‚‹
    """
    logger.info("run_cooccurrence_network (pyvisç‰ˆ) å®Ÿè¡Œ...")
    results = {"data": pd.DataFrame(), "image_base64": None, "html_content": None, "summary": "", "ai_legend": None, "communities": None}
    
    # --- 1. UIã‹ã‚‰æ¸¡ã•ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è§£æ ---
    try:
        # UI (Step 5) ã§è¨­å®šã•ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å–å¾— (ui_... ã§å§‹ã¾ã‚‹ã‚­ãƒ¼)
        flag_col = suggestion.get('ui_selected_flag_col')
        selected_keywords = suggestion.get('ui_selected_keywords')
        text_col = suggestion.get('ui_selected_text_col')
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ (ä¸€æ‹¬å®Ÿè¡Œæ™‚ãªã©)
        if not flag_col:
            flag_col = find_col(df, ['å¸‚åŒºç”ºæ‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰', 'location', 'city', 'åœ°åŸŸ']) or find_col(df, ['è©±é¡Œã‚«ãƒ†ã‚´ãƒª', 'topic', 'category'])
        if not text_col:
            text_col = find_col(df, ['ANALYSIS_TEXT_COLUMN', 'text', 'content', 'æœ¬æ–‡'])
        if selected_keywords is None: # None ã¨ [] ã¯åŒºåˆ¥
            try:
                s = df[flag_col].dropna().astype(str).str.split(',').explode().str.strip()
                s = s[~s.isin(['', 'nan', 'Nan', 'NaN'])]
                selected_keywords = s.value_counts().index.tolist()[:10] # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯Top10
            except Exception:
                selected_keywords = []
        
        solver = suggestion.get('solver', 'barnesHut')
        gravity = suggestion.get('gravity', -2000)
        node_distance = suggestion.get('node_distance', 200)
        spring_length = suggestion.get('spring_length', 250)
        top_n_words_limit = suggestion.get('top_n_words_limit', 100)
        max_degree_cutoff = suggestion.get('max_degree_cutoff', 50)
        min_occurrence = suggestion.get('min_occurrence', 10)
        default_node_size = suggestion.get('default_node_size', 15)
        default_text_size = suggestion.get('default_text_size', 50)
        run_ai_legend = suggestion.get('run_ai_legend', False)
        
    except Exception as e:
        msg = f"UIãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è§£æã‚¨ãƒ©ãƒ¼: {e}"
        logger.error(f"run_cooccurrence_network: {msg}", exc_info=True)
        results["summary"] = msg
        return results

    if not selected_keywords:
        msg = "çµã‚Šè¾¼ã¿ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒé¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"
        logger.warning(f"run_cooccurrence_network: {msg}"); results["summary"] = msg; return results
    if not flag_col or not text_col:
        msg = "å¯¾è±¡åˆ— (çµã‚Šè¾¼ã¿åˆ—ã¾ãŸã¯ãƒ†ã‚­ã‚¹ãƒˆåˆ—) ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
        logger.warning(f"run_cooccurrence_network: {msg}"); results["summary"] = msg; return results
    if flag_col not in df.columns or text_col not in df.columns:
        msg = f"å¯¾è±¡åˆ— ({flag_col} ã¾ãŸã¯ {text_col}) ãŒDFã«å­˜åœ¨ã—ã¾ã›ã‚“ã€‚"
        logger.warning(f"run_cooccurrence_network: {msg}"); results["summary"] = msg; return results

    nlp = load_spacy_model()
    if nlp is None:
        msg = "spaCyæ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸã€‚"
        logger.error(msg); results["summary"] = msg; return results

    try:
        # --- 2. spaCy å‡¦ç† ---
        target_pos = {'NOUN', 'PROPN', 'ADJ', 'VERB'}
        stop_words = {
            'ã®', 'ã«', 'ã¯', 'ã‚’', 'ãŒ', 'ã§', 'ã¦', 'ã§ã™', 'ã¾ã™', 'ã“ã¨', 'ã‚‚ã®', 'ãã‚Œ', 'ã‚ã‚Œ',
            'ã“ã‚Œ', 'ãŸã‚', 'ã„ã‚‹', 'ã™ã‚‹', 'ã‚ã‚‹', 'ãªã„', 'ã„ã†', 'ã‚ˆã†', 'ãã†', 'ãªã©', 'ã•ã‚“',
            'çš„', 'äºº', 'è‡ªåˆ†', 'ç§', 'åƒ•', 'ä½•', 'ãã®', 'ã“ã®', 'ã‚ã®', 'ã‚Œã‚‹', 'ã‚‰ã‚Œã‚‹',
            'ã¦ã‚‹', 'ãªã‚‹', 'ä¸­', 'ã¨ã“ã‚', 'ãŸã¡', 'äººé”', 'ä»Šå›', 'æœ¬å½“', 'ã¨ã¦ã‚‚', 'è‰²ã€…'
        }
        
        custom_stop_words_str = suggestion.get('ui_custom_stop_words', '')
        if custom_stop_words_str:
            try:
                # ã‚«ãƒ³ãƒã€ç©ºç™½ã€æ”¹è¡Œã€èª­ç‚¹ï¼ˆã€ï¼‰ã§åŒºåˆ‡ã‚‰ã‚ŒãŸå˜èªã‚’ã‚»ãƒƒãƒˆã«è¿½åŠ 
                custom_set = set(
                    word.strip() for word in re.split(r'[\s,ã€\n]+', custom_stop_words_str) if word.strip()
                )
                stop_words.update(custom_set)
                logger.info(f"å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯: ã‚«ã‚¹ã‚¿ãƒ é™¤å¤–èª {len(custom_set)}ä»¶ ã‚’è¿½åŠ ã€‚")
            except Exception as e:
                logger.warning(f"ã‚«ã‚¹ã‚¿ãƒ é™¤å¤–èªã®è§£æå¤±æ•—: {e}")
        
        G = nx.Graph()
        
        # 1. ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§DataFrameã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        escaped_keywords = [re.escape(k) for k in selected_keywords]
        pattern = '|'.join(escaped_keywords)
        df_filtered = df[df[flag_col].astype(str).str.contains(pattern, na=False)]
        
        if df_filtered.empty:
            msg = "é¸æŠã—ãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€æŠ•ç¨¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
            logger.warning(f"run_cooccurrence_network: {msg}"); results["summary"] = msg; return results

        texts_to_analyze = df_filtered[text_col].dropna().astype(str)
        
        # 2. Top N å˜èªãƒªã‚¹ãƒˆã®ä½œæˆ
        st.session_state.progress_text = "å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯: (1/3) Top N å˜èªã‚’è¨ˆç®—ä¸­..."
        all_words = []
        for text in texts_to_analyze:
            doc = nlp(text)
            for token in doc:
                if (token.pos_ in target_pos) and (not token.is_stop) and (token.lemma_ not in stop_words) and (len(token.lemma_) > 1):
                    if token.lemma_ not in selected_keywords:
                        all_words.append(token.lemma_)
        
        if not all_words:
            msg = "ãƒ•ã‚£ãƒ«ã‚¿çµæœã‹ã‚‰åˆ†æå¯¾è±¡ã®å˜èªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
            logger.warning(f"run_cooccurrence_network: {msg}"); results["summary"] = msg; return results
            
        top_n_words_set = set(pd.Series(all_words).value_counts().head(top_n_words_limit).index)
        
        # 3. ã‚°ãƒ©ãƒ•(G)ã®æ§‹ç¯‰
        st.session_state.progress_text = "å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯: (2/3) ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æ§‹ç¯‰ä¸­..."
        for text in texts_to_analyze:
            doc = nlp(text)
            words_in_text = set()
            for token in doc:
                if (token.pos_ in target_pos) and (token.lemma_ in top_n_words_set):
                    words_in_text.add(token.lemma_)
            
            for word1, word2 in combinations(sorted(list(words_in_text)), 2):
                if G.has_edge(word1, word2):
                    G[word1][word2]['weight'] += 1
                else:
                    G.add_edge(word1, word2, weight=1)

        # 4. ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        edges_to_remove = [(u, v) for u, v, data in G.edges(data=True) if data['weight'] < min_occurrence]
        G.remove_edges_from(edges_to_remove)
        G.remove_nodes_from(list(nx.isolates(G)))

        degrees = dict(G.degree())
        nodes_to_remove = [node for node, degree in degrees.items() if degree > max_degree_cutoff]
        G.remove_nodes_from(nodes_to_remove)
        G.remove_nodes_from(list(nx.isolates(G)))
        
        if G.number_of_nodes() == 0 or G.number_of_edges() == 0:
            msg = f"ãƒ•ã‚£ãƒ«ã‚¿æ¡ä»¶ (æœ€å°å…±èµ·: {min_occurrence}, æœ€å¤§æ¥ç¶š: {max_degree_cutoff}) ã«ã‚ˆã‚Šã€è¡¨ç¤ºå¯èƒ½ãªãƒãƒ¼ãƒ‰ãŒ0ä»¶ã«ãªã‚Šã¾ã—ãŸã€‚"
            logger.warning(f"run_cooccurrence_network: {msg}"); results["summary"] = msg; return results

        # 5. pyvis ã‚°ãƒ©ãƒ•ã®ç”Ÿæˆ
        st.session_state.progress_text = "å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯: (3/3) ã‚°ãƒ©ãƒ•ã‚’æç”»ä¸­..."
        net = Network(height="700px", width="100%", cdn_resources='in_line')
        
        degrees = dict(G.degree())
        min_degree, max_degree = (min(degrees.values()) or 1), (max(degrees.values()) or 1)
        
        # 6. ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£æ¤œå‡ºã¨è‰²åˆ†ã‘
        community_map = {}
        communities_with_words = {}
        ai_legend_map = {}
        try:
            communities_list = community.greedy_modularity_communities(G)
            communities_with_words = {i: list(comm) for i, comm in enumerate(communities_list)}
            community_map = {node: i for i, comm in communities_with_words.items() for node in comm}
            logger.info(f"ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£æ¤œå‡ºæˆåŠŸã€‚{len(communities_list)}å€‹ã®ã‚¯ãƒ©ã‚¹ã‚¿ã‚’ç™ºè¦‹ã€‚")
        except Exception as e:
            logger.warning(f"ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£æ¤œå‡ºã«å¤±æ•—: {e}ã€‚è‰²åˆ†ã‘ãªã—ã§ç¶šè¡Œã—ã¾ã™ã€‚")
            
        results["communities"] = communities_with_words # å‡¡ä¾‹è¡¨ç¤ºç”¨ã«æ ¼ç´

        for node in G.nodes():
            if node not in degrees: continue
            size_factor = degrees.get(node, 0)
            size = default_node_size + 30 * (size_factor - min_degree) / (max_degree - min_degree + 1e-6)
            group_id = community_map.get(node, 0)
            color = COLOR_PALETTE[group_id % len(COLOR_PALETTE)]
            
            net.add_node(
                node, label=node, size=size, title=f"{node} (ã‚¯ãƒ©ã‚¹ã‚¿: {group_id}, çµåˆæ•°: {size_factor})",
                color=color,
                font={"size": default_text_size}
            )
            
        for u, v, data in G.edges(data=True):
            weight = data['weight']
            net.add_edge(u, v, title=f"å…±èµ·å›æ•°: {weight}", value=weight)

        if solver == 'barnesHut':
            net.barnes_hut(gravity=gravity, overlap=0.1)
        else:
            net.repulsion(node_distance=node_distance, spring_length=spring_length)
        net.solver = solver
        net.show_buttons(filter_=['physics', 'nodes', 'layout'])
        
        # 7. HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç”Ÿæˆã—ã¦è¿”ã™
        html_file = "cooccurrence_network.html"
        net.save_graph(html_file)
        with open(html_file, 'r', encoding='utf-8') as f:
            html_content = f.read()
        
        results["html_content"] = html_content
        
        edge_list = pd.DataFrame(G.edges(data=True), columns=["source", "target", "data"])
        edge_list['weight'] = edge_list['data'].apply(lambda x: x['weight'])
        results["data"] = edge_list[['source', 'target', 'weight']].sort_values(by="weight", ascending=False)
        results["summary"] = f"'{flag_col}' ( {', '.join(selected_keywords[:3])}...) ã§çµã‚Šè¾¼ã¿ã€å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ (pyvis) ã‚’ç”Ÿæˆã€‚{G.number_of_nodes()}ãƒãƒ¼ãƒ‰, {G.number_of_edges()}ã‚¨ãƒƒã‚¸ã€‚"

        # 8. AIå‡¡ä¾‹ç”Ÿæˆ
        if run_ai_legend and communities_with_words:
            st.session_state.progress_text = "å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯: (AI) å‡¡ä¾‹ã‚’ç”Ÿæˆä¸­..."
            llm = get_llm(model_name=MODEL_FLASH_LITE, temperature=0.1)
            if llm:
                prompt = PromptTemplate.from_template(
                    "ä»¥ä¸‹ã®ã€Œå˜èªãƒªã‚¹ãƒˆã€ã®å…±é€šãƒ†ãƒ¼ãƒã‚’ã€3èªä»¥å†…ã€‘ã§è€ƒæ¡ˆã—ã¦ãã ã•ã„ã€‚\n"
                    "# å˜èªãƒªã‚¹ãƒˆ (ä¸Šä½10ä»¶): {word_list_str}\n"
                    "# å›ç­” (3èªä»¥å†…):"
                )
                chain = prompt | llm | StrOutputParser()
                
                for group_id, words in communities_with_words.items():
                    if not words: continue
                    words_top10 = sorted(words, key=lambda w: degrees.get(w, 0), reverse=True)[:10]
                    words_str = ", ".join(words_top10)
                    try:
                        raw_label = chain.invoke({"word_list_str": words_str})
                        cleaned_label = re.sub(r'^(#|å›ç­”)\s*\(.*?\)\s*:\s*', '', raw_label.strip())
                        ai_legend_map[group_id] = cleaned_label
                        time.sleep(1.0) # Rate Limit
                    except Exception as e:
                        logger.error(f"AIå‡¡ä¾‹ç”Ÿæˆã‚¨ãƒ©ãƒ¼ (Group {group_id}): {e}")
                        ai_legend_map[group_id] = "(AIã‚¨ãƒ©ãƒ¼)"
                results["ai_legend"] = ai_legend_map
                results["summary"] += " AIã«ã‚ˆã‚‹å‡¡ä¾‹ç”Ÿæˆå®Œäº†ã€‚"
            else:
                results["summary"] += " AIå‡¡ä¾‹ç”Ÿæˆã‚¹ã‚­ãƒƒãƒ— (LLMãƒ­ãƒ¼ãƒ‰å¤±æ•—)ã€‚"
        
        st.session_state.progress_text = "å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ å®Œäº†ã€‚"
        return results

    except Exception as e:
        logger.error(f"run_cooccurrence_network (pyvisç‰ˆ) ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        results["summary"] = f"å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}"
        st.session_state.progress_text = f"å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ ã‚¨ãƒ©ãƒ¼: {e}"
        return results

def run_generic_category_summary(df: pd.DataFrame, suggestion: Dict[str, Any]) -> Dict[str, Any]:
    """
    (â˜…) æ±ç”¨: ã‚«ãƒ†ã‚´ãƒªåˆ—ã”ã¨ã«æŠ•ç¨¿æ•°ã€ã‚µãƒãƒª(AI)ã€ä¸Šä½ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’åˆ†æã™ã‚‹
    """
    logger.info("run_generic_category_summary å®Ÿè¡Œ...")
    results = {"data": pd.DataFrame(), "image_base64": None, "summary": ""}
    
    # 1. UI (Step 5) ã‹ã‚‰æ¸¡ã•ã‚ŒãŸã€Œåˆ†æè»¸ã¨ãªã‚‹ã‚«ãƒ†ã‚´ãƒªåˆ—ã€ã‚’å–å¾—
    default_topic_col = find_col(df, ['è©±é¡Œã‚«ãƒ†ã‚´ãƒª', 'topic', 'category'])
    topic_col = suggestion.get('ui_selected_category_col', default_topic_col)
    text_col = find_col(df, ['ANALYSIS_TEXT_COLUMN', 'text', 'content', 'æœ¬æ–‡'])

    if not topic_col or not text_col:
        msg = f"åˆ†æã«å¿…è¦ãªåˆ— (ã‚«ãƒ†ã‚´ãƒªåˆ—ã¾ãŸã¯ãƒ†ã‚­ã‚¹ãƒˆåˆ—) ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
        logger.warning(f"run_generic_category_summary: {msg}")
        return {"data": pd.DataFrame([{"error": msg}]), "image_base64": None, "summary": msg}
    if topic_col not in df.columns or text_col not in df.columns:
        msg = f"æŒ‡å®šã•ã‚ŒãŸåˆ— ('{topic_col}', '{text_col}') ãŒDFã«å­˜åœ¨ã—ã¾ã›ã‚“ã€‚"
        logger.warning(f"run_generic_category_summary: {msg}")
        return {"data": pd.DataFrame([{"error": msg}]), "image_base64": None, "summary": msg}
    
    # 2. (Enhancement 2.4) ä¸Šä½ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å€™è£œåˆ—ã‚’å‹•çš„ã«æ±ºå®š
    flag_cols = [col for col in df.columns if col.endswith('ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰')]
    location_col = find_col(df, ['å¸‚åŒºç”ºæ‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰', 'location', 'city', 'åœ°åŸŸ'])
    # location_col ã¨ topic_col è‡ªèº«ã‚’é™¤å¤–
    cols_to_use_for_keywords = [col for col in flag_cols if col != location_col and col != topic_col]
    logger.info(f"TOPã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é›†è¨ˆå¯¾è±¡ (åœ°åŸŸ/ãƒˆãƒ”ãƒƒã‚¯é™¤å¤–): {cols_to_use_for_keywords}")
    
    # 3. (Enhancement 2.1) ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰ã›ãšã€åˆ—ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯å€¤ä¸Šä½10ä»¶ã‚’å¯¾è±¡
    try:
        s = df[topic_col].astype(str).str.split(', ').explode()
        s = s[s.str.strip().isin(['', 'nan', 'None', 'N/A', 'è©²å½“ãªã—']) == False]
        s = s.str.strip()
        if s.empty:
            raise ValueError(f"ã‚«ãƒ†ã‚´ãƒªåˆ— '{topic_col}' ã«æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        
        target_categories = s.value_counts().head(10).index.tolist()
        logger.info(f"'{topic_col}' ã®ä¸Šä½10ã‚«ãƒ†ã‚´ãƒªã‚’åˆ†æå¯¾è±¡ã¨ã—ã¾ã™: {target_categories}")
        
    except Exception as e:
        msg = f"ã‚«ãƒ†ã‚´ãƒªåˆ— '{topic_col}' ã®å€¤ã®å–å¾—ã«å¤±æ•—: {e}"
        logger.error(msg, exc_info=True)
        return {"data": pd.DataFrame([{"error": msg}]), "image_base64": None, "summary": msg}

    results_list = []
    
    total_cats = len(target_categories)
    if 'progress_text' not in st.session_state:
            st.session_state.progress_text = ""
            
    for i, category in enumerate(target_categories):
        # (Bug 1.3) ã‚µãƒ–é€²æ—ã‚’æ›´æ–°
        st.session_state.progress_text = f"ã‚«ãƒ†ã‚´ãƒªæ·±æ˜ã‚Š ({i+1}/{total_cats}): {category}"
        
        df_filtered = df[df[topic_col].astype(str).str.contains(re.escape(category), na=False)]
        post_count = len(df_filtered)
        
        if post_count == 0:
            results_list.append({
                "category": category,
                "post_count": 0,
                "summary_ai": "N/A (æŠ•ç¨¿ãªã—)",
                "top_keywords": []
            })
            continue
        
        ai_suggestion = {
            "name": f"Summary for {category}",
            "description": f"ã€Œ{category}ã€ã‚«ãƒ†ã‚´ãƒªã«é–¢ã™ã‚‹ä»¥ä¸‹ã®æŠ•ç¨¿ã‚µãƒ³ãƒ—ãƒ«ã‚’èª­ã¿ã€ä¸»è¦ãªè©±é¡Œã‚’1ï½2æ–‡ã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚"
        }
        summary_ai = run_ai_summary_batch(df_filtered, ai_suggestion)
        
        # 4. (Enhancement 2.4) ä¸Šä½ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ (Python)
        top_keywords = []
        if cols_to_use_for_keywords and not df_filtered.empty:
            all_keywords_series = []
            for kw_col in cols_to_use_for_keywords:
                if kw_col in df_filtered.columns:
                    s_kw = df_filtered[kw_col].astype(str).str.split(', ').explode()
                    s_kw = s_kw[s_kw.str.strip().isin(['', 'nan', 'None', 'N/A', 'è©²å½“ãªã—']) == False]
                    s_kw = s_kw.str.strip()
                    if not s_kw.empty:
                        all_keywords_series.append(s_kw)
            if all_keywords_series:
                combined_s = pd.concat(all_keywords_series)
                top_keywords = combined_s.value_counts().head(5).index.tolist()
        
        results_list.append({
            "category": category,
            "post_count": post_count,
            "summary_ai": summary_ai,
            "top_keywords": top_keywords
        })
        
        time.sleep(max(TAGGING_SLEEP_TIME / 2, 1.0))

    st.session_state.progress_text = "ã‚«ãƒ†ã‚´ãƒªæ·±æ˜ã‚Š å®Œäº†ã€‚"
    results_df = pd.DataFrame(results_list)
    
    image_base64 = generate_graph_image(
        df=results_df,
        plot_type='bar',
        x_col='category',
        y_col='post_count',
        title=f"ã€Œ{topic_col}ã€åˆ¥ æŠ•ç¨¿æ•° (Top 10)"
    )
    
    summary = f"ã€Œ{topic_col}ã€åˆ¥ã®åˆ†æã‚’å®Ÿè¡Œã€‚æŠ•ç¨¿æ•°ã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆã—ã¾ã—ãŸã€‚"
    return {"data": results_df, "image_base64": image_base64, "summary": summary}

# --- 
# [ä¿®æ­£ç‰ˆ] app.py ã® L1628-L1715 (run_generic_engagement_top5)
# (str.contains() ãƒã‚°ã‚’ä¿®æ­£)
# ---
def run_generic_engagement_top5(df: pd.DataFrame, suggestion: Dict[str, Any]) -> Dict[str, Any]:
    """
    (â˜…) æ±ç”¨: ã‚«ãƒ†ã‚´ãƒªåˆ—åˆ¥ã«æ•°å€¤åˆ—TOP5æŠ•ç¨¿ã¨æ¦‚è¦(AI)ã‚’åˆ†æã™ã‚‹
    (â˜…) ä¿®æ­£: str.contains() ãƒã‚°ã‚’ä¿®æ­£ã—ã€explode() ãƒ™ãƒ¼ã‚¹ã®é›†è¨ˆã«å¤‰æ›´
    """
    logger.info("run_generic_engagement_top5 å®Ÿè¡Œ...")
    results = {"data": pd.DataFrame(), "image_base64": None, "summary": ""}

    # 1. UI (Step 5) ã‹ã‚‰æ¸¡ã•ã‚ŒãŸã€Œåˆ†æè»¸ã¨ãªã‚‹åˆ—ã€ã‚’å–å¾—
    default_topic_col = find_col(df, ['è©±é¡Œã‚«ãƒ†ã‚´ãƒª', 'topic', 'category'])
    default_text_col = find_col(df, ['ANALYSIS_TEXT_COLUMN', 'text', 'content', 'æœ¬æ–‡'])
    default_eng_col = find_engagement_cols(df, ['eng', 'like', 'ã„ã„ã­', 'ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ'])
    default_eng_col = default_eng_col[0] if default_eng_col else None

    topic_col = suggestion.get('ui_selected_category_col', default_topic_col)
    text_col = suggestion.get('ui_selected_text_col', default_text_col)
    engagement_col = suggestion.get('ui_selected_numeric_col', default_eng_col)
    
    if not topic_col or not text_col or not engagement_col:
        msg = f"åˆ†æã«å¿…è¦ãªåˆ— (ã‚«ãƒ†ã‚´ãƒªåˆ—, ãƒ†ã‚­ã‚¹ãƒˆåˆ—, æ•°å€¤åˆ—) ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
        logger.warning(f"run_generic_engagement_top5: {msg}")
        return {"data": pd.DataFrame([{"error": msg}]), "image_base64": None, "summary": msg}
    if topic_col not in df.columns:
        msg = f"ã‚«ãƒ†ã‚´ãƒªåˆ— '{topic_col}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
        return {"data": pd.DataFrame([{"error": msg}]), "image_base64": None, "summary": msg}
    if engagement_col not in df.columns or not pd.api.types.is_numeric_dtype(df[engagement_col]):
        msg = f"æ•°å€¤åˆ— '{engagement_col}' ãŒæ•°å€¤åˆ—ã¨ã—ã¦å­˜åœ¨ã—ã¾ã›ã‚“ã€‚"
        return {"data": pd.DataFrame([{"error": msg}]), "image_base64": None, "summary": msg}
    if text_col not in df.columns:
        msg = f"ãƒ†ã‚­ã‚¹ãƒˆåˆ— '{text_col}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
        return {"data": pd.DataFrame([{"error": msg}]), "image_base64": None, "summary": msg}

    # 2. (â˜…) --- ä¿®æ­£: explode ãƒ™ãƒ¼ã‚¹ã®é›†è¨ˆãƒ­ã‚¸ãƒƒã‚¯ ---
    try:
        # (â˜…) 1. å…ƒã®DFã‚’ explode ã™ã‚‹ (ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã‚’å …ç‰¢ã«å‡¦ç†)
        df_exploded = df.assign(**{topic_col: df[topic_col].astype(str).str.split(',')}).explode(topic_col)
        df_exploded[topic_col] = df_exploded[topic_col].str.strip()

        # (â˜…) 2. ç©ºç™½ãƒ»N/Aç­‰ã‚’é™¤å¤–
        s = df_exploded[topic_col]
        s = s[s.str.strip().isin(['', 'nan', 'None', 'N/A', 'è©²å½“ãªã—']) == False]
        
        if s.empty:
            raise ValueError(f"ã‚«ãƒ†ã‚´ãƒªåˆ— '{topic_col}' ã«æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            
        # (â˜…) 3. ä¸Šä½10ã‚«ãƒ†ã‚´ãƒªã‚’æ±ºå®š (ã“ã‚ŒãŒæ­£ã—ã„æ¯æ•°)
        target_categories = s.value_counts().head(10).index.tolist()
        logger.info(f"'{topic_col}' ã®ä¸Šä½10ã‚«ãƒ†ã‚´ãƒªã‚’åˆ†æå¯¾è±¡ã¨ã—ã¾ã™: {target_categories}")
    except Exception as e:
        msg = f"ã‚«ãƒ†ã‚´ãƒªåˆ— '{topic_col}' ã®å€¤ã®å–å¾—ã«å¤±æ•—: {e}"
        logger.error(msg, exc_info=True)
        return {"data": pd.DataFrame([{"error": msg}]), "image_base64": None, "summary": msg}

    # 3. (Enhancement 2.3) ãƒ¡ãƒ‡ã‚£ã‚¢ãƒªãƒ³ã‚¯åˆ—ã‚’ç‰¹å®š
    link_col_candidates = ['link', 'url', 'media_url', 'æŠ•ç¨¿URL', 'URL', 'Link', 'Url']
    found_link_col = find_col(df, link_col_candidates)
    if found_link_col:
        logger.info(f"ãƒ¡ãƒ‡ã‚£ã‚¢ãƒªãƒ³ã‚¯åˆ—: '{found_link_col}' ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
    else:
        logger.warning(f"ãƒ¡ãƒ‡ã‚£ã‚¢ãƒªãƒ³ã‚¯åˆ— ({link_col_candidates}) ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

    results_list = []
    
    total_cats = len(target_categories)
    if 'progress_text' not in st.session_state:
            st.session_state.progress_text = ""

    for i, category in enumerate(target_categories):
        st.session_state.progress_text = f"æ•°å€¤åˆ—TOP5åˆ†æä¸­ ({i+1}/{total_cats}): {category}"
        
        df_filtered = df_exploded[df_exploded[topic_col] == category]
        post_count = len(df_filtered)
        
        if post_count == 0:
            continue
            
        # (â˜…) ãƒ•ã‚£ãƒ«ã‚¿æ¸ˆã¿DFã‹ã‚‰ nlargest ã‚’å–å¾— (ã“ã‚Œã¯å…ƒã®ãƒ­ã‚¸ãƒƒã‚¯ã§OK)
        df_top5 = df_filtered.nlargest(5, engagement_col, keep='first')
        top5_posts_data = []
        
        if df_top5.empty:
                results_list.append({
                "category": category,
                "post_count": post_count,
                "top_posts": []
            })
                continue

        # (â˜…) --- [å“è³ªå‘ä¸Šæ¡ˆ B-2] AIå‘¼ã³å‡ºã—ã‚’ãƒ«ãƒ¼ãƒ—ã®å¤–ã«å‡ºã™ ---
        top_5_texts_list = df_top5[text_col].astype(str).tolist()
        combined_texts_for_ai = "\n---\n".join([f"æŠ•ç¨¿{idx+1}: {text[:300]}..." for idx, text in enumerate(top_5_texts_list)])
        
        ai_suggestion_combined = {
            "name": f"Summary for Top 5 {category}",
            "description": f"ä»¥ä¸‹ã®ã€Œ{category}ã€ã‚«ãƒ†ã‚´ãƒªã§ã€Œ{engagement_col}ã€ãŒå¤šã‹ã£ãŸæŠ•ç¨¿ï¼ˆ{len(top_5_texts_list)}ä»¶ï¼‰ã®ã‚µãƒ³ãƒ—ãƒ«ã§ã™ã€‚ã“ã‚Œã‚‰ã®æŠ•ç¨¿ã«å…±é€šã™ã‚‹ã€Œäººæ°—ã®ç†ç”±ã€ã‚„ã€Œå‚¾å‘ã€ã‚’1ã€œ2æ–‡ã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚\n\n# æŠ•ç¨¿ã‚µãƒ³ãƒ—ãƒ«:\n{combined_texts_for_ai}"
        }
        common_summary_ai = run_ai_summary_batch(df_filtered, ai_suggestion_combined)
        time.sleep(max(TAGGING_SLEEP_TIME / 2, 1.0)) # 1ã‚«ãƒ†ã‚´ãƒª1ã‚³ãƒ¼ãƒ«å¾Œã®ã‚¹ãƒªãƒ¼ãƒ—

        for _, row in df_top5.iterrows():
            post_text = str(row[text_col])
            engagement_value = row[engagement_col]
            
            # (â˜…) 1å›ã ã‘å‘¼ã³å‡ºã—ãŸAIã®å…±é€šã‚µãƒãƒªã‚’ä½¿ç”¨
            summary_ai_for_post = common_summary_ai 
            
            # 4. (Enhancement 2.3) ãƒ¡ãƒ‡ã‚£ã‚¢ãƒªãƒ³ã‚¯ã‚’å–å¾—
            link_value = None
            if found_link_col and found_link_col in row and pd.notna(row[found_link_col]):
                link_value = str(row[found_link_col])
            
            top5_posts_data.append({
                "engagement": int(engagement_value),
                "summary_ai": summary_ai_for_post, 
                "original_text_snippet": post_text[:100],
                "media_link": link_value
            })
            
            # (â˜…) ãƒ«ãƒ¼ãƒ—å†…ã®AIã‚³ãƒ¼ãƒ«ã¨ time.sleep ã‚’å‰Šé™¤

        results_list.append({
            "category": category,
            "post_count": post_count,
            "top_posts": top5_posts_data
        })

    st.session_state.progress_text = "æ•°å€¤åˆ—TOP5åˆ†æ å®Œäº†ã€‚"
    results_df = pd.DataFrame(results_list)
    
    summary = f"ã€Œ{topic_col}ã€åˆ¥ã®é«˜ã€Œ{engagement_col}ã€æŠ•ç¨¿TOP5ã‚’æŠ½å‡ºã—ã¾ã—ãŸã€‚"
    return {"data": results_df, "image_base64": None, "summary": summary}

# (New Feature 3.1) A/Bæ¯”è¼ƒé–¢æ•°
def run_ab_comparison(df: pd.DataFrame, suggestion: Dict[str, Any]) -> Dict[str, Any]:
    """
    (Step B) 2ã¤ã®ã‚°ãƒ«ãƒ¼ãƒ—(A, B)ã®æŠ•ç¨¿æ•°ã¨äººæ°—è¦³å…‰åœ°ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’æ¯”è¼ƒã™ã‚‹
    """
    logger.info("run_ab_comparison å®Ÿè¡Œ...")
    results = {"data": {}, "image_base64": None, "summary": ""}
    
    try:
        # 1. UIã‹ã‚‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å–å¾—
        ab_params = suggestion.get('ui_ab_params', {})
        a_col = ab_params.get('a_col')
        a_val = ab_params.get('a_val')
        b_col = ab_params.get('b_col')
        b_val = ab_params.get('b_val')
        
        # æ±ç”¨çš„ã«åˆ—ã‚’è¦‹ã¤ã‘ã‚‹
        location_col = find_col(df, ['å¸‚åŒºç”ºæ‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰', 'location', 'city', 'åœ°åŸŸ'])
        topic_col = find_col(df, ['è©±é¡Œã‚«ãƒ†ã‚´ãƒª', 'topic', 'category'])

        if not all([a_col, a_val, b_col, b_val, location_col, topic_col]):
            msg = f"A/Bæ¯”è¼ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™ (A/Båˆ—/å€¤ã€åœ°åŸŸåˆ—ã€ãƒˆãƒ”ãƒƒã‚¯åˆ—ãŒå¿…è¦ã§ã™)"
            logger.warning(f"run_ab_comparison: {msg}")
            return {"data": {"error": msg}, "image_base64": None, "summary": msg}

        # 2. ã‚°ãƒ«ãƒ¼ãƒ—A, Bã®DataFrameã‚’ä½œæˆ
        df_A = df[df[a_col].astype(str).str.contains(re.escape(a_val), na=False)]
        df_B = df[df[b_col].astype(str).str.contains(re.escape(b_val), na=False)]
        
        if df_A.empty or df_B.empty:
             msg = f"ã‚°ãƒ«ãƒ¼ãƒ—A ({a_val}: {len(df_A)}ä»¶) ã¾ãŸã¯ ã‚°ãƒ«ãƒ¼ãƒ—B ({b_val}: {len(df_B)}ä»¶) ã®ãƒ‡ãƒ¼ã‚¿ãŒ0ä»¶ã§ã™ã€‚"
             logger.warning(f"run_ab_comparison: {msg}")
             return {"data": {"error": msg}, "image_base64": None, "summary": msg}

        # 3. ã‚«ãƒ†ã‚´ãƒªåˆ¥æŠ•ç¨¿æ•° æ¯”è¼ƒ
        cats_A = df_A[topic_col].value_counts().rename(f"Count (A: {a_val})")
        cats_B = df_B[topic_col].value_counts().rename(f"Count (B: {b_val})")
        
        df_cat_compare = pd.concat([cats_A, cats_B], axis=1).fillna(0).astype(int)
        df_cat_compare['Total'] = df_cat_compare.sum(axis=1)
        df_cat_compare.sort_values(by='Total', ascending=False, inplace=True)
        sum_A = df_cat_compare[cats_A.name].sum()
        sum_B = df_cat_compare[cats_B.name].sum()
        df_cat_compare[f"Share (A: {a_val})"] = (df_cat_compare[cats_A.name] / sum_A).map('{:.1%}'.format) if sum_A > 0 else 0
        df_cat_compare[f"Share (B: {b_val})"] = (df_cat_compare[cats_B.name] / sum_B).map('{:.1%}'.format) if sum_B > 0 else 0

        # 4. è¦³å…‰åœ°åˆ¥(åœ°åŸŸ) é †ä½å¤‰å‹• æ¯”è¼ƒ
        locs_A = df_A[location_col].value_counts().rename(f"Count (A: {a_val})")
        locs_B = df_B[location_col].value_counts().rename(f"Count (B: {b_val})")
        
        df_rank_compare = pd.concat([locs_A, locs_B], axis=1).fillna(0).astype(int)
        df_rank_compare[f"Rank (A: {a_val})"] = df_rank_compare[locs_A.name].rank(ascending=False, method='min').astype(int)
        df_rank_compare[f"Rank (B: {b_val})"] = df_rank_compare[locs_B.name].rank(ascending=False, method='min').astype(int)
        
        df_rank_compare['Rank Change (A vs B)'] = (df_rank_compare[f"Rank (B: {b_val})"] - df_rank_compare[f"Rank (A: {a_val})"]).astype(int)
        
        df_rank_compare.sort_values(by=f"Count (B: {b_val})", ascending=False, inplace=True)
        df_rank_compare = df_rank_compare[[
            f"Rank (A: {a_val})", f"Count (A: {a_val})", 
            f"Rank (B: {b_val})", f"Count (B: {b_val})", 
            'Rank Change (A vs B)'
        ]]
        
        summary = f"A/Bæ¯”è¼ƒ: ã€Œ{a_val}ã€ (A: {len(df_A)}ä»¶) vs ã€Œ{b_val}ã€ (B: {len(df_B)}ä»¶) ã‚’å®Ÿè¡Œã€‚"
        
        results["data"] = {
            "category_comparison": df_cat_compare.reset_index().rename(columns={'index': topic_col}).to_dict(orient='records'),
            "ranking_comparison": df_rank_compare.reset_index().rename(columns={'index': location_col}).head(20).to_dict(orient='records')
        }
        results["summary"] = summary
        
        return results
        
    except Exception as e:
        logger.error(f"run_ab_comparison error: {e}", exc_info=True)
        return {"data": {"error": f"A/Bæ¯”è¼ƒã‚¨ãƒ©ãƒ¼: {e}"}, "image_base64": None, "summary": f"A/Bæ¯”è¼ƒã‚¨ãƒ©ãƒ¼: {e}"}

# --- 8.2. (â˜…) Step B: AIåˆ†æãƒ˜ãƒ«ãƒ‘ãƒ¼ (Bug 1.1 ä¿®æ­£) ---
def run_ai_summary_batch(df: pd.DataFrame, suggestion: Dict[str, Any]) -> str:
    """
    (Step B) AI (Flash Lite) ã‚’ä½¿ç”¨ã—ã¦ã€æŒ‡å®šã•ã‚ŒãŸã‚¿ã‚¹ã‚¯(description)ã‚’å®Ÿè¡Œã™ã‚‹ã€‚
    (â˜…) æ”¹å–„: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ã€Œè¦ç´„ã€ã‹ã‚‰ã€Œè€ƒå¯Ÿã€ã«å¤‰æ›´
    """
    logger.info(f"run_ai_summary_batch å®Ÿè¡Œ (ã‚¿ã‚¹ã‚¯: {suggestion.get('name', 'N/A')})...")
    
    llm = get_llm(model_name=MODEL_FLASH_LITE, temperature=0.1, timeout_seconds=120)
    if llm is None:
        logger.error("run_ai_summary_batch: LLM (Flash Lite) ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")
        return "AIãƒ¢ãƒ‡ãƒ«(Flash Lite)ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"

    try:
        ai_prompt_instruction = suggestion.get('description', 'ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚¤ãƒ³ã‚µã‚¤ãƒˆã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚')
        
        total_rows_count = len(df)
        
        text_col = find_col(df, ['ANALYSIS_TEXT_COLUMN', 'text', 'content', 'æœ¬æ–‡'])
        
        if text_col:
            # ãƒ†ã‚­ã‚¹ãƒˆåˆ—ãŒã‚ã‚‹å ´åˆã€ã‚µãƒ³ãƒ—ãƒ«ã‚’æŠ½å‡º
            sample_size = min(50, total_rows_count)
            if sample_size > 0:
                text_samples = df[text_col].dropna().sample(n=sample_size, random_state=1).tolist()
                data_context = "\n".join([f"- {text[:200]}..." for text in text_samples])
            else:
                data_context = "ï¼ˆã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ãªã—ï¼‰"
        else:
            # ãƒ†ã‚­ã‚¹ãƒˆåˆ—ãŒãªã„å ´åˆã€DFã®å…ˆé ­ã‚’JSONã§æ¸¡ã™
            data_context = df.head(10).to_json(orient='records', force_ascii=False)

        prompt = PromptTemplate.from_template(
            """
            ã‚ãªãŸã¯ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒŠãƒªã‚¹ãƒˆã§ã™ã€‚ã€ŒæŒ‡ç¤ºã€ã¨ã€Œãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«ã€ã«åŸºã¥ãã€
            å˜ãªã‚‹è¦ç´„ã§ã¯ãªãã€ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰èª­ã¿å–ã‚Œã‚‹ã€ã‚¤ãƒ³ã‚µã‚¤ãƒˆï¼ˆç™ºè¦‹ï¼‰ã€‘ã‚„ã€å‚¾å‘ã®èƒŒæ™¯ï¼ˆä»®èª¬ï¼‰ã€‘ã‚’
            ç°¡æ½”ã«è€ƒå¯Ÿã—ã¦ãã ã•ã„ã€‚

            # æŒ‡ç¤º (Task):
            {ai_instruction}

            # ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ« (Data Sample):
            (åˆ†æå¯¾è±¡: å…¨ {total_rows} ä»¶ã‹ã‚‰ã®æŠœç²‹)
            {data_context}

            # è€ƒå¯Ÿã®ãƒã‚¤ãƒ³ãƒˆ:
            - ãƒ‡ãƒ¼ã‚¿ãŒç¤ºã—ã¦ã„ã‚‹ã€Œæœ€ã‚‚é‡è¦ãªäº‹å®Ÿã€ã¯ä½•ã‹ï¼Ÿ
            - ãªãœãã®å‚¾å‘ãŒèµ·ãã¦ã„ã‚‹ã®ã‹ï¼ˆèƒŒæ™¯ãƒ»åŸå› ã®ä»®èª¬ï¼‰ï¼Ÿ
            - ï¼ˆã‚‚ã—ã‚ã‚Œã°ï¼‰ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰èª­ã¿å–ã‚Œã‚‹ã€Œæ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®ãƒ’ãƒ³ãƒˆã€ã¯ä½•ã‹ï¼Ÿ

            # å›ç­” (åˆ†æçµæœã®è€ƒå¯Ÿã®ã¿ã‚’Markdownå½¢å¼ã§):
            """
        )

        chain = prompt | llm | StrOutputParser()
        
        response_str = chain.invoke({
            "ai_instruction": ai_prompt_instruction,
            "data_context": data_context,
            "total_rows": total_rows_count
        })
        
        return response_str.strip()

    except Exception as e:
        logger.error(f"run_ai_summary_batch å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        return f"AIåˆ†æã‚¿ã‚¹ã‚¯ã®å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"

# --- 8.3. (â˜…) Step B: åˆ†æå®Ÿè¡Œãƒ«ãƒ¼ã‚¿ãƒ¼ (æ±ç”¨åŒ–å¯¾å¿œ) ---
def execute_analysis(
    analysis_name: str,
    df: pd.DataFrame,
    suggestion: Dict[str, Any]
) -> Dict[str, Any]:
    """
    (Step B) åˆ†æåã«åŸºã¥ãã€é©åˆ‡ãªPythonã¾ãŸã¯AIã®å®Ÿè¡Œé–¢æ•°ã‚’å‘¼ã³å‡ºã™ãƒ«ãƒ¼ã‚¿ãƒ¼
    """
    try:
        analysis_type = suggestion.get('type', 'python')
        
        if analysis_type == 'python':
            if analysis_name == "å…¨ä½“ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹":
                return run_overall_metrics(df, suggestion)
            elif analysis_name.startswith("å˜ç´”é›†è¨ˆ:"):
                return run_simple_count(df, suggestion)
            elif analysis_name.startswith("ã‚¯ãƒ­ã‚¹é›†è¨ˆ"):
                return run_crosstab(df, suggestion)
            elif analysis_name == "æ™‚ç³»åˆ—ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æ":
                return run_timeseries(df, suggestion)
            elif analysis_name == "ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°ï¼ˆé »å‡ºå˜èªï¼‰":
                return run_text_mining(df, suggestion)
            elif analysis_name == "å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯":
                return run_cooccurrence_network_pyvis(df, suggestion)
            # æ±ç”¨ã‚¿ã‚¹ã‚¯åã«å¯¾å¿œ
            elif analysis_name == "ã‚«ãƒ†ã‚´ãƒªåˆ—ã®é›†è¨ˆã¨æ·±æ˜ã‚Š":
                return run_generic_category_summary(df, suggestion)
            elif analysis_name == "ã‚«ãƒ†ã‚´ãƒªåˆ¥ æ•°å€¤åˆ—TOP5åˆ†æ":
                return run_generic_engagement_top5(df, suggestion)
            # A/Bæ¯”è¼ƒã®ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°
            elif analysis_name == "A/B æ¯”è¼ƒåˆ†æ":
                return run_ab_comparison(df, suggestion)
            else:
                logger.warning(f"Pythonåˆ†æ '{analysis_name}' ã®å®Ÿè¡Œãƒ­ã‚¸ãƒƒã‚¯ãŒå®šç¾©ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚AIåˆ†æã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™ã€‚")
                suggestion['description'] = f"ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«ã‚’ä½¿ã„ã€'{analysis_name}' ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚"
                ai_result_str = run_ai_summary_batch(df, suggestion)
                return {"data": ai_result_str, "image_base64": None, "summary": ai_result_str[:100] + "..."}
        
        elif analysis_type == 'ai':
            ai_result_str = run_ai_summary_batch(df, suggestion)
            return {"data": ai_result_str, "image_base64": None, "summary": ai_result_str[:100] + "..."}
            
        else:
            err_msg = f"ä¸æ˜ãªåˆ†æã‚¿ã‚¤ãƒ— ('{analysis_type}') ã§ã™: {analysis_name}"
            return {"data": err_msg, "image_base64": None, "summary": err_msg}
            
    except Exception as e:
        logger.error(f"execute_analysis ('{analysis_name}') å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        err_msg = f"åˆ†æ '{analysis_name}' ã®å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"
        return {"data": err_msg, "image_base64": None, "summary": err_msg}

# --- 8.4. (â˜…) Step B: JSONå‡ºåŠ›ãƒ˜ãƒ«ãƒ‘ãƒ¼ (æ±ç”¨åŒ–å¯¾å¿œ) ---
def convert_results_to_json_string(results_dict: Dict[str, Any]) -> str:
    """
    (Step B) å®Ÿè¡Œã•ã‚ŒãŸåˆ†æçµæœ(dict)ã‚’ã€Step Cã§èª­ã¿è¾¼ã‚€ãŸã‚ã®JSONLæ–‡å­—åˆ—ã«å¤‰æ›ã™ã‚‹ã€‚
    "OverallSummary" (å…¨ä½“ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹) ã¯ç‰¹åˆ¥æ‰±ã„ã—ã€ä»–ã®ã‚¿ã‚¹ã‚¯ã®ã‚µãƒãƒªæƒ…å ±ã‚‚é›†ç´„ã™ã‚‹ã€‚
    """
    logger.info(f"JSONLå¤‰æ›é–‹å§‹: {len(results_dict)}ä»¶ã®çµæœã‚’å‡¦ç†...")
    json_lines = []
    overall_summary_data = {}
    task_summaries = {} # ä»–ã‚¿ã‚¹ã‚¯ã®ã‚µãƒãƒªã‚’é›†ç´„ã™ã‚‹ãŸã‚

    # --- 1. ã¾ãš "å…¨ä½“ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹" (OverallSummary) ã‚’æ¢ã™ ---
    overall_task_name = "å…¨ä½“ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹"
    if overall_task_name in results_dict:
        result = results_dict[overall_task_name]
        overall_summary_data = {
            "analysis_task": "OverallSummary",
            "data": result.get("data", {"error": "data not found"}),
            "summary": result.get("summary", ""),
            "image_base64": None,
            "image_note": "No image",
            "analysis_summaries": {} # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€
        }
    else:
        logger.warning("JSONLå¤‰æ›: 'å…¨ä½“ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹' (OverallSummary) ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")


    # --- 2. "OverallSummary" ä»¥å¤–ã®ã‚¿ã‚¹ã‚¯ã‚’å‡¦ç† ---
    for task_name, result in results_dict.items():
        if task_name == overall_task_name:
            continue # å¾Œã§å‡¦ç†ã™ã‚‹ã®ã§ã‚¹ã‚­ãƒƒãƒ—

        try:
            line_data = {}
            line_data["analysis_task"] = task_name
            line_data["summary"] = result.get("summary", "N/A")

            # data ã®å‹ã«å¿œã˜ã¦ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚º
            data = result.get("data")
            if isinstance(data, pd.DataFrame):
                # æ±ç”¨åŒ–: TOP5ç³»ã‚¿ã‚¹ã‚¯ã¯DFã ãŒã€ä¸­èº«ã¯è¾æ›¸ã®ãƒªã‚¹ãƒˆ
                if task_name == "ã‚«ãƒ†ã‚´ãƒªåˆ¥ æ•°å€¤åˆ—TOP5åˆ†æ":
                    line_data["data"] = data.to_dict(orient='records')
                else:
                    # é€šå¸¸ã®DFã¯JSONæ–‡å­—åˆ—ã« (ãƒˆãƒ¼ã‚¯ãƒ³æ•°ç¯€ç´„)
                    if len(data) > 500:
                        line_data["data"] = data.head(500).to_json(orient='records', force_ascii=False)
                        line_data["note"] = f"Data truncated. Showing 500 of {len(data)} records."
                    else:
                        line_data["data"] = data.to_json(orient='records', force_ascii=False)
            
            elif isinstance(data, pd.Series):
                line_data["data"] = data.to_dict()
            
            elif isinstance(data, dict) or isinstance(data, list): # ãƒ¡ãƒˆãƒªã‚¯ã‚¹, A/Bæ¯”è¼ƒ
                line_data["data"] = data

            elif isinstance(data, str): # AIã®å›ç­”
                line_data["data"] = data
            
            elif data is None or (hasattr(data, 'empty') and data.empty):
                line_data["data"] = None
                record["note"] = "No data returned from analysis."
            
            else:
                line_data["data"] = str(data)

            # ç”»åƒ (Base64) ã¨ HTML (pyvis) ã®å‡¦ç†
            html_content = result.get("html_content") # pyvisç”¨
            image_base64 = result.get("image_base64")

            if html_content:
                 line_data["image_base64"] = None
                 line_data["image_note"] = "No image (pyvis HTML)"
            elif image_base64 and len(image_base64) < (1024 * 1024 * 1.0):
                line_data["image_base64"] = image_base64
                line_data["image_note"] = "Base64 encoded PNG image attached."
            elif image_base64:
                line_data["image_base64"] = None
                line_data["image_note"] = "Image was generated but exceeded 1MB and was not included."
            else:
                line_data["image_base64"] = None
                line_data["image_note"] = "No image generated for this task."

            json_lines.append(json.dumps(line_data, ensure_ascii=False, default=str))
            task_summaries[task_name] = line_data["summary"] # ã‚µãƒãƒªã‚’åé›†
            
        except Exception as e:
            logger.error(f"JSONLå¤‰æ›ã‚¨ãƒ©ãƒ¼ ({task_name}): {e}", exc_info=True)
            json_lines.append(json.dumps({"analysis_task": task_name, "error": str(e)}))

    # --- 3. OverallSummary ã«åé›†ã—ãŸã‚µãƒãƒªã‚’çµåˆ ---
    if overall_summary_data:
        overall_summary_data["analysis_summaries"] = task_summaries
        # JSONLã® *å…ˆé ­* ã« OverallSummary ã‚’è¿½åŠ 
        json_lines.insert(0, json.dumps(overall_summary_data, ensure_ascii=False))
    
    logger.info(f"JSONLå¤‰æ›å®Œäº†: {len(json_lines)}è¡Œã®JSONLã‚’ç”Ÿæˆã€‚")
    return "\n".join(json_lines)

def render_step_b():
    """(Step B) åˆ†ææ‰‹æ³•ã®ææ¡ˆãƒ»å®Ÿè¡Œãƒ»ãƒ‡ãƒ¼ã‚¿å‡ºåŠ›UIã‚’æç”»ã™ã‚‹"""
    st.title("ğŸ“Š Step B: ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–åˆ†æã¨ãƒ‡ãƒ¼ã‚¿å‡ºåŠ›")

    # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã®åˆæœŸåŒ–
    if 'df_flagged_B' not in st.session_state:
        st.session_state.df_flagged_B = pd.DataFrame()
    if 'suggestions_B' not in st.session_state: # ã™ã¹ã¦ã®ææ¡ˆ (ã‚¿ã‚¹ã‚¯å -> è©³ç´°dict)
        st.session_state.suggestions_B = {}
    if 'selected_tasks_B' not in st.session_state: # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒãƒã‚§ãƒƒã‚¯ã—ãŸã‚¿ã‚¹ã‚¯å (set)
        st.session_state.selected_tasks_B = set()
    if 'step_b_results' not in st.session_state: # å®Ÿè¡Œçµæœ (ã‚¿ã‚¹ã‚¯å -> çµæœdict)
        st.session_state.step_b_results = {}
    if 'step_b_json_output' not in st.session_state:
        st.session_state.step_b_json_output = None
    if 'progress_text' not in st.session_state:
         st.session_state.progress_text = ""
    if 'suggestions_attempted_B' not in st.session_state: # ææ¡ˆãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ãŸã‹
        st.session_state.suggestions_attempted_B = False
        
    if 'tips_list' not in st.session_state:
        st.session_state.tips_list = []
    if 'current_tip_index' not in st.session_state:
        st.session_state.current_tip_index = 0
    if 'last_tip_time' not in st.session_state:
        st.session_state.last_tip_time = time.time()
    # A/Bæ¯”è¼ƒç”¨ã®ã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆå€¤ã‚’ä¿æŒ
    if 'step_b_ab_params' not in st.session_state:
        st.session_state.step_b_ab_params = {}
    # æ±ç”¨ã‚«ãƒ†ã‚´ãƒªåˆ†æç”¨ã®ã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆå€¤ã‚’ä¿æŒ
    if 'step_b_generic_params' not in st.session_state:
        st.session_state.step_b_generic_params = {}

    # --- 1. ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ ---
    st.header("Step 1: ã‚­ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ¸ˆã¿CSVã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
    st.info(f"Step A ã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã—ãŸ CSV (Curated_Data.csv) ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
    uploaded_flagged_file = st.file_uploader(
        "ãƒ•ãƒ©ã‚°ä»˜ã‘æ¸ˆã¿CSVãƒ•ã‚¡ã‚¤ãƒ«",
        type=['csv'],
        key="step_b_uploader"
    )

    if uploaded_flagged_file:
        try:
            current_file_id = f"{uploaded_flagged_file.name}_{uploaded_flagged_file.size}"
            # ãƒ•ã‚¡ã‚¤ãƒ«ãŒå¤‰æ›´ã•ã‚ŒãŸå ´åˆã®ã¿ãƒªãƒ­ãƒ¼ãƒ‰ï¼†ãƒªã‚»ãƒƒãƒˆ
            if ('df_flagged_B' not in st.session_state or 
                st.session_state.df_flagged_B.empty or 
                st.session_state.get('current_file_id_B') != current_file_id):
                
                logger.info(f"Step B: æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ« {current_file_id} ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚")
                df, err = read_file(uploaded_flagged_file)
                if err:
                    st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {err}")
                    st.session_state.df_flagged_B = pd.DataFrame()
                    st.session_state.current_file_id_B = None
                    return
                
                st.session_state.df_flagged_B = df
                st.session_state.current_file_id_B = current_file_id
                
                # é–¢é€£ã‚¹ãƒ†ãƒ¼ãƒˆã‚’ã™ã¹ã¦ãƒªã‚»ãƒƒãƒˆ
                st.session_state.suggestions_B = {} 
                st.session_state.selected_tasks_B = set()
                st.session_state.step_b_results = {}
                st.session_state.step_b_json_output = None
                st.session_state.suggestions_attempted_B = False
                st.session_state.step_b_ab_params = {}
                st.session_state.step_b_generic_params = {}
                
                st.success(f"ãƒ•ã‚¡ã‚¤ãƒ«ã€Œ{uploaded_flagged_file.name}ã€èª­è¾¼å®Œäº† ({len(df)}è¡Œ)")
                with st.expander("ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ (å…ˆé ­5è¡Œ)", expanded=True):
                    st.dataframe(df.head())
            
            else:
                if 'df_flagged_B' in st.session_state and not st.session_state.df_flagged_B.empty:
                    with st.expander("ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ (å…ˆé ­5è¡Œ)"):
                        st.dataframe(st.session_state.df_flagged_B.head())
                
        except Exception as e:
            logger.error(f"Step B ãƒ•ã‚¡ã‚¤ãƒ«èª­è¾¼ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            st.session_state.df_flagged_B = pd.DataFrame()
            st.session_state.current_file_id_B = None
            return
    else:
        # ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚¯ãƒªã‚¢ã•ã‚ŒãŸã‚‰ã€é–¢é€£ã‚¹ãƒ†ãƒ¼ãƒˆã‚‚ã‚¯ãƒªã‚¢
        st.session_state.df_flagged_B = pd.DataFrame()
        st.session_state.suggestions_B = {}
        st.session_state.selected_tasks_B = set()
        st.session_state.step_b_results = {}
        st.session_state.step_b_json_output = None
        st.session_state.current_file_id_B = None
        st.session_state.suggestions_attempted_B = False
        st.session_state.step_b_ab_params = {}
        st.session_state.step_b_generic_params = {}
        st.warning("åˆ†æã‚’ç¶šã‘ã‚‹ã«ã¯ã€Step A ã§ç”Ÿæˆã—ãŸCSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
        return

    df_B = st.session_state.df_flagged_B
    
    all_cols = list(df_B.columns)
    
    # æ±ç”¨ã‚«ãƒ†ã‚´ãƒªåˆ—: ...ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰, ã‚«ãƒ†ã‚´ãƒª, topic ãªã©
    base_flag_cols = find_cols(df_B, ['key', 'keyword', 'ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰', 'ã‚«ãƒ†ã‚´ãƒª', 'topic', 'ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°'])
    location_col_search = find_col(df_B, ['å¸‚åŒºç”ºæ‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰', 'location', 'city', 'åœ°åŸŸ'])
    # æ±ç”¨ã‚«ãƒ†ã‚´ãƒªåˆ— (å ´æ‰€åˆ—ã‚’é™¤å¤–)
    flag_cols = sorted(list(set([c for c in base_flag_cols if c is not None and c != location_col_search])))
    # å ´æ‰€åˆ—
    location_cols = [location_col_search] if location_col_search else []
    # ã‚«ãƒ†ã‚´ãƒª + å ´æ‰€
    all_categorical_cols = flag_cols + location_cols
    
    # ãƒ†ã‚­ã‚¹ãƒˆåˆ—
    object_cols = df_B.select_dtypes(include='object').columns.tolist()
    text_cols = [col for col in object_cols if col not in all_categorical_cols]
    main_text_col_search = find_col(df_B, ['ANALYSIS_TEXT_COLUMN'])
    if main_text_col_search and main_text_col_search in text_cols:
         text_cols.insert(0, text_cols.pop(text_cols.index(main_text_col_search)))
    
    # æ—¥ä»˜åˆ—
    date_col_search = find_col(df_B, ['date', 'time', 'æ—¥ä»˜', 'æ—¥æ™‚'])
    date_cols = [date_col_search] if date_col_search is not None else []
    
    # æ•°å€¤åˆ—
    numeric_cols = df_B.select_dtypes(include=np.number).columns.tolist()
    engagement_cols = find_engagement_cols(df_B, ['eng', 'like', 'ã„ã„ã­', 'ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ'])

    # --- 2. åˆ†ææ‰‹æ³•ã®ææ¡ˆ ---
    st.header("Step 2: åˆ†ææ‰‹æ³•ã®ææ¡ˆ")
    st.markdown(f"ï¼ˆ(â˜…) AIææ¡ˆãƒ¢ãƒ‡ãƒ«: `{MODEL_FLASH_LITE}`ï¼‰")
    
    analysis_prompt_B = st.text_area(
        "ï¼ˆä»»æ„ï¼‰AIã«è¿½åŠ ã§æŒ‡ç¤ºã—ãŸã„åˆ†æã‚¿ã‚¹ã‚¯ã‚’å…¥åŠ›:",
        placeholder="ä¾‹: ã‚°ãƒ«ãƒ¡æŠ•ç¨¿ã¨è‡ªç„¶æŠ•ç¨¿ã®å‚¾å‘ã‚’æ¯”è¼ƒã—ãŸã„ã€‚",
        key="step_b_prompt"
    )

    if st.button("ğŸ’¡ åˆ†ææ‰‹æ³•ã‚’ææ¡ˆã•ã›ã‚‹ (Step 2)", key="suggest_button_B", type="primary"):
        st.session_state.suggestions_attempted_B = True # ææ¡ˆã‚’å®Ÿè¡Œã—ãŸãƒ•ãƒ©ã‚°
        
        if not st.session_state.tips_list:
            with st.spinner("åˆ†æTIPSã‚’AIã§ç”Ÿæˆä¸­..."):
                st.session_state.tips_list = get_analysis_tips_list_from_ai()
                st.session_state.current_tip_index = random.randint(0, len(st.session_state.tips_list) - 1) if st.session_state.tips_list else 0
                st.session_state.last_tip_time = time.time()
            
        with st.spinner(f"ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã¨æŒ‡ç¤ºå†…å®¹ã‚’åˆ†æã—ã€æ‰‹æ³•ã‚’ææ¡ˆä¸­ ({MODEL_FLASH_LITE})..."):
            st.session_state.step_b_results = {}
            st.session_state.step_b_json_output = None
            
            base_suggestions = suggest_analysis_techniques_py(df_B)
            ai_suggestions = []
            if analysis_prompt_B.strip():
                ai_suggestions = suggest_analysis_techniques_ai(
                    analysis_prompt_B, df_B, base_suggestions
                )
            base_names = {s['name'] for s in base_suggestions}
            filtered_ai_suggestions = [s for s in ai_suggestions if s['name'] not in base_names]
            all_suggestions = sorted(base_suggestions + filtered_ai_suggestions, key=lambda x: x['priority'])
            
            if not all_suggestions:
                st.session_state.suggestions_B = {}
                st.session_state.selected_tasks_B = set()
            else:
                # ææ¡ˆã‚’è¾æ›¸ã¨ã—ã¦ä¿å­˜
                st.session_state.suggestions_B = {s['name']: s for s in all_suggestions}
                # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã™ã¹ã¦é¸æŠçŠ¶æ…‹ã«ã™ã‚‹
                st.session_state.selected_tasks_B = set(st.session_state.suggestions_B.keys())
                st.success(f"åˆ†ææ‰‹æ³•ã®ææ¡ˆãŒå®Œäº†ã—ã¾ã—ãŸ ({len(all_suggestions)}ä»¶)ã€‚Step 3 ã§å®Ÿè¡Œã™ã‚‹ã‚¿ã‚¹ã‚¯ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
            
            st.rerun() # ææ¡ˆå¾Œã€UIã‚’å†æç”»ã—ã¦ Step 3 ã‚’è¡¨ç¤º

    # --- 3. (â˜…) å®Ÿè¡Œã‚¿ã‚¹ã‚¯ã®é¸æŠï¼ˆãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ï¼‰ ---
    if not st.session_state.suggestions_attempted_B:
        st.info("Step 2 ã§ã€Œåˆ†ææ‰‹æ³•ã‚’ææ¡ˆã•ã›ã‚‹ã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")
        return
    
    if st.session_state.suggestions_attempted_B and not st.session_state.suggestions_B:
        st.warning(
            "åˆ†ææ‰‹æ³•ã®ææ¡ˆãŒ 0ä»¶ ã§ã—ãŸã€‚\n"
            "ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸCSVãƒ•ã‚¡ã‚¤ãƒ«ã«ã€åˆ†æå¯èƒ½ãªåˆ—ï¼ˆ`...ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰`ã§çµ‚ã‚ã‚‹åˆ—ã‚„ã€`ANALYSIS_TEXT_COLUMN`ãªã©ï¼‰ãŒ"
            "æ­£ã—ãå«ã¾ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
        )
        return

    st.markdown("---")
    st.header("Step 3: å®Ÿè¡Œã™ã‚‹åˆ†æã‚¿ã‚¹ã‚¯ã®é¸æŠ")
    st.info("ä¸€æ‹¬å®Ÿè¡Œã—ãŸã„åˆ†æã‚¿ã‚¹ã‚¯ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")

    selected_tasks = set()
    
    def select_all_analyses():
        st.session_state.selected_tasks_B = set(st.session_state.suggestions_B.keys())
        
    def deselect_all_analyses():
        st.session_state.selected_tasks_B = set()

    col_select, col_deselect = st.columns(2)
    with col_select:
        st.button("ã™ã¹ã¦é¸æŠ", key="select_all_b", use_container_width=True, on_click=select_all_analyses)
    with col_deselect:
        st.button("ã™ã¹ã¦è§£é™¤", key="deselect_all_b", use_container_width=True, on_click=deselect_all_analyses)

    st.markdown("---")
    
    cols = st.columns(3)
    i = 0
    sorted_suggestions = sorted(
        st.session_state.suggestions_B.items(), 
        key=lambda item: item[1].get('priority', 99)
    )
    
    for task_name, details in sorted_suggestions:
        with cols[i % 3]:
            is_checked = st.checkbox(
                task_name,
                value=(task_name in st.session_state.selected_tasks_B),
                key=f"cb_{task_name}",
                help=details.get('description', '')
            )
            if is_checked:
                selected_tasks.add(task_name)
        i += 1
    
    st.session_state.selected_tasks_B = selected_tasks


    # --- 4. (â˜…) é¸æŠé …ç›®ã®ä¸€æ‹¬å®Ÿè¡Œ (Bug 1.3 UIãƒ•ãƒªãƒ¼ã‚ºå¯¾å¿œ) ---
    if st.button(f"ğŸƒ é¸æŠã—ãŸ {len(st.session_state.selected_tasks_B)} ä»¶ã®åˆ†æã‚’å®Ÿè¡Œ (Step 4)", type="primary", use_container_width=True):
        st.session_state.progress_text = "é¸æŠé …ç›®ã®å®Ÿè¡Œã‚’é–‹å§‹ã—ã¾ã™..."
        
        progress_text_placeholder_bulk = st.empty()
        progress_text_placeholder_bulk.info(st.session_state.progress_text)

        tasks_to_run = st.session_state.selected_tasks_B
        cleared_results_count = 0
        for task_name in list(st.session_state.step_b_results.keys()):
            if task_name not in tasks_to_run:
                del st.session_state.step_b_results[task_name]
                cleared_results_count += 1
        if cleared_results_count > 0:
            logger.info(f"é¸æŠè§£é™¤ã•ã‚ŒãŸ {cleared_results_count} ä»¶ã®å¤ã„çµæœã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸã€‚")
        
        total_tasks = len(tasks_to_run)
        if total_tasks == 0:
            st.warning("å®Ÿè¡Œã™ã‚‹ã‚¿ã‚¹ã‚¯ãŒé¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
            progress_text_placeholder_bulk.empty()
            st.rerun()

        progress_bar = st.progress(0.0, text="ä¸€æ‹¬å®Ÿè¡Œ å¾…æ©Ÿä¸­...")
        tip_placeholder_b_bulk = st.empty()
        
        with st.spinner(f"å…¨ {total_tasks} ä»¶ã®åˆ†æã‚’å®Ÿè¡Œä¸­..."):
            i = 0
            for task_name in tasks_to_run:
                if task_name not in st.session_state.suggestions_B:
                    continue
                
                suggestion_details = st.session_state.suggestions_B[task_name]
                i += 1
                st.session_state.progress_text = f"({i}/{total_tasks}) ã€Œ{task_name}ã€ã‚’å®Ÿè¡Œä¸­..."
                
                progress_bar.progress(i / total_tasks, text=f"å®Ÿè¡Œä¸­: {task_name}")
                progress_text_placeholder_bulk.info(st.session_state.progress_text)
                
                try:
                    result_data = execute_analysis(task_name, df_B, suggestion_details)
                    st.session_state.step_b_results[task_name] = result_data
                except Exception as e:
                    logger.error(f"ä¸€æ‹¬å®Ÿè¡Œã‚¨ãƒ©ãƒ¼ ({task_name}): {e}", exc_info=True)
                    st.session_state.step_b_results[task_name] = {
                        "data": f"ä¸€æ‹¬å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}",
                        "image_base64": None,
                        "summary": f"ã‚¨ãƒ©ãƒ¼: {e}"
                    }
            
            st.session_state.progress_text = "å…¨åˆ†æã®å®Ÿè¡ŒãŒå®Œäº†ã—ã¾ã—ãŸã€‚"
            progress_bar.progress(1.0, text="å®Ÿè¡Œ å®Œäº†")
            tip_placeholder_b_bulk.empty()
            progress_text_placeholder_bulk.empty() # å®Œäº†ã—ãŸã‚‰æ¶ˆã™
            st.success("é¸æŠã•ã‚ŒãŸåˆ†æã®å®Ÿè¡ŒãŒå®Œäº†ã—ã¾ã—ãŸã€‚Step 5 ã§çµæœã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            st.rerun()


    # --- 5. (â˜…) åˆ†æã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä¿®æ­£ (æ±ç”¨åŒ–) ---
    st.markdown("---")
    st.header("Step 5: åˆ†æã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä¿®æ­£")
    
    if not st.session_state.step_b_results:
        st.info("Step 4 ã§åˆ†æã‚’å®Ÿè¡Œã™ã‚‹ã¨ã€ã“ã“ã«ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚")
        return
    
    st.info("å„åˆ†æé …ç›®ã®ã€Œâ–¼ã€ã‚’é–‹ãã€ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä¿®æ­£ã—ã¦ã€å€‹åˆ¥ã«ã€Œå†å®Ÿè¡Œ/æ›´æ–°ã€ã‚‚å¯èƒ½ã§ã™ã€‚")
    
    tip_placeholder = st.empty()
    if st.session_state.tips_list:
        try:
            current_tip = st.session_state.tips_list[st.session_state.current_tip_index]
            tip_placeholder.info(f"ğŸ’¡ ãƒ‡ãƒ¼ã‚¿åˆ†æTIPS: {current_tip}")
        except IndexError:
            st.session_state.current_tip_index = 0

    progress_text_placeholder = st.empty()
    if st.session_state.progress_text:
         progress_text_placeholder.info(st.session_state.progress_text)
         
    sorted_executed_tasks = sorted(
        st.session_state.step_b_results.keys(),
        key=lambda task_name: st.session_state.suggestions_B.get(task_name, {}).get('priority', 99)
    )

    # æ±ç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿UIã®ãŸã‚ã®ãƒ˜ãƒ«ãƒ‘ãƒ¼
    def get_generic_param(task_name, param_key, default_value):
        return st.session_state.step_b_generic_params.get(task_name, {}).get(param_key, default_value)

    def set_generic_param(task_name, param_key, value):
        if task_name not in st.session_state.step_b_generic_params:
            st.session_state.step_b_generic_params[task_name] = {}
        st.session_state.step_b_generic_params[task_name][param_key] = value

    for task_name in sorted_executed_tasks:
        if task_name not in st.session_state.suggestions_B:
            continue
            
        suggestion_details = st.session_state.suggestions_B[task_name].copy()
        result = st.session_state.step_b_results[task_name]
        
        st.markdown("---")
        
        # ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼è¡¨ç¤º
        st.subheader(f"âœ… ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼: {task_name}")
        
        if task_name == "ã‚«ãƒ†ã‚´ãƒªåˆ¥ æ•°å€¤åˆ—TOP5åˆ†æ" and isinstance(result.get('data'), pd.DataFrame):
            for _, row in result['data'].iterrows():
                st.markdown(f"**{row['category']}** (æŠ•ç¨¿æ•°: {row['post_count']})")
                if row['top_posts']:
                     for post in row['top_posts']:
                         st.markdown(f"  - **EG: {post['engagement']}** - {post['summary_ai']}")
                         if post.get('media_link'):
                             st.markdown(f"    [Link]({post['media_link']})")
                st.markdown("---")
        elif task_name == "A/B æ¯”è¼ƒåˆ†æ" and isinstance(result.get('data'), dict):
            if "category_comparison" in result["data"]:
                st.markdown("##### ã‚«ãƒ†ã‚´ãƒªåˆ¥ æŠ•ç¨¿æ•°æ¯”è¼ƒ")
                st.dataframe(pd.DataFrame(result["data"]["category_comparison"]))
            if "ranking_comparison" in result["data"]:
                st.markdown("##### åœ°åŸŸåˆ¥ é †ä½å¤‰å‹• (Top 20)")
                st.dataframe(pd.DataFrame(result["data"]["ranking_comparison"]))
        
        # å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ (pyvis HTML)
        elif task_name == "å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯" and result.get("html_content"):
            components.html(result.get("html_content"), height=710)
            ai_legend_map = result.get("ai_legend")
            communities_map = result.get("communities")
            
            if ai_legend_map:
                st.markdown("##### AIã«ã‚ˆã‚‹æ¨å®šãƒˆãƒ”ãƒƒã‚¯:")
                legend_items = []
                for group_id, topic in ai_legend_map.items():
                    color = COLOR_PALETTE[group_id % len(COLOR_PALETTE)]
                    legend_html = f"""
                    <span style="display: inline-block; margin: 4px; padding: 8px 12px; border-radius: 8px; background-color: #f0f2f6; border: 1px solid #e0e0e0;">
                        <span style='color:{color}; font-size: 20px; font-weight: bold; vertical-align: middle;'>â– </span>
                        <span style="vertical-align: middle; margin-left: 8px; font-size: 14px;">{topic} (G{group_id})</span>
                    </span>
                    """
                    legend_items.append(legend_html.replace("\n", ""))
                st.markdown("<div style='line-height: 1.8;'>" + " ".join(legend_items) + "</div>", unsafe_allow_html=True)
            
            elif communities_map:
                st.markdown("##### æ¤œå‡ºã•ã‚ŒãŸã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ (è‰²åˆ†ã‘):")
                legend_items = []
                for group_id in communities_map.keys():
                    color = COLOR_PALETTE[group_id % len(COLOR_PALETTE)]
                    legend_html = f"""
                    <span style="display: inline-block; margin: 4px; padding: 8px 12px; border-radius: 8px; background-color: #f0f2f6; border: 1px solid #e0e0e0;">
                        <span style='color:{color}; font-size: 20px; font-weight: bold; vertical-align: middle;'>â– </span>
                        <span style="vertical-align: middle; margin-left: 8px; font-size: 14px;">ã‚°ãƒ«ãƒ¼ãƒ— {group_id}</span>
                    </span>
                    """
                    legend_items.append(legend_html.replace("\n", ""))
                st.markdown("<div style='line-height: 1.8;'>" + " ".join(legend_items) + "</div>", unsafe_allow_html=True)

        # ãã®ä»–ã®åˆ†æ
        else:
            if result.get('image_base64'):
                st.image(base64.b64decode(result['image_base64']))
            
            if isinstance(result.get('data'), pd.DataFrame):
                st.dataframe(result['data'].head(10))
            elif isinstance(result.get('data'), dict):
                st.json(result['data'])
            elif isinstance(result.get('data'), str):
                st.markdown(result['data'])
            
        st.caption(f"ã‚µãƒãƒª: {result.get('summary', 'N/A')}")

        # (â˜…) å€‹åˆ¥å®Ÿè¡Œã‚¨ãƒªã‚¢ (æ±ç”¨åŒ–å¯¾å¿œ)
        with st.expander(f"ã€Œ{task_name}ã€ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä¿®æ­£ãƒ»å†å®Ÿè¡Œ"):
            
            st.markdown(f"**èª¬æ˜:** {suggestion_details.get('description', 'N/A')}")
            st.markdown("##### (ã‚ªãƒ—ã‚·ãƒ§ãƒ³) ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å¤‰æ›´")
            
            try:
                # 1. å˜ç´”é›†è¨ˆ
                if task_name.startswith("å˜ç´”é›†è¨ˆ:"):
                    default_col = suggestion_details['suitable_cols'][0]
                    new_col = st.selectbox(f"é›†è¨ˆå¯¾è±¡ã®åˆ— ({task_name})", options=all_categorical_cols, index=all_categorical_cols.index(default_col) if default_col in all_categorical_cols else 0, key=f"sel_{task_name}")
                    suggestion_details['ui_selected_col'] = new_col
                
                # 2. ã‚¯ãƒ­ã‚¹é›†è¨ˆ
                elif task_name.startswith("ã‚¯ãƒ­ã‚¹é›†è¨ˆ"):
                    default_col1 = suggestion_details['suitable_cols'][0]
                    default_col2 = suggestion_details['suitable_cols'][1]
                    c1, c2 = st.columns(2)
                    new_col1 = c1.selectbox(f"åˆ— 1 (è¡Œ) ({task_name})", options=all_categorical_cols, index=all_categorical_cols.index(default_col1) if default_col1 in all_categorical_cols else 0, key=f"sel_{task_name}_1")
                    new_col2 = c2.selectbox(f"åˆ— 2 (åˆ—) ({task_name})", options=all_categorical_cols, index=all_categorical_cols.index(default_col2) if default_col2 in all_categorical_cols else 1, key=f"sel_{task_name}_2")
                    suggestion_details['ui_selected_col1'] = new_col1
                    suggestion_details['ui_selected_col2'] = new_col2

                # 3. æ™‚ç³»åˆ—
                elif task_name == "æ™‚ç³»åˆ—ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æ":
                    default_dt = suggestion_details['suitable_cols']['datetime'][0]
                    default_kw = suggestion_details['suitable_cols']['keywords'][0]
                    c1, c2 = st.columns(2)
                    new_dt = c1.selectbox(f"æ—¥æ™‚åˆ— ({task_name})", options=date_cols, index=date_cols.index(default_dt) if default_dt in date_cols else 0, key=f"sel_{task_name}_dt")
                    new_kw = c2.selectbox(f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ— ({task_name})", options=all_categorical_cols, index=all_categorical_cols.index(default_kw) if default_kw in all_categorical_cols else 0, key=f"sel_{task_name}_kw")
                    suggestion_details['ui_selected_dt_col'] = new_dt
                    suggestion_details['ui_selected_kw_col'] = new_kw
                
                # 4. ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°
                elif task_name == "ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°ï¼ˆé »å‡ºå˜èªï¼‰":
                    default_col = suggestion_details['suitable_cols'][0]
                    if not text_cols:
                         st.warning("åˆ†æå¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆåˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
                         new_col = None
                    else:
                        new_col = st.selectbox(f"ãƒ†ã‚­ã‚¹ãƒˆåˆ— ({task_name})", options=text_cols, index=text_cols.index(default_col) if default_col in text_cols else 0, key=f"sel_{task_name}_txt")
                    suggestion_details['ui_selected_text_col'] = new_col

                    custom_sw = st.text_area(
                        "é™¤å¤–ã—ãŸã„å˜èªï¼ˆã‚«ãƒ³ãƒ, ã‚¹ãƒšãƒ¼ã‚¹, æ”¹è¡ŒåŒºåˆ‡ã‚Šï¼‰:",
                        value=suggestion_details.get('ui_custom_stop_words', ''),
                        key=f"sw_{task_name}",
                        height=100,
                        placeholder="ä¾‹: å¼Šç¤¾, å•†å“A, ã‚µãƒ¼ãƒ“ã‚¹B, ..."
                    )
                    suggestion_details['ui_custom_stop_words'] = custom_sw

                # 5. å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
                elif task_name == "å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯":
                    # 1. çµã‚Šè¾¼ã¿åˆ—
                    flag_col_options = all_categorical_cols
                    default_flag_col = suggestion_details.get('ui_selected_flag_col', location_cols[0] if location_cols else (flag_cols[0] if flag_cols else None))
                    
                    flag_col = st.selectbox(
                        "1. çµã‚Šè¾¼ã¿ã«ä½¿ç”¨ã™ã‚‹ã‚«ãƒ†ã‚´ãƒªåˆ—:", flag_col_options,
                        index=flag_col_options.index(default_flag_col) if default_flag_col in flag_col_options else 0,
                        key=f"cn_filter_col_{task_name}",
                        help="ã“ã“ã§é¸ã‚“ã åˆ—ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ã€åˆ†æå¯¾è±¡ã®æŠ•ç¨¿ã‚’çµã‚Šè¾¼ã¿ã¾ã™ã€‚"
                    )
                    suggestion_details['ui_selected_flag_col'] = flag_col

                    # 2. çµã‚Šè¾¼ã¿ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
                    try:
                        s = df_B[flag_col].dropna().astype(str).str.split(',').explode().str.strip()
                        s = s[~s.isin(['', 'nan', 'Nan', 'NaN'])]
                        keyword_counts = s.value_counts()
                        options = keyword_counts.index.tolist()[:50]
                        default_options_kws = suggestion_details.get('ui_selected_keywords', keyword_counts.index.tolist()[:10])
                    except Exception:
                        options = []
                        default_options_kws = suggestion_details.get('ui_selected_keywords', [])

                    selected_keywords = st.multiselect(
                        f"2. çµã‚Šè¾¼ã‚€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆã€Œ{flag_col}ã€åˆ— Top 50ï¼‰:",
                        options,
                        default=default_options_kws,
                        key=f"cn_selected_keywords_{flag_col}",
                        help="åˆ†æå¯¾è±¡ã¨ã™ã‚‹æŠ•ç¨¿ã«å«ã¾ã‚Œã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’é¸æŠã—ã¾ã™ã€‚"
                    )
                    suggestion_details['ui_selected_keywords'] = selected_keywords
                    
                    # 3. ãƒ†ã‚­ã‚¹ãƒˆåˆ—
                    default_text_col = suggestion_details.get('ui_selected_text_col', text_cols[0] if text_cols else None)
                    if not text_cols:
                         st.warning("åˆ†æå¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆåˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
                         text_col = None
                    else:
                        text_col = st.selectbox(
                            "3. åˆ†æå¯¾è±¡ã®è‡ªç”±è¨˜è¿°åˆ—:", text_cols,
                            index=text_cols.index(default_text_col) if default_text_col in text_cols else 0,
                            key=f"cn_text_col_{task_name}"
                        )
                    suggestion_details['ui_selected_text_col'] = text_col
                    
                    st.markdown("---")
                    st.markdown("**ãƒ†ã‚­ã‚¹ãƒˆåˆ†æ è¨­å®š**")
                    custom_sw_cn = st.text_area(
                        "é™¤å¤–ã—ãŸã„å˜èªï¼ˆã‚«ãƒ³ãƒ, ã‚¹ãƒšãƒ¼ã‚¹, æ”¹è¡ŒåŒºåˆ‡ã‚Šï¼‰:",
                        value=suggestion_details.get('ui_custom_stop_words', ''),
                        key=f"sw_{task_name}",
                        height=100,
                        placeholder="ä¾‹: å¼Šç¤¾, å•†å“A, ã‚µãƒ¼ãƒ“ã‚¹B, ..."
                    )
                    suggestion_details['ui_custom_stop_words'] = custom_sw_cn
                    
                    st.markdown("---")
                    st.markdown("**ã‚°ãƒ©ãƒ•è©³ç´°è¨­å®š**")
                    ui_cols = st.columns([0.5, 0.5])
                    
                    with ui_cols[0]:
                        # 4. ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ
                        st.markdown("**ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆãƒ»ç‰©ç†æ¼”ç®—**")
                        solver = st.selectbox(
                            "ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ (layout)", ['barnesHut', 'fruchterman_reingold', 'repulsion'],
                            index=['barnesHut', 'fruchterman_reingold', 'repulsion'].index(suggestion_details.get('solver', 'barnesHut')),
                            key=f"cn_solver_{task_name}"
                        )
                        suggestion_details['solver'] = solver
                        gravity = st.slider(
                            "é‡åŠ› (Gravity)", -50000, -1000, suggestion_details.get('gravity', -2000), step=1000,
                            key=f"cn_gravity_{task_name}"
                        )
                        suggestion_details['gravity'] = gravity
                        node_distance = st.slider(
                            "ãƒãƒ¼ãƒ‰é–“ã®åç™ºåŠ›", 100, 500, suggestion_details.get('node_distance', 200),
                            key=f"cn_distance_{task_name}"
                        )
                        suggestion_details['node_distance'] = node_distance
                        spring_length = st.slider(
                            "ã‚¨ãƒƒã‚¸ã®é•·ã•", 50, 500, suggestion_details.get('spring_length', 250),
                            key=f"cn_spring_{task_name}"
                        )
                        suggestion_details['spring_length'] = spring_length

                    with ui_cols[1]:
                        # 5. ãƒ•ã‚£ãƒ«ã‚¿
                        st.markdown("**ãƒ•ã‚£ãƒ«ã‚¿è¨­å®š**")
                        top_n_words_limit = st.slider(
                            "åˆ†æå¯¾è±¡ã®å˜èªæ•° (Top N)", 50, 300, suggestion_details.get('top_n_words_limit', 100),
                            key=f"cn_top_n_{task_name}"
                        )
                        suggestion_details['top_n_words_limit'] = top_n_words_limit
                        max_degree_cutoff = st.slider(
                            "æœ€å¤§æ¥ç¶šæ•° (Exclude Hubs)", 10, 100, suggestion_details.get('max_degree_cutoff', 50),
                            key=f"cn_max_degree_{task_name}"
                        )
                        suggestion_details['max_degree_cutoff'] = max_degree_cutoff
                        min_occurrence = st.slider(
                            "æœ€å°å…±èµ·å›æ•° (Min Freq)", 1, 30, suggestion_details.get('min_occurrence', 10),
                            key=f"cn_slider_v3_{task_name}"
                        )
                        suggestion_details['min_occurrence'] = min_occurrence
                        
                        # 6. ãƒ‡ã‚¶ã‚¤ãƒ³
                        st.markdown("**ãƒ‡ã‚¶ã‚¤ãƒ³è¨­å®š**")
                        default_node_size = st.slider(
                            "åŸºæº–ãƒãƒ¼ãƒ‰ã‚µã‚¤ã‚º", 5, 50, suggestion_details.get('default_node_size', 15),
                            key=f"cn_node_size_v2_{task_name}"
                        )
                        suggestion_details['default_node_size'] = default_node_size
                        default_text_size = st.slider(
                            "ãƒ†ã‚­ã‚¹ãƒˆã‚µã‚¤ã‚º", 10, 100, suggestion_details.get('default_text_size', 50),
                            key=f"cn_text_size_v2_{task_name}"
                        )
                        suggestion_details['default_text_size'] = default_text_size
                    
                    # 7. AIå‡¡ä¾‹
                    st.markdown("---")
                    run_ai_legend = st.checkbox(
                        "ğŸ¤– AIã§å‡¡ä¾‹ã‚’ç”Ÿæˆ (Î²) (å®Ÿè¡Œã«æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™)",
                        value=suggestion_details.get('run_ai_legend', False),
                        key=f"cn_run_ai_legend_{task_name}"
                    )
                    suggestion_details['run_ai_legend'] = run_ai_legend

                # 6. æ±ç”¨ã‚«ãƒ†ã‚´ãƒªæ·±æ˜ã‚Š
                elif task_name == "ã‚«ãƒ†ã‚´ãƒªåˆ—ã®é›†è¨ˆã¨æ·±æ˜ã‚Š":
                    defaults = suggestion_details['suitable_cols']
                    cat_options = defaults['category_cols']
                    if not cat_options:
                         st.warning("åˆ†æå¯èƒ½ãªã‚«ãƒ†ã‚´ãƒªåˆ—ï¼ˆ...ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ ç­‰ï¼‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
                         new_cat_col = None
                    else:
                        default_cat_col = get_generic_param(task_name, 'cat_col', cat_options[0])
                        new_cat_col = st.selectbox(
                            f"é›†è¨ˆå¯¾è±¡ã®ã‚«ãƒ†ã‚´ãƒªåˆ— ({task_name})", options=cat_options, 
                            index=cat_options.index(default_cat_col) if default_cat_col in cat_options else 0, 
                            key=f"sel_{task_name}_cat"
                        )
                    suggestion_details['ui_selected_category_col'] = new_cat_col
                    set_generic_param(task_name, 'cat_col', new_cat_col)

                # 7. æ±ç”¨ æ•°å€¤åˆ—TOP5
                elif task_name == "ã‚«ãƒ†ã‚´ãƒªåˆ¥ æ•°å€¤åˆ—TOP5åˆ†æ":
                    defaults = suggestion_details['suitable_cols']
                    
                    cat_options = defaults['category_cols']
                    num_options = defaults['numeric_cols']
                    
                    if not cat_options or not num_options:
                        st.warning("åˆ†æã«å¿…è¦ãªã‚«ãƒ†ã‚´ãƒªåˆ—ã¾ãŸã¯æ•°å€¤åˆ—ï¼ˆã„ã„ã­ ç­‰ï¼‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
                    else:
                        c1, c2 = st.columns(2)
                        default_cat_col = get_generic_param(task_name, 'cat_col', cat_options[0])
                        default_num_col = get_generic_param(task_name, 'num_col', num_options[0])

                        new_cat_col = c1.selectbox(
                            f"ã‚«ãƒ†ã‚´ãƒªåˆ— ({task_name})", options=cat_options, 
                            index=cat_options.index(default_cat_col) if default_cat_col in cat_options else 0, 
                            key=f"sel_{task_name}_cat_top5"
                        )
                        new_num_col = c2.selectbox(
                            f"æ•°å€¤åˆ—ï¼ˆé›†è¨ˆå¯¾è±¡ï¼‰ ({task_name})", options=num_options, 
                            index=num_options.index(default_num_col) if default_num_col in num_options else 0, 
                            key=f"sel_{task_name}_num_top5"
                        )
                        
                        suggestion_details['ui_selected_category_col'] = new_cat_col
                        suggestion_details['ui_selected_numeric_col'] = new_num_col
                        suggestion_details['ui_selected_text_col'] = defaults['text_col'][0] # ãƒ†ã‚­ã‚¹ãƒˆåˆ—ã¯å›ºå®š
                        
                        set_generic_param(task_name, 'cat_col', new_cat_col)
                        set_generic_param(task_name, 'num_col', new_num_col)

                # 8. A/Bæ¯”è¼ƒã®UI
                elif task_name == "A/B æ¯”è¼ƒåˆ†æ":
                    st.info("æ¯”è¼ƒã—ãŸã„2ã¤ã®ã‚°ãƒ«ãƒ¼ãƒ—ï¼ˆAã¨Bï¼‰ã‚’å®šç¾©ã—ã¦ãã ã•ã„ã€‚")
                    ab_col_options = suggestion_details['suitable_cols']['category_cols']
                    
                    ab_params = st.session_state.step_b_ab_params
                    
                    c1, c2 = st.columns(2)
                    with c1:
                        st.markdown("##### ã‚°ãƒ«ãƒ¼ãƒ— A")
                        a_col_key = f"ab_a_col_{task_name}"
                        a_val_key = f"ab_a_val_{task_name}"
                        
                        default_a_col = ab_params.get('a_col', ab_col_options[0] if ab_col_options else None)
                        a_col = st.selectbox(
                            "A: æ¯”è¼ƒåˆ—", ab_col_options, 
                            index=ab_col_options.index(default_a_col) if default_a_col in ab_col_options else 0, 
                            key=a_col_key
                        )
                        try:
                            a_val_options = sorted(list(df_B[a_col].astype(str).str.split(', ').explode().str.strip().unique()))
                        except Exception:
                            a_val_options = []
                        
                        default_a_val = ab_params.get('a_val', a_val_options[0] if a_val_options else None)
                        a_val = st.selectbox(
                            "A: æ¯”è¼ƒå€¤", a_val_options, 
                            index=a_val_options.index(default_a_val) if default_a_val in a_val_options else 0, 
                            key=a_val_key
                        )
                    with c2:
                        st.markdown("##### ã‚°ãƒ«ãƒ¼ãƒ— B")
                        b_col_key = f"ab_b_col_{task_name}"
                        b_val_key = f"ab_b_val_{task_name}"
                        
                        default_b_col = ab_params.get('b_col', ab_col_options[0] if ab_col_options else None)
                        b_col = st.selectbox(
                            "B: æ¯”è¼ƒåˆ—", ab_col_options, 
                            index=ab_col_options.index(default_b_col) if default_b_col in ab_col_options else 0, 
                            key=b_col_key
                        )
                        try:
                            b_val_options = sorted(list(df_B[b_col].astype(str).str.split(', ').explode().str.strip().unique()))
                        except Exception:
                            b_val_options = []
                            
                        default_b_val = ab_params.get('b_val', b_val_options[1] if len(b_val_options) > 1 else (b_val_options[0] if b_val_options else None))
                        b_val = st.selectbox(
                            "B: æ¯”è¼ƒå€¤", b_val_options, 
                            index=b_val_options.index(default_b_val) if default_b_val in b_val_options else 0, 
                            key=b_val_key
                        )
                    
                    current_ab_params = {'a_col': a_col, 'a_val': a_val, 'b_col': b_col, 'b_val': b_val}
                    suggestion_details['ui_ab_params'] = current_ab_params
                    st.session_state.step_b_ab_params = current_ab_params
                
                # 9. (AIã‚¿ã‚¹ã‚¯)
                elif suggestion_details.get('type') == 'ai':
                    st.info("ã“ã®ã‚¿ã‚¹ã‚¯ã¯AIã«ã‚ˆã£ã¦å®Ÿè¡Œã•ã‚Œã¾ã™ã€‚AIã¸ã®æŒ‡ç¤ºï¼ˆèª¬æ˜ï¼‰ã‚’å¤‰æ›´ã§ãã¾ã™ã€‚")
                    new_desc = st.text_area(
                        "AIã¸ã®æŒ‡ç¤º (description):",
                        value=suggestion_details.get('description', ''),
                        key=f"ai_desc_{task_name}",
                        height=100
                    )
                    suggestion_details['description'] = new_desc

            except Exception as e:
                st.error(f"ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿UIã®æç”»ã«å¤±æ•—: {e}")
                logger.error(f"ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿UIæç”»ã‚¨ãƒ©ãƒ¼ ({task_name}): {e}", exc_info=True)

            # å€‹åˆ¥å®Ÿè¡Œãƒœã‚¿ãƒ³
            if st.button(f"ã€Œ{task_name}ã€ã‚’å†å®Ÿè¡Œ/æ›´æ–°", key=f"run_{task_name}"):
                st.session_state.progress_text = f"ã€Œ{task_name}ã€ã‚’å€‹åˆ¥ã«å®Ÿè¡Œä¸­..."
                with st.spinner(f"ã€Œ{task_name}ã€ã‚’å®Ÿè¡Œä¸­..."):
                    try:
                        result_data = execute_analysis(task_name, df_B, suggestion_details)
                        st.session_state.step_b_results[task_name] = result_data # çµæœã‚’æ›´æ–°
                        st.session_state.suggestions_B[task_name] = suggestion_details # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä¿å­˜
                        st.session_state.progress_text = f"ã€Œ{task_name}ã€ã®å®Ÿè¡ŒãŒå®Œäº†ã—ã¾ã—ãŸã€‚"
                        st.rerun() # UIã‚’æ›´æ–°ã—ã¦ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã«åæ˜ 
                    except Exception as e:
                         st.error(f"åˆ†æå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
                         logger.error(f"å€‹åˆ¥å®Ÿè¡Œã‚¨ãƒ©ãƒ¼ ({task_name}): {e}", exc_info=True)
                         st.session_state.progress_text = f"ã€Œ{task_name}ã€ã®å®Ÿè¡Œã«å¤±æ•—ã—ã¾ã—ãŸã€‚"


    # --- 6. (â˜…) æœ€çµ‚ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ ---
    st.markdown("---")
    st.header("Step 6: æœ€çµ‚JSONã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ")
    
    total_results = len(st.session_state.step_b_results)
    
    if total_results == 0:
        st.warning("Step 4 ã§åˆ†æã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
    else:
        st.success(f"ç¾åœ¨ {total_results} ä»¶ã®åˆ†æçµæœãŒãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã•ã‚Œã¦ã„ã¾ã™ã€‚")

    
    if st.button("StepCç”¨ JSONã‚’ç”Ÿæˆãƒ»ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ (Step 6)", type="primary", use_container_width=True):
        if total_results == 0:
            st.error("åˆ†æãŒ1ã¤ã‚‚å®Ÿè¡Œã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚Step 4 ã§åˆ†æã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        else:
            with st.spinner("æœ€çµ‚JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆä¸­..."):
                try:
                    json_output_string = convert_results_to_json_string(st.session_state.step_b_results)
                    st.session_state.step_b_json_output = json_output_string
                    st.success("StepCç”¨ã®JSONãƒ‡ãƒ¼ã‚¿ãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸï¼")
                except Exception as e:
                    logger.error(f"Step B æœ€çµ‚JSONå‡ºåŠ›å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
                    st.error(f"åˆ†æçµæœã®JSONå¤‰æ›ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚»ã‚¯ã‚·ãƒ§ãƒ³
    if st.session_state.step_b_json_output:
        st.info(f"ä»¥ä¸‹ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ã¯ã€Step 5 ã§ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ»å®Ÿè¡Œã•ã‚ŒãŸ {len(st.session_state.step_b_results)} ä»¶ã®åˆ†æçµæœãŒã™ã¹ã¦å«ã¾ã‚Œã¦ã„ã¾ã™ã€‚")
        
        st.download_button(
            label="åˆ†æãƒ‡ãƒ¼ã‚¿ (analysis_data.json) ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            data=st.session_state.step_b_json_output,
            file_name="analysis_data.json",
            mime="application/json",
            type="primary",
            use_container_width=True
        )
        
        st.markdown("---")
        st.subheader("å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿ (JSONL) ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")
        
        preview_summaries = []
        try:
            for line in st.session_state.step_b_json_output.splitlines():
                line_data = json.loads(line)
                task_name = line_data.get("analysis_task")
                summary = line_data.get("summary", line_data.get("analysis_summaries", "No summary."))
                img_note = line_data.get("image_note", "No image.")
                
                if task_name == "OverallSummary":
                    preview_summaries.append(f"--- Overall Summary ---")
                    if isinstance(summary, dict):
                        for k, v in summary.items():
                             preview_summaries.append(f"  - {k}: {str(v)[:100]}...")
                    continue
                
                preview_summaries.append(f"[{task_name}] (Image: {img_note})")
                
        except Exception as e:
            preview_summaries = ["JSONãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®ç”Ÿæˆã«å¤±æ•—", str(e)]

        st.text_area(
            "JSONL (ã‚µãƒãƒªãƒ¼ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼):",
            value="\n".join(preview_summaries),
            height=300,
            key="json_preview_B_summary",
            disabled=True
        )
        st.success("ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€Step C (AIãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ) ã«é€²ã‚“ã§ãã ã•ã„ã€‚")

# --- 9. (â˜…) Step C: AIãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ (Proãƒ¢ãƒ‡ãƒ«) ---
# (è¦ä»¶: Step Cã¯ gemini-2.5-pro ã‚’ä½¿ç”¨)

def run_step_c_analysis(
    jsonl_data_string: str,
    model_name: str,
    progress_bar: st.delta_generator.DeltaGenerator,
    log_placeholder: st.delta_generator.DeltaGenerator,
    custom_instruction: str = "" # (â˜…) æ”¹å–„ C-4: UIã‹ã‚‰ã‚«ã‚¹ã‚¿ãƒ æŒ‡ç¤ºã‚’å—ã‘å–ã‚‹
) -> str:
    """
    (â˜…) Step C: AIãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ (ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ»RateLimitå¯¾å¿œç‰ˆ)
    
    [æ–°ãƒ­ã‚¸ãƒƒã‚¯] ãƒãƒ³ã‚°ã‚¢ãƒƒãƒ— (504 Timeout) ã‚’å›é¿ã™ã‚‹ãŸã‚ã€AIã«Base64ç”»åƒ(å·¨å¤§ãƒˆãƒ¼ã‚¯ãƒ³)ã‚’
    æ¸¡ã™ã®ã‚’ *ã‚„ã‚* ã€ã€Œãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã€ã®ã¿ã‚’æ¸¡ã—ã¦è€ƒå¯Ÿã‚’ç”Ÿæˆã•ã›ã‚‹ã€‚
    Pythonå´ã§ã€AIã®è€ƒå¯Ÿ(ãƒ†ã‚­ã‚¹ãƒˆ)ã¨ã€å…ƒã®Base64ç”»åƒã‚’ã€Œå†çµåˆã€ã™ã‚‹ã€‚
    """
    logger.info(f"Step C AIãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ (ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å‡¦ç†) é–‹å§‹... (Model: {model_name})")

    # (â˜…) --- 1. ãƒ¢ãƒ‡ãƒ«ã®RPMåˆ¶é™ã¨ã‚¹ãƒªãƒ¼ãƒ—æ™‚é–“ã‚’å®šç¾© ---
    if model_name == MODEL_PRO:
        rpm_limit = 2
        tpm_limit = 125000
    else: # (â˜…) ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ Flash
        model_name = MODEL_FLASH
        rpm_limit = 10
        tpm_limit = 250000
        
    sleep_time = (60 / rpm_limit) + 0.5 # (e.g., Pro: 30.5s, Flash: 6.5s)
    
    logger.info(f"ãƒ¢ãƒ‡ãƒ«: {model_name}, RPM: {rpm_limit}, å¾…æ©Ÿ: {sleep_time:.1f}ç§’")

    # (â˜…) --- 2. ãƒãƒ£ãƒ³ã‚¯ç”Ÿæˆç”¨ã®AIãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’å®šç¾© (å“è³ªå‘ä¸Š) ---
    
    # (â˜…) [æ”¹å–„ C-4] ã‚«ã‚¹ã‚¿ãƒ æŒ‡ç¤ºãŒç©ºã§ãªã„å ´åˆã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«æŒ¿å…¥ã™ã‚‹ãƒ–ãƒ­ãƒƒã‚¯ã‚’å®šç¾©
    custom_instruction_block = ""
    if custom_instruction and custom_instruction.strip():
        custom_instruction_block = f"""
        # (é‡è¦) ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®è¿½åŠ æŒ‡ç¤º:
        * {custom_instruction.strip()}
        * ã“ã®æŒ‡ç¤ºã‚’æœ€å„ªå…ˆã§è€ƒæ…®ã—ã¦ãã ã•ã„ã€‚
        """

    # (â˜…) [æ”¹å–„ C-1, C-2]
    ITERATIVE_SLIDE_PROMPT_TEMPLATE = """
    ã‚ãªãŸã¯ã‚·ãƒ‹ã‚¢ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒŠãƒªã‚¹ãƒˆã§ã‚ã‚Šã€ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå‘ã‘ãƒ¬ãƒãƒ¼ãƒˆã®ã€Œã‚¹ãƒ©ã‚¤ãƒ‰1æšã€ã®
    ã€ãƒ†ã‚­ã‚¹ãƒˆéƒ¨åˆ†ã€‘ã‚’ä½œæˆã—ã¦ã„ã¾ã™ã€‚
    æä¾›ã•ã‚Œã‚‹ã€Œåˆ†æã‚¿ã‚¹ã‚¯ãƒ‡ãƒ¼ã‚¿ã€ã‚’èª­ã¿ã€ã“ã®ã‚¿ã‚¹ã‚¯å°‚ç”¨ã®
    ã‚¹ãƒ©ã‚¤ãƒ‰ã‚¿ã‚¤ãƒˆãƒ«ã¨è€ƒå¯Ÿï¼ˆslide_contentï¼‰ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

    # åˆ†æã‚¿ã‚¹ã‚¯ãƒ‡ãƒ¼ã‚¿ (ãƒ†ã‚­ã‚¹ãƒˆãƒ»æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã®ã¿):
    {task_data_text_only}
    
    # (â˜…) [æ”¹å–„ C-2] ç”»åƒã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ:
    {image_context}
    
    # (â˜…) [æ”¹å–„ C-4] ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¨ä½“æ–¹é‡:
    {custom_instruction}

    # æŒ‡ç¤º:
    1.  **ã‚¿ã‚¤ãƒˆãƒ«**: `task_data_text_only` ã® `analysis_task` åã«åŸºã¥ãã€ professional ãªã€Œslide_titleã€ã‚’è€ƒæ¡ˆã—ã¦ãã ã•ã„ã€‚
    
    2.  **(â˜…) [æ”¹å–„ C-1] è€ƒå¯Ÿ (æœ€é‡è¦)**: 
        `task_data_text_only` ã® `summary` ã¨ `data`ï¼ˆãƒ†ãƒ¼ãƒ–ãƒ«ãƒ‡ãƒ¼ã‚¿ï¼‰ã‚’è§£é‡ˆã—ã€ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒçŸ¥ã‚‹ã¹ãã€ã‚¤ãƒ³ã‚µã‚¤ãƒˆã€‘ã‚’ **Markdownã®ç®‡æ¡æ›¸ã** ã§è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚
        ä»¥ä¸‹ã®3ã¤ã®è¦–ç‚¹ï¼ˆä½•ã‚’ãƒ»ãªãœãƒ»ã ã‹ã‚‰ä½•ï¼‰ã§æ§‹æˆã—ã¦ãã ã•ã„ã€‚
        
        * **ä½•ã‚’ï¼ˆWhatï¼‰:** ãƒ‡ãƒ¼ã‚¿ãŒç¤ºã™æœ€ã‚‚é‡è¦ãªã€Œäº‹å®Ÿã€ã‚„ã€Œå‚¾å‘ã€ã¯ä½•ã‹ï¼Ÿ (ä¾‹: `**ã€‡ã€‡** ãŒ **XX%** å¢—åŠ ...`)
        * **ãªãœï¼ˆWhyï¼‰:** ãªãœãã®å‚¾å‘ãŒèµ·ãã¦ã„ã‚‹ã®ã‹ï¼Ÿï¼ˆèƒŒæ™¯ã‚„åŸå› ã®ã€Œä»®èª¬ã€ï¼‰
        * **ã ã‹ã‚‰ä½•ï¼ˆSo Whatï¼‰:** ã“ã®äº‹å®Ÿã‹ã‚‰æ¨æ¸¬ã§ãã‚‹ã€Œæ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®ãƒ’ãƒ³ãƒˆã€ã¯ä½•ã‹ï¼Ÿ

    3.  **æ›¸å¼**:
        - å›ç­”ã¯ã€Markdownå½¢å¼ã€‘ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚
        - é‡è¦ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚„æ•°å€¤ã¯ `**å¤ªå­—**` ã§å¼·èª¿ã—ã¦ãã ã•ã„ã€‚

    # å‡ºåŠ›å½¢å¼ (å³å®ˆ):
    * JSONä»¥å¤–ã®ãƒ†ã‚­ã‚¹ãƒˆã¯çµ¶å¯¾ã«å«ã‚ãšã€ã€å˜ä¸€ã®JSONã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã€‘`{{ ... }}` ã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
    * ä»¥ä¸‹ã®æ§‹é€ ã‚’å³æ ¼ã«å®ˆã£ã¦ãã ã•ã„ã€‚
        {{
          "slide_title": "ï¼ˆæŒ‡ç¤º1ã§è€ƒæ¡ˆã—ãŸã‚¿ã‚¤ãƒˆãƒ«ï¼‰",
          "slide_content": [
            "ï¼ˆæŒ‡ç¤º2, 3 ã«åŸºã¥ã Markdown å½¢å¼ã®ã‚¤ãƒ³ã‚µã‚¤ãƒˆ1: **ä½•ã‚’**...ï¼‰",
            "ï¼ˆæŒ‡ç¤º2, 3 ã«åŸºã¥ã Markdown å½¢å¼ã®ã‚¤ãƒ³ã‚µã‚¤ãƒˆ2: **ãªãœ**...ï¼‰",
            "ï¼ˆæŒ‡ç¤º2, 3 ã«åŸºã¥ã Markdown å½¢å¼ã®ã‚¤ãƒ³ã‚µã‚¤ãƒˆ3: **ã ã‹ã‚‰ä½•**...ï¼‰"
          ]
        }}

    # å›ç­” (å˜ä¸€ã®JSONã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ã¿):
    """
    
    prompt = PromptTemplate.from_template(ITERATIVE_SLIDE_PROMPT_TEMPLATE)

    # (â˜…) ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’ 120ç§’ (2åˆ†) ã«è¨­å®š
    llm = get_llm(model_name=model_name, temperature=0.2, timeout_seconds=120)
    if llm is None:
        st.error(f"AIãƒ¢ãƒ‡ãƒ«({model_name})ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")
        return "[]" # ç©ºã®JSONãƒªã‚¹ãƒˆ
    
    chain = prompt | llm | StrOutputParser()

    # (â˜…) --- 3. é€æ¬¡å‡¦ç†ãƒ«ãƒ¼ãƒ— (å¤‰æ›´ãªã—) ---
    report_slides_list = []
    log_messages_ui = []
    
    tasks_all = jsonl_data_string.strip().splitlines()
    
    # 3.1. OverallSummaryã‚’æŠ½å‡ºã—ã€æ®‹ã‚Šã‚’å‡¦ç†å¯¾è±¡ã‚¿ã‚¹ã‚¯ã¨ã™ã‚‹
    summary_line = "{}"
    tasks_to_process = []
    for line in tasks_all:
        if '"analysis_task": "OverallSummary"' in line:
            summary_line = line
        else:
            tasks_to_process.append(line)
            
    if not tasks_to_process:
        logger.warning("å‡¦ç†å¯¾è±¡ã®åˆ†æã‚¿ã‚¹ã‚¯ãŒ0ä»¶ã§ã™ã€‚")
        return "[]"

    total_tasks = len(tasks_to_process)
    logger.info(f"å…¨ {total_tasks} ã‚¿ã‚¹ã‚¯ã‚’é€æ¬¡å‡¦ç†ã—ã¾ã™ã€‚")
    
    # 3.2. è¡¨ç´™ã‚¹ãƒ©ã‚¤ãƒ‰ã‚’è¿½åŠ 
    report_slides_list.append({
        "slide_title": "SNSãƒ‡ãƒ¼ã‚¿åˆ†æãƒ¬ãƒãƒ¼ãƒˆ",
        "slide_layout": "title_only",
        "slide_content": ["AI-Generated Analysis (Powered by Gemini)"],
        "image_base64": None
    })
    
    # 3.3. ç›®æ¬¡ã‚¹ãƒ©ã‚¤ãƒ‰ã‚’è¿½åŠ  (ã“ã®æ™‚ç‚¹ã§ã¯ã‚¿ã‚¹ã‚¯åã®ã¿)
    try:
        agenda_items = []
        for i, task_line in enumerate(tasks_to_process):
            try:
                task_name = json.loads(task_line).get('analysis_task', f'åˆ†æã‚¿ã‚¹ã‚¯ {i+1}')
            except json.JSONDecodeError:
                task_name = f'åˆ†æã‚¿ã‚¹ã‚¯ {i+1} (èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼)'
            agenda_items.append(f"{i+1}. {task_name}")

        agenda_items.append(f"{len(tasks_to_process) + 1}. çµè«–ã¨æˆ¦ç•¥çš„æè¨€")
        report_slides_list.append({
            "slide_title": "æœ¬æ—¥ã®ã‚¢ã‚¸ã‚§ãƒ³ãƒ€",
            "slide_layout": "title_and_content",
            "slide_content": agenda_items,
            "image_base64": None
        })
    except Exception as e:
        logger.error(f"ç›®æ¬¡ã‚¹ãƒ©ã‚¤ãƒ‰ã®ç”Ÿæˆã«å¤±æ•—: {e}")

    # 3.4. ãƒ¡ã‚¤ãƒ³ã®åˆ†æã‚¹ãƒ©ã‚¤ãƒ‰ã‚’ãƒ«ãƒ¼ãƒ—å‡¦ç†
    for i, task_line in enumerate(tasks_to_process):
        
        task_name = f"Task {i+1}/{total_tasks}"
        original_task_json = {}
        
        try:
            # 3.4.1. ã‚¿ã‚¹ã‚¯ã®ãƒ‘ãƒ¼ã‚¹ã¨ç”»åƒ/ãƒ†ã‚­ã‚¹ãƒˆã®åˆ†é›¢
            original_task_json = json.loads(task_line)
            task_name = original_task_json.get('analysis_task', task_name)

            # 1. ç”»åƒã‚’Pythonå¤‰æ•°ã«é€€é¿
            image_to_pass_through = original_task_json.get("image_base64")
            
            # (â˜…) [æ”¹å–„ C-2] ç”»åƒã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å®šç¾©
            image_context_str = "ï¼ˆã“ã®ã‚¹ãƒ©ã‚¤ãƒ‰ã«ã¯ç”»åƒã¯å«ã¾ã‚Œã¾ã›ã‚“ã€‚ï¼‰"
            if image_to_pass_through:
                image_context_str = (
                    "ï¼ˆ(æ³¨) ã“ã®ã‚¹ãƒ©ã‚¤ãƒ‰ã«ã¯ã‚°ãƒ©ãƒ•ã‚„ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç­‰ã®ã€Œãƒ‡ãƒ¼ã‚¿å¯è¦–åŒ–ç”»åƒã€ãŒ1æšå«ã¾ã‚Œã¾ã™ã€‚\n"
                    "   ã‚ãªãŸã«ç”»åƒã¯è¦‹ãˆã¾ã›ã‚“ãŒã€`data` ã‚„ `summary` ã‚’æ ¹æ‹ ã«ã€"
                    "   ãã®ç”»åƒãŒã€Œä½•ã‚’æ„å‘³ã™ã‚‹ã®ã‹ã€ã‚’è§£èª¬ã™ã‚‹è€ƒå¯Ÿã‚’è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚ï¼‰"
                )
            
            # 2. AIã«æ¸¡ã™ã€Œãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã€ã®JSONã‚’ä½œæˆ
            text_only_task_json = original_task_json.copy()
            text_only_task_json["image_base64"] = None
            if "data" in text_only_task_json and len(json.dumps(text_only_task_json["data"])) > 1000:
                text_only_task_json["data"] = f"ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼: {str(text_only_task_json['data'])[:1000]}...ï¼‰"
            
            task_data_text_only_str = json.dumps(text_only_task_json, ensure_ascii=False) # (â˜…) ensure_ascii=False
            
        except Exception as e:
            logger.error(f"ã‚¿ã‚¹ã‚¯ '{task_name}' ã®JSONãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—: {e}")
            log_messages_ui.append(f"  -> ERROR: '{task_name}' ã®JSONãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            continue

        # 3.4.2. UIï¼ˆé€²æ—ãƒãƒ¼ãƒ»ãƒ­ã‚°ï¼‰ã®æ›´æ–°
        progress_percent = (i + 1) / (total_tasks + 1)
        progress_bar.progress(progress_percent, text=f"Step C (ã‚¹ãƒ©ã‚¤ãƒ‰ç”Ÿæˆä¸­): {i+1}/{total_tasks} (ãƒ¢ãƒ‡ãƒ«: {model_name})")
        log_messages_ui.append(f"[{i+1}/{total_tasks}] '{task_name}' ã®å‡¦ç†ã‚’é–‹å§‹...")
        log_placeholder.text_area("å®Ÿè¡Œãƒ­ã‚°:", "\n".join(log_messages_ui[::-1]), height=250, key=f"step_c_log_{i}")

        try:
            # 3.4.3. AIã¸ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆ (ãƒ†ã‚­ã‚¹ãƒˆã®ã¿)
            log_messages_ui.append(f"  -> AI ({model_name}) ã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡... (Timeout: 120s)")
            log_placeholder.text_area("å®Ÿè¡Œãƒ­ã‚°:", "\n".join(log_messages_ui[::-1]), height=250, key=f"step_c_log_{i}_sending")
            
            response_str = chain.invoke({
                "task_data_text_only": task_data_text_only_str,
                "image_context": image_context_str, # (â˜…) C-2
                "custom_instruction": custom_instruction_block # (â˜…) C-4
            })
            
            log_messages_ui.append(f"  -> AI ãŒå¿œç­”ã—ã¾ã—ãŸã€‚ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è§£æä¸­...")
            log_placeholder.text_area("å®Ÿè¡Œãƒ­ã‚°:", "\n".join(log_messages_ui[::-1]), height=250, key=f"step_c_log_{i}_received")

            # (â˜…) [æ”¹å–„ C-3] å …ç‰¢ãªJSONãƒ‘ãƒ¼ã‚¹
            start = response_str.find('{')
            end = response_str.rfind('}')
            
            if start != -1 and end != -1 and end > start:
                json_str = response_str[start:end+1]
                ai_response_json = json.loads(json_str)
                
                # 3.4.4. AIã®è€ƒå¯Ÿã¨ã€é€€é¿ã•ã›ãŸç”»åƒã‚’ã€Œå†çµåˆã€
                final_slide_object = {
                    "slide_title": ai_response_json.get("slide_title", task_name),
                    "slide_layout": "text_and_image" if image_to_pass_through else "title_and_content",
                    "slide_content": ai_response_json.get("slide_content", ["AIã«ã‚ˆã‚‹è€ƒå¯Ÿã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚"]),
                    "image_base64": image_to_pass_through # (â˜…) ã“ã“ã§ç”»åƒã‚’æˆ»ã™
                }
                report_slides_list.append(final_slide_object)
                log_messages_ui.append(f"  -> SUCCESS: ã‚¹ãƒ©ã‚¤ãƒ‰ '{final_slide_object.get('slide_title')}' ã‚’ç”Ÿæˆã—ã¾ã—ãŸã€‚")
            else:
                raise Exception("AIãŒJSONã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ `{{...}}` ã‚’è¿”ã—ã¾ã›ã‚“ã§ã—ãŸã€‚")
        
        except Exception as e:
            logger.error(f"ã‚¿ã‚¹ã‚¯ '{task_name}' ã®å‡¦ç†ã«å¤±æ•—: {e}", exc_info=True)
            log_messages_ui.append(f"  -> ERROR: '{task_name}' ã®å‡¦ç†ã«å¤±æ•—ã€‚{e}")
            report_slides_list.append({
                "slide_title": f"ã‚¨ãƒ©ãƒ¼: {task_name}",
                "slide_layout": "title_and_content",
                "slide_content": [f"ã“ã®ã‚¹ãƒ©ã‚¤ãƒ‰ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚", f"ã‚¨ãƒ©ãƒ¼: {e}"],
                "image_base64": None
            })
        
        # 3.4.5. Rate Limit ã®ãŸã‚ã®å¾…æ©Ÿ
        if i < total_tasks:
            log_messages_ui.append(f"  -> Rate Limit (RPM) ã®ãŸã‚ {sleep_time:.1f} ç§’å¾…æ©Ÿã—ã¾ã™...")
            log_placeholder.text_area("å®Ÿè¡Œãƒ­ã‚°:", "\n".join(log_messages_ui[::-1]), height=250, key=f"step_c_log_{i}_sleep")
            time.sleep(sleep_time)

    # (â˜…) 4. çµè«–ã‚¹ãƒ©ã‚¤ãƒ‰ã®ç”Ÿæˆ (å“è³ªå‘ä¸Š)
    try:
        chunk_name = f"çµè«–ã‚¹ãƒ©ã‚¤ãƒ‰"
        progress_percent = 1.0
        progress_bar.progress(progress_percent, text=f"Step C (ãƒãƒ£ãƒ³ã‚¯å‡¦ç†ä¸­): {chunk_name} (ãƒ¢ãƒ‡ãƒ«: {model_name})")
        log_messages_ui.append(f"[{total_tasks+1}/{total_tasks+1}] {chunk_name} ã®å‡¦ç†ã‚’é–‹å§‹...")
        log_placeholder.text_area("å®Ÿè¡Œãƒ­ã‚°:", "\n".join(log_messages_ui[::-1]), height=250, key="step_c_log_final")

        conclusion_llm = get_llm(model_name=model_name, temperature=0.2, timeout_seconds=120)
        if conclusion_llm is None:
            raise Exception("çµè«–ã‚¹ãƒ©ã‚¤ãƒ‰ç”¨AIãƒ¢ãƒ‡ãƒ«ã®å–å¾—ã«å¤±æ•—")

        # (â˜…) [æ”¹å–„ C-1, C-4] çµè«–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚‚å¼·åŒ–
        CONCLUSION_PROMPT_TEMPLATE = """
        ã‚ãªãŸã¯ã‚·ãƒ‹ã‚¢ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒŠãƒªã‚¹ãƒˆã§ã™ã€‚
        ä»¥ä¸‹ã®ã€Œåˆ†æã‚µãƒãƒªãƒ¼ã€ã¨ã€Œç”Ÿæˆã—ãŸã‚¹ãƒ©ã‚¤ãƒ‰ã‚¿ã‚¤ãƒˆãƒ«ã€ã«åŸºã¥ãã€
        ãƒ¬ãƒãƒ¼ãƒˆã®ç· ã‚ããã‚Šã¨ãªã‚‹ã€çµè«–ã¨æˆ¦ç•¥çš„æè¨€ã€‘ã®ã‚¹ãƒ©ã‚¤ãƒ‰1æšåˆ†ã®JSONã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

        # åˆ†æã‚µãƒãƒªãƒ¼ (OverallSummary):
        {summary_data_line}
        
        # ç”Ÿæˆæ¸ˆã¿ã‚¹ãƒ©ã‚¤ãƒ‰ã‚¿ã‚¤ãƒˆãƒ«:
        {slide_titles}

        # (â˜…) ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¨ä½“æ–¹é‡:
        {custom_instruction}

        # æŒ‡ç¤º:
        1.  ã‚¿ã‚¤ãƒˆãƒ«ã¯ã€Œçµè«–ã¨æˆ¦ç•¥çš„æè¨€ã€ã¨ã—ã¾ã™ã€‚
        2.  ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã¯ã€Œtitle_and_contentã€ã¨ã—ã¾ã™ã€‚
        3.  **(â˜…) [æ”¹å–„ C-1] å†…å®¹**:
            åˆ†æå…¨ä½“ã‹ã‚‰å°ã‹ã‚Œã‚‹ã€Œçµè«–ï¼ˆä¸»è¦ãªç™ºè¦‹ï¼‰ã€ã¨ã€ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒæ¬¡ã«å–ã‚‹ã¹ãã€Œå…·ä½“çš„ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆæè¨€ï¼‰ã€ã‚’ã€Markdownã®ç®‡æ¡æ›¸ãã§3ã€œ5ç‚¹ã«ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚
            
            * **çµè«– (Key Findings):** ï¼ˆä¾‹: `**ã€‡ã€‡** ãŒæœ€ã‚‚é‡è¦ãªèª²é¡Œã§ã‚ã‚‹ã¨åˆ¤æ˜...`ï¼‰
            * **æè¨€ (Recommendations):** ï¼ˆä¾‹: `**ã€‡ã€‡** ã«ãƒªã‚½ãƒ¼ã‚¹ã‚’é›†ä¸­æŠ•ä¸‹ã—ã€...`ï¼‰
        
        4.  ç”»åƒ (image_base64) ã¯ null ã¨ã—ã¾ã™ã€‚

        # å‡ºåŠ›å½¢å¼ (å³å®ˆ):
        * JSONä»¥å¤–ã®ãƒ†ã‚­ã‚¹ãƒˆã¯çµ¶å¯¾ã«å«ã‚ãšã€ã€å˜ä¸€ã®JSONã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã€‘`{{ ... }}` ã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

        # å›ç­” (å˜ä¸€ã®JSONã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ã¿):
        """
        
        conclusion_prompt = PromptTemplate.from_template(CONCLUSION_PROMPT_TEMPLATE)
        conclusion_chain = conclusion_prompt | conclusion_llm | StrOutputParser()
        
        log_messages_ui.append(f"  -> AI ({model_name}) ã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡... (Timeout: 120s)")
        log_placeholder.text_area("å®Ÿè¡Œãƒ­ã‚°:", "\n".join(log_messages_ui[::-1]), height=250, key="step_c_log_final_sending")
        
        response_str = conclusion_chain.invoke({
            "summary_data_line": summary_line,
            "slide_titles": json.dumps([s.get('slide_title') for s in report_slides_list], ensure_ascii=False),
            "custom_instruction": custom_instruction_block # (â˜…) C-4
        })
        
        log_messages_ui.append(f"  -> AI ãŒå¿œç­”ã—ã¾ã—ãŸã€‚ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è§£æä¸­...")
        log_placeholder.text_area("å®Ÿè¡Œãƒ­ã‚°:", "\n".join(log_messages_ui[::-1]), height=250, key="step_c_log_final_received")

        # (â˜…) [æ”¹å–„ C-3] å …ç‰¢ãªJSONãƒ‘ãƒ¼ã‚¹
        start = response_str.find('{')
        end = response_str.rfind('}')
            
        if start != -1 and end != -1 and end > start:
            json_str = response_str[start:end+1]
            report_slides_list.append(json.loads(json_str))
            log_messages_ui.append(f"  -> SUCCESS: çµè«–ã‚¹ãƒ©ã‚¤ãƒ‰ã‚’ç”Ÿæˆã—ã¾ã—ãŸã€‚")
        else:
            raise Exception("AIãŒçµè«–ã‚¹ãƒ©ã‚¤ãƒ‰ã®JSONã‚’è¿”ã—ã¾ã›ã‚“ã§ã—ãŸã€‚")
            
    except Exception as e:
         logger.error(f"çµè«–ã‚¹ãƒ©ã‚¤ãƒ‰ã®ç”Ÿæˆã«å¤±æ•—: {e}")
         log_messages_ui.append(f"  -> ERROR: çµè«–ã‚¹ãƒ©ã‚¤ãƒ‰ã®ç”Ÿæˆã«å¤±æ•—ã€‚{e}")
         report_slides_list.append({
                "slide_title": "çµè«–ã¨æˆ¦ç•¥çš„æè¨€ (ç”Ÿæˆå¤±æ•—)",
                "slide_layout": "title_and_content",
                "slide_content": [f"çµè«–ã‚¹ãƒ©ã‚¤ãƒ‰ã®è‡ªå‹•ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚", f"ã‚¨ãƒ©ãƒ¼: {e}"],
                "image_base64": None
            })

    # (â˜…) 5. æœ€çµ‚çš„ãªJSONæ–‡å­—åˆ—ã‚’è¿”ã™
    progress_bar.progress(1.0, text="Step C: å®Œäº†ï¼")
    log_placeholder.text_area("å®Ÿè¡Œãƒ­ã‚°:", "\n".join(log_messages_ui[::-1]), height=250, key="step_c_log_done")
    
    return json.dumps(report_slides_list, ensure_ascii=False, indent=2)

def render_step_c():
    """(Step C) AIãƒ¬ãƒãƒ¼ãƒˆç”ŸæˆUIã‚’æç”»ã™ã‚‹"""
    st.title(f"ğŸ–‹ï¸ Step C: AIåˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ") 

    # Step C å›ºæœ‰ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆ
    if 'step_c_jsonl_data' not in st.session_state:
        st.session_state.step_c_jsonl_data = None
    
    # (â˜…) æ”¹å–„ C-4: UIã§ç·¨é›†ã™ã‚‹æŒ‡ç¤º (Task â‘¨ ã«ç›¸å½“)
    if 'step_c_custom_instruction' not in st.session_state:
        st.session_state.step_c_custom_instruction = ""
        
    if 'step_c_report_json' not in st.session_state:
        st.session_state.step_c_report_json = None
    if 'step_c_model' not in st.session_state:
        st.session_state.step_c_model = MODEL_FLASH 
    if 'current_file_id_C' not in st.session_state:
        st.session_state.current_file_id_C = None

    # --- 1. ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ ---
    st.header("Step 1: åˆ†æãƒ‡ãƒ¼ã‚¿ (JSON) ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
    st.info("Step B ã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã—ãŸ `analysis_data.json` ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
    uploaded_report_file = st.file_uploader(
        "åˆ†æãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ« (analysis_data.json)",
        type=['json', 'jsonl', 'txt'],
        key="step_c_uploader"
    )

    if uploaded_report_file:
        try:
            current_file_id_C = f"{uploaded_report_file.name}_{uploaded_report_file.size}"
            if st.session_state.get('current_file_id_C') != current_file_id_C:
                logger.info(f"Step C: æ–°ã—ã„JSON {current_file_id_C} ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚")
                jsonl_data_string = uploaded_report_file.getvalue().decode('utf-8')
                st.session_state.step_c_jsonl_data = jsonl_data_string
                st.session_state.step_c_report_json = None # (â˜…) çµæœã‚’ãƒªã‚»ãƒƒãƒˆ
                st.session_state.current_file_id_C = current_file_id_C
                st.success(f"ãƒ•ã‚¡ã‚¤ãƒ«ã€Œ{uploaded_report_file.name}ã€èª­è¾¼å®Œäº†")
            
        except Exception as e:
            logger.error(f"Step C ãƒ•ã‚¡ã‚¤ãƒ«èª­è¾¼ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            st.session_state.step_c_jsonl_data = None
            st.session_state.current_file_id_C = None
            return
    else:
        st.session_state.step_c_jsonl_data = None
        st.session_state.step_c_report_json = None
        st.session_state.current_file_id_C = None
        st.warning("åˆ†æã‚’ç¶šã‘ã‚‹ã«ã¯ã€Step B ã§ç”Ÿæˆã—ãŸ JSON ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
        return

    # --- 2. åˆ†æãƒ¬ãƒãƒ¼ãƒˆã®å®Ÿè¡Œ ---
    st.header("Step 2: AIåˆ†æãƒ¬ãƒãƒ¼ãƒˆã®å®Ÿè¡Œ")

    # (â˜…) --- ä¿®æ­£: ãƒ¢ãƒ‡ãƒ«é¸æŠUI ---
    st.markdown("åˆ†æã«ä½¿ç”¨ã™ã‚‹AIãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
    
    model_options = [MODEL_FLASH, MODEL_PRO]
    try:
        default_index = model_options.index(st.session_state.step_c_model)
    except ValueError:
        default_index = 0
        
    selected_model_name = st.radio(
        "ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«",
        options=model_options,
        index=default_index,
        key="step_c_model_radio",
        horizontal=True,
    )
    st.session_state.step_c_model = selected_model_name

    if selected_model_name == MODEL_PRO:
        st.warning(
            f"**`{MODEL_PRO}` (ç„¡æ–™æ ) ã¯ 2 RPM (30ç§’/ãƒªã‚¯ã‚¨ã‚¹ãƒˆ) ã®åˆ¶é™ãŒã‚ã‚Šã¾ã™ã€‚**\n"
            f"ã‚¹ãƒ©ã‚¤ãƒ‰10æšã®ç”Ÿæˆã«ã¯ç´„5åˆ†ã‹ã‹ã‚Šã¾ã™ã€‚ã”æ³¨æ„ãã ã•ã„ã€‚"
        )
    else:
        st.info(
            f"**`{MODEL_FLASH}` (ç„¡æ–™æ ) ã¯ 10 RPM (6ç§’/ãƒªã‚¯ã‚¨ã‚¹ãƒˆ) ã®åˆ¶é™ãŒã‚ã‚Šã¾ã™ã€‚**\n"
            f"æ¯”è¼ƒçš„ é«˜é€Ÿã«ç”Ÿæˆã§ãã¾ã™ã€‚ï¼ˆæ¨å¥¨ï¼‰"
        )
    
    st.markdown("---")
    st.subheader("ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰AIã¸ã®è¿½åŠ æŒ‡ç¤º")
    st.info("ãƒ¬ãƒãƒ¼ãƒˆå…¨ä½“ã‚’é€šã—ã¦AIã«æ„è­˜ã•ã›ãŸã„ã€Œåˆ†æã®è¦–ç‚¹ã€ã‚„ã€Œç‰¹ã«æ³¨ç›®ã™ã¹ãç‚¹ã€ãŒã‚ã‚Œã°å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    
    st.session_state.step_c_custom_instruction = st.text_area(
        "AIã¸ã®è¿½åŠ æŒ‡ç¤ºï¼ˆå…¨ä½“ã®åˆ†ææ–¹é‡ï¼‰:",
        value=st.session_state.step_c_custom_instruction,
        placeholder="ä¾‹: ã€Œç«¶åˆAç¤¾ã¨æ¯”è¼ƒã—ãŸéš›ã®ã€æˆ‘ã€…ã®å¼·ã¿ã€ã«ç„¦ç‚¹ã‚’å½“ã¦ã¦è€ƒå¯Ÿã—ã¦ãã ã•ã„ã€‚\nä¾‹: ä»Šå›ã®åˆ†æã®ç›®çš„ã¯ã€Œè‹¥å¹´å±¤å‘ã‘ã®æ–°è¦æ–½ç­–ç«‹æ¡ˆã€ã§ã™ã€‚ãã®è¦–ç‚¹ã§æè¨€ã‚’ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚",
        height=100,
        key="step_c_custom_instruction_input"
    )
    st.markdown("---")
    
    if st.button(f"åˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ (Step 2)", key="execute_button_C", type="primary", use_container_width=True):
        if not st.session_state.step_c_jsonl_data:
            st.error("ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚Step 1ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
            return
        
        progress_bar = st.progress(0.0, text="Step C: åˆ†æå¾…æ©Ÿä¸­...")
        log_placeholder = st.empty()

        selected_model = st.session_state.step_c_model
        
        try:
            st.session_state.step_c_report_json = run_step_c_analysis(
                st.session_state.step_c_jsonl_data,
                selected_model,
                progress_bar, 
                log_placeholder,
                st.session_state.step_c_custom_instruction 
            )
            st.success("AIã«ã‚ˆã‚‹åˆ†æãƒ¬ãƒãƒ¼ãƒˆãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸï¼")
            
        except Exception as e:
            logger.error(f"Step C å®Ÿè¡Œä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            st.error(f"åˆ†æå®Ÿè¡Œä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            progress_bar.progress(1.0, text="ã‚¨ãƒ©ãƒ¼ã«ã‚ˆã‚Šä¸­æ–­")


    # --- 3. çµæœã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã¨ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ ---
    if st.session_state.step_c_report_json:
        st.header("Step 3: åˆ†æãƒ¬ãƒãƒ¼ãƒˆï¼ˆJSONï¼‰ã®ç¢ºèªã¨ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ")
        st.info("ä»¥ä¸‹ã®æ§‹é€ åŒ–JSONã¯ã€Step D (PowerPointç”Ÿæˆ) ã§ä½¿ç”¨ã—ã¾ã™ã€‚")

        st.download_button(
            label="åˆ†æãƒ¬ãƒãƒ¼ãƒˆ (report_for_powerpoint.json) ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            data=st.session_state.step_c_report_json,
            file_name="report_for_powerpoint.json",
            mime="application/json",
            type="primary",
            use_container_width=True
        )

        st.markdown("---")
        st.subheader("ç”Ÿæˆã•ã‚ŒãŸãƒ¬ãƒãƒ¼ãƒˆ ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")
        
        try:
            report_data = json.loads(st.session_state.step_c_report_json)
            if isinstance(report_data, list) and all(isinstance(item, dict) for item in report_data):
                st.text_area(
                    "AIãŒç”Ÿæˆã—ãŸæ§‹é€ åŒ–JSON (ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼: å…ˆé ­5000æ–‡å­—)",
                    value=st.session_state.step_c_report_json[:5000] + "...",
                    height=300,
                    key="json_preview_C",
                    disabled=True
                )
                
                st.markdown("---")
                st.subheader(f"ã‚¹ãƒ©ã‚¤ãƒ‰æ§‹æˆ ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ ({len(report_data)}æš)")
                for i, slide in enumerate(report_data):
                    title = slide.get('slide_title', 'ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ãªã—ï¼‰')
                    layout = slide.get('slide_layout', 'N/A')
                    
                    slide_content_list = slide.get('slide_content')
                    if isinstance(slide_content_list, list) and slide_content_list:
                        content_preview = str(slide_content_list[0]) if slide_content_list[0] else "ï¼ˆç©ºã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ï¼‰"
                    else:
                        content_preview = "ï¼ˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãªã—ï¼‰"

                    has_image = "æœ‰ã‚Š" if slide.get("image_base64") else "ç„¡ã—"
                    
                    expander_label = f"**{i+1}: {title}** (Layout: {layout}, Image: {has_image})"
                    with st.expander(expander_label):
                        # (â˜…) ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã§ Markdown ã‚’ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°
                        st.markdown(f"**å†…å®¹:**")
                        if isinstance(slide_content_list, list):
                            for content_line in slide_content_list:
                                st.markdown(content_line)
                        else:
                            st.markdown(str(slide_content_list)) # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            else:
                st.error("AIã®å›ç­”ãŒæœŸå¾…ã—ãŸã‚¹ãƒ©ã‚¤ãƒ‰ã®ãƒªã‚¹ãƒˆå½¢å¼ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
                st.text_area("AIã®ç”Ÿå›ç­” (JSON):", value=st.session_state.step_c_report_json, height=200, disabled=True)
                
        except Exception as e:
            st.error(f"ãƒ¬ãƒãƒ¼ãƒˆã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            st.text_area("AIã®ç”Ÿå›ç­” (ãƒ‘ãƒ¼ã‚¹å¤±æ•—):", value=st.session_state.step_c_report_json, height=200, disabled=True)
            
        st.success("ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€Step D (PowerPointç”Ÿæˆ) ã«é€²ã‚“ã§ãã ã•ã„ã€‚")


# (â˜…) ---Step D---
try:
    import pptx
    from pptx import Presentation
    from pptx.util import Inches, Pt
    from pptx.enum.shapes import MSO_SHAPE
    from pptx.enum.dml import MSO_THEME_COLOR
    from pptx.text.text import _Run # (â˜…) Markdownå‰Šé™¤ã®ãŸã‚
    from pptx.table import _Cell    # (â˜…) Markdownå‰Šé™¤ã®ãŸã‚
except ImportError:
    st.error(
        "PowerPointç”Ÿæˆãƒ©ã‚¤ãƒ–ãƒ©ãƒª(python-pptx)ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
        "pip install python-pptx ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚"
    )

def add_markdown_text(text_frame, content_list: List[str]):
    """
    TextFrame (pptx) ã«ã€Markdown (å¤ªå­—) ã‚’è§£é‡ˆã—ãªãŒã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿½åŠ ã™ã‚‹
    """
    if not text_frame or not content_list:
        return

    try:
        # æ—¢å­˜ã®æ®µè½ã‚’ã‚¯ãƒªã‚¢ (æœ€åˆã®1ã¤ã¯æ®‹ã™)
        tf = text_frame
        tf.clear()
        
        is_first_paragraph = True
        
        for item in content_list:
            if not isinstance(item, str):
                item = str(item)

            # (â˜…) Markdownãƒ†ãƒ¼ãƒ–ãƒ«å½¢å¼ã®ç°¡æ˜“ã‚µãƒãƒ¼ãƒˆ
            if item.strip().startswith('|') and item.strip().endswith('|'):
                try:
                    p = tf.add_paragraph()
                    p.text = item # (â˜…) ãƒ†ãƒ¼ãƒ–ãƒ«ã¯ãã®ã¾ã¾ï¼ˆãƒ•ã‚©ãƒ³ãƒˆå¤‰æ›´ï¼‰
                    p.font.name = 'Yu Gothic' # (â˜…) ç­‰å¹…ãƒ•ã‚©ãƒ³ãƒˆæ¨å¥¨ã ãŒã€æ—¥æœ¬èªç’°å¢ƒã‚’å„ªå…ˆ
                    p.font.size = Pt(10)
                    continue
                except Exception:
                    pass # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¦é€šå¸¸ã®ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†
            
            # (â˜…) Markdown (å¤ªå­—) ã®å‡¦ç†
            if is_first_paragraph:
                p = tf.paragraphs[0]
                is_first_paragraph = False
            else:
                p = tf.add_paragraph()

            # `**` ã§æ–‡å­—åˆ—ã‚’åˆ†å‰²
            parts = item.split('**')
            
            for i, part in enumerate(parts):
                if not part: continue
                
                run = p.add_run()
                run.text = part
                
                # `**` ã§æŒŸã¾ã‚ŒãŸå¥‡æ•°ç•ªç›®ã®éƒ¨åˆ† (i=1, 3, 5...) ã‚’å¤ªå­—ã«ã™ã‚‹
                if i % 2 == 1:
                    run.font.bold = True
                
    except Exception as e:
        logger.error(f"add_markdown_text å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        # ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        if 'p' in locals():
            p = text_frame.add_paragraph()
            p.text = "[Markdownã®è§£æã«å¤±æ•—ã—ã¾ã—ãŸ]"

def run_step_d_ai_correction(
    current_json_str: str, 
    correction_prompt: str
) -> str:
    """
    (Step D) ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ä¿®æ­£æŒ‡ç¤ºã«åŸºã¥ãã€AI (Pro) ãŒã‚¹ãƒ©ã‚¤ãƒ‰æ§‹æˆJSONã‚’ä¿®æ­£ã™ã‚‹
    """
    logger.info("Step D AIã‚¹ãƒ©ã‚¤ãƒ‰ä¿®æ­£ (Pro) å®Ÿè¡Œ...")
    
    # (â˜…) Step D ã® AIä¿®æ­£ã¯ã€é«˜å“è³ªãª Pro ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
    llm = get_llm(model_name=MODEL_PRO, temperature=0.1, timeout_seconds=120)
    if llm is None:
        logger.error("run_step_d_ai_correction: LLM (Pro) ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")
        st.error("AIãƒ¢ãƒ‡ãƒ«(Pro)ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
        return current_json_str # (â˜…) å¤±æ•—æ™‚ã¯å…ƒã®JSONã‚’è¿”ã™

    # (â˜…) --- [æ”¹å–„ D-3] ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å¼·åŒ– ---
    prompt = PromptTemplate.from_template(
        """
        ã‚ãªãŸã¯PowerPointã®æ§‹æˆä½œå®¶ã§ã™ã€‚
        ä»¥ä¸‹ã®ã€Œç¾åœ¨ã®ã‚¹ãƒ©ã‚¤ãƒ‰æ§‹æˆ (JSON)ã€ã‚’èª­ã¿ã€ã€Œä¿®æ­£æŒ‡ç¤ºã€ã«åŸºã¥ã„ã¦JSONã‚’å³å¯†ã«ä¿®æ­£ã—ã¦ãã ã•ã„ã€‚

        # ä¿®æ­£æŒ‡ç¤º:
        {user_prompt}
        
        # (â˜…) ä¿®æ­£ã®ç›®çš„:
        * ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯ã€Œè³‡æ–™ã®ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã®æµã‚Œã‚’æ”¹å–„ã€ã—ãŸã‚Šã€Œãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ã‚ˆã‚Šå¼·èª¿ã€ã™ã‚‹ãŸã‚ã«æŒ‡ç¤ºã‚’å‡ºã—ã¦ã„ã¾ã™ã€‚
        * ï¼ˆä¾‹: ã€Œå‰Šé™¤ã—ã¦ã€ã¯ã€ãã®ã‚¹ãƒ©ã‚¤ãƒ‰ãŒä¸è¦ã¨åˆ¤æ–­ã•ã‚ŒãŸãŸã‚ã§ã™ï¼‰

        # ç¾åœ¨ã®ã‚¹ãƒ©ã‚¤ãƒ‰æ§‹æˆ (JSON):
        {current_json}

        # æŒ‡ç¤º:
        1. ã€Œä¿®æ­£æŒ‡ç¤ºã€ã‚’ã€å³å¯†ã«ã€‘å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚(ä¾‹: ã€Œå‰Šé™¤ã—ã¦ã€ãªã‚‰ã€ãã®JSONã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’é…åˆ—ã‹ã‚‰å–ã‚Šé™¤ã)
        2. ã€Œä¿®æ­£æŒ‡ç¤ºã€ã«ãªã„ã‚¹ãƒ©ã‚¤ãƒ‰ã¯ã€çµ¶å¯¾ã«ä¿®æ­£ãƒ»å‰Šé™¤ã—ãªã„ã§ãã ã•ã„ã€‚
        3. å‡ºåŠ›ã¯ã€JSONé…åˆ—å½¢å¼ã®ã¿ã€‘ (`[...]`) ã¨ã—ã¾ã™ã€‚
        4. JSONä»¥å¤–ã®ãƒ†ã‚­ã‚¹ãƒˆï¼ˆã€Œæ‰¿çŸ¥ã—ã¾ã—ãŸã€ãªã©ï¼‰ã¯ã€çµ¶å¯¾ã«ã€‘å«ã‚ãªã„ã§ãã ã•ã„ã€‚

        # å›ç­” (ä¿®æ­£å¾Œã®JSONé…åˆ—ã®ã¿):
        """
    )
    # (â˜…) --- ã“ã“ã¾ã§ ---
    
    chain = prompt | llm | StrOutputParser()
    
    try:
        response_str = chain.invoke({
            "user_prompt": correction_prompt,
            "current_json": current_json_str
        })
        
        # (â˜…) å …ç‰¢ãªJSONãƒ‘ãƒ¼ã‚¹
        start = response_str.find('[')
        end = response_str.rfind(']')
        
        if start != -1 and end != -1 and end > start:
            json_str = response_str[start:end+1]
            logger.info("Step D AIã‚¹ãƒ©ã‚¤ãƒ‰ä¿®æ­£ å®Œäº†ã€‚")
            return json_str
        else:
            logger.error("Step D AIã‚¹ãƒ©ã‚¤ãƒ‰ä¿®æ­£: AIãŒJSONé…åˆ—ã‚’è¿”ã—ã¾ã›ã‚“ã§ã—ãŸã€‚")
            st.error("AIãŒJSONé…åˆ—ã‚’è¿”ã—ã¾ã›ã‚“ã§ã—ãŸã€‚ä¿®æ­£ã¯ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸã€‚")
            return current_json_str
            
    except Exception as e:
        logger.error(f"run_step_d_ai_correction å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        st.error(f"AIã‚¹ãƒ©ã‚¤ãƒ‰ä¿®æ­£ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return current_json_str


def create_powerpoint_presentation(
    template_file: Optional[BytesIO],
    report_data: List[Dict[str, Any]],
    layout_map_names: Dict[str, str]
) -> BytesIO:
    """
    (Step D) ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ(.pptx)ã¨ã‚¹ãƒ©ã‚¤ãƒ‰æ§‹æˆ(JSON)ã«åŸºã¥ãã€
    python-pptx ã‚’ä½¿ç”¨ã—ã¦æœ€çµ‚çš„ãªPowerPointãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆã™ã‚‹ã€‚
    (â˜…) æ”¹å–„: ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®æ—¢å­˜ã‚¹ãƒ©ã‚¤ãƒ‰ã‚’ä¿æŒã—ã€Markdown(å¤ªå­—)ã‚’åæ˜ 
    """
    logger.info("PowerPointç”Ÿæˆå‡¦ç† é–‹å§‹...")
    
    try:
        # (â˜…) 1. ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®èª­ã¿è¾¼ã¿
        if template_file:
            template_file.seek(0)
            prs = Presentation(template_file)
            logger.info("ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½¿ç”¨ã—ã¦PPTXã‚’ç”Ÿæˆã—ã¾ã™ã€‚")
            
            # (â˜…) --- [æ”¹å–„ D-2] ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚¹ãƒ©ã‚¤ãƒ‰ã®å‰Šé™¤ãƒ­ã‚¸ãƒƒã‚¯ã‚’å‰Šé™¤ ---
            # (L2822-L2828 ã®å‰Šé™¤)
            logger.info(f"ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®æ—¢å­˜ã‚¹ãƒ©ã‚¤ãƒ‰ {len(prs.slides)} æšã‚’ä¿æŒã—ã¾ã™ã€‚")
            # (â˜…) --- ã“ã“ã¾ã§ ---

        else:
            prs = Presentation()
            logger.info("ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½¿ç”¨ã—ã¦PPTXã‚’ç”Ÿæˆã—ã¾ã™ã€‚")

        # (â˜…) 2. ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒé¸æŠã—ãŸãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã‚’ãƒãƒƒãƒ”ãƒ³ã‚° (å¤‰æ›´ãªã—)
        layout_map = {
            "title_only": find_layout_by_name(prs, layout_map_names.get("title")),
            "agenda": find_layout_by_name(prs, layout_map_names.get("agenda")),
            "title_and_content": find_layout_by_name(prs, layout_map_names.get("content_text")),
            "text_and_image": find_layout_by_name(prs, layout_map_names.get("content_image")),
        }
        
        fallback_layout = prs.slide_layouts[1] # ã€Œã‚¿ã‚¤ãƒˆãƒ«ã¨ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€
        fallback_title_layout = prs.slide_layouts[0] # ã€Œã‚¿ã‚¤ãƒˆãƒ« ã‚¹ãƒ©ã‚¤ãƒ‰ã€
        
        if layout_map["title_only"] is None:
             layout_map["title_only"] = fallback_title_layout
             logger.warning("ã€Œè¡¨ç´™ã€ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã€Œã‚¿ã‚¤ãƒˆãƒ« ã‚¹ãƒ©ã‚¤ãƒ‰ã€ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
        if layout_map["agenda"] is None:
             layout_map["agenda"] = fallback_layout
             logger.warning("ã€Œç›®æ¬¡ã€ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã€Œã‚¿ã‚¤ãƒˆãƒ«ã¨ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
        if layout_map["title_and_content"] is None:
             layout_map["title_and_content"] = fallback_layout
             logger.warning("ã€Œãƒ†ã‚­ã‚¹ãƒˆã€ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã€Œã‚¿ã‚¤ãƒˆãƒ«ã¨ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
        if layout_map["text_and_image"] is None:
             layout_map["text_and_image"] = fallback_layout
             logger.warning("ã€Œç”»åƒ+ãƒ†ã‚­ã‚¹ãƒˆã€ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã€Œã‚¿ã‚¤ãƒˆãƒ«ã¨ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")

        logger.info(f"ä½¿ç”¨ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆãƒãƒƒãƒ”ãƒ³ã‚°: {layout_map_names}")

        # (â˜…) --- 3. ã‚¹ãƒ©ã‚¤ãƒ‰ã®ç”Ÿæˆ (JSONãƒ‡ãƒ¼ã‚¿ã‚’ãƒ«ãƒ¼ãƒ—) ---
        
        # (â˜…) 3.1. è¡¨ç´™ã‚¹ãƒ©ã‚¤ãƒ‰ (å¤‰æ›´ãªã—)
        first_slide_data = report_data[0]
        if first_slide_data.get("slide_layout") == "title_only":
            slide = prs.slides.add_slide(layout_map["title_only"])
            try:
                slide.shapes.title.text = first_slide_data.get("slide_title", "åˆ†æãƒ¬ãƒãƒ¼ãƒˆ")
            except: pass
            try:
                if len(slide.placeholders) > 1 and slide.placeholders[1]:
                     # (â˜…) [æ”¹å–„ D-1] è¡¨ç´™ã®ã‚µãƒ–ã‚¿ã‚¤ãƒˆãƒ«ã‚‚ Markdown ãƒ˜ãƒ«ãƒ‘ãƒ¼çµŒç”±ã«å¤‰æ›´
                     add_markdown_text(
                         slide.placeholders[1].text_frame, 
                         first_slide_data.get("slide_content", [""])
                     )
            except: pass
            
            report_data = report_data[1:] # (â˜…) è¡¨ç´™ã‚’ãƒªã‚¹ãƒˆã‹ã‚‰å‰Šé™¤
        
        # (â˜…) 3.2. ç›®æ¬¡(Agenda)ã‚¹ãƒ©ã‚¤ãƒ‰ã®è‡ªå‹•ç”Ÿæˆ (Step C ã§ç”Ÿæˆæ¸ˆã®ãŸã‚å¤‰æ›´ãªã—)

        # (â˜…) --- 3.3. ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¹ãƒ©ã‚¤ãƒ‰ (æ®‹ã‚Š) ---
        for i, slide_data in enumerate(report_data):
            slide_title = slide_data.get("slide_title", f"ã‚¹ãƒ©ã‚¤ãƒ‰ {i+2}") 
            slide_layout_key = slide_data.get("slide_layout", "title_and_content")
            slide_content = slide_data.get("slide_content", ["ï¼ˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãªã—ï¼‰"])
            
            image_base64 = slide_data.get("image_base64")

            if image_base64 and slide_layout_key == "title_and_content":
                slide_layout_key = "text_and_image"
            
            if slide_title == "æœ¬æ—¥ã®ã‚¢ã‚¸ã‚§ãƒ³ãƒ€":
                layout_to_use = layout_map["agenda"]
            elif image_base64:
                layout_to_use = layout_map["text_and_image"]
            else:
                layout_to_use = layout_map["title_and_content"]
            
            slide = prs.slides.add_slide(layout_to_use)
            
            try:
                slide.shapes.title.text = slide_title
            except Exception as e:
                logger.warning(f"ã‚¹ãƒ©ã‚¤ãƒ‰ {i+2} ã®ã‚¿ã‚¤ãƒˆãƒ«è¨­å®šå¤±æ•—: {e}")

            # (â˜…) --- ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¨ç”»åƒã®é…ç½® (ãƒ­ã‚¸ãƒƒã‚¯ã‚’å …ç‰¢åŒ–) ---
            try:
                text_placeholders = []
                image_placeholders = []
                
                for shape in slide.placeholders:
                    if shape.placeholder_format.idx == 0: continue
                    if shape.has_text_frame:
                        text_placeholders.append(shape)
                    elif shape.placeholder_format.idx > 100: 
                        image_placeholders.append(shape)

                # (â˜…) ç”»åƒãŒã‚ã‚‹å ´åˆã®å‡¦ç† (text_and_image)
                if image_base64:
                    # (â˜…) 1. ãƒ†ã‚­ã‚¹ãƒˆã‚’æŒ¿å…¥
                    if text_placeholders:
                        tf = text_placeholders[0].text_frame
                        # (â˜…) --- [æ”¹å–„ D-1] Markdown ãƒ˜ãƒ«ãƒ‘ãƒ¼ã‚’å‘¼ã³å‡ºã™ ---
                        add_markdown_text(tf, slide_content)
                        # (â˜…) --- (L2898 ã® re.sub ã‚’å‰Šé™¤) ---
                    
                    # (â˜…) 2. ç”»åƒã‚’æŒ¿å…¥
                    image_ph = None
                    if image_placeholders:
                        image_ph = image_placeholders[0]
                    elif len(text_placeholders) > 1:
                        image_ph = text_placeholders[1] 

                    if image_ph:
                        try:
                            img_bytes = base64.b64decode(image_base64)
                            img_stream = BytesIO(img_bytes)
                            image_ph.insert_picture(img_stream)
                            logger.info(f"ã‚¹ãƒ©ã‚¤ãƒ‰ '{slide_title}': ã‚°ãƒ©ãƒ•ç”»åƒã®æŒ¿å…¥ã«æˆåŠŸã€‚")
                        except Exception as e:
                            logger.error(f"ã‚¹ãƒ©ã‚¤ãƒ‰ '{slide_title}': ã‚°ãƒ©ãƒ•ç”»åƒã®æŒ¿å…¥ã«å¤±æ•—: {e}")
                            if image_ph.has_text_frame:
                                image_ph.text_frame.text = f"ï¼ˆç”»åƒæŒ¿å…¥ã‚¨ãƒ©ãƒ¼: {e}ï¼‰"
                    else:
                         logger.warning(f"ã‚¹ãƒ©ã‚¤ãƒ‰ '{slide_title}': ç”»åƒç”¨ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")

                # (â˜…) ç”»åƒãŒãªã„å ´åˆã®å‡¦ç† (title_and_content)
                else:
                    if not text_placeholders:
                         logger.warning(f"ã‚¹ãƒ©ã‚¤ãƒ‰ '{slide_title}': ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
                         continue
                    tf = text_placeholders[0].text_frame
                    # (â˜…) --- [æ”¹å–„ D-1] Markdown ãƒ˜ãƒ«ãƒ‘ãƒ¼ã‚’å‘¼ã³å‡ºã™ ---
                    add_markdown_text(tf, slide_content)
                    # (â˜…) --- (L2928 ã® re.sub ã‚’å‰Šé™¤) ---

            except Exception as e:
                logger.error(f"ã‚¹ãƒ©ã‚¤ãƒ‰ {i+2} ('{slide_title}') ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„/ç”»åƒè¨­å®šä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)

        logger.info("PowerPointç”Ÿæˆå‡¦ç† å®Œäº†ã€‚")
        file_stream = BytesIO()
        prs.save(file_stream)
        file_stream.seek(0)
        return file_stream

    except Exception as e:
        logger.error(f"create_powerpoint_presentation å…¨ä½“ã§ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        st.error(f"PowerPointã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return None

def render_step_d():
    """(Step D) PowerPointç”ŸæˆUIã‚’æç”»ã™ã‚‹"""
    st.title(f"ãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ (PowerPoint) ç”Ÿæˆ (Step D)")

    if 'step_d_template_file' not in st.session_state:
        st.session_state.step_d_template_file = None
    if 'step_d_template_file_id' not in st.session_state:
        st.session_state.step_d_template_file_id = None
    if 'step_d_report_data' not in st.session_state:
        st.session_state.step_d_report_data = []
    if 'current_report_file_id_D' not in st.session_state:
        st.session_state.current_report_file_id_D = None
        
    if 'step_d_generated_pptx' not in st.session_state:
        st.session_state.step_d_generated_pptx = None
    if 'step_d_layout_map' not in st.session_state:
        st.session_state.step_d_layout_map = {}
    if 'tips_list' not in st.session_state:
        st.session_state.tips_list = []
    if 'current_tip_index' not in st.session_state:
        st.session_state.current_tip_index = 0
    if 'last_tip_time' not in st.session_state:
        st.session_state.last_tip_time = time.time()

    # --- 1. ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ (â˜…) ---
    st.header("Step 1: ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ PowerPoint ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
    st.info(
        "ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰ä½¿ç”¨ã—ãŸã„ .pptx ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãŒã‚ã‚Œã°ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚\n"
        "AIãŒç”Ÿæˆã—ãŸã‚¹ãƒ©ã‚¤ãƒ‰ã¯ã€ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå†…ã®æ—¢å­˜ã‚¹ãƒ©ã‚¤ãƒ‰ï¼ˆè¡¨ç´™ãªã©ï¼‰ã®ã€Œå¾Œã€ã«è¿½åŠ ã•ã‚Œã¾ã™ã€‚"
    )
    
    template_file = st.file_uploader(
        "PowerPoint ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ (.pptx)",
        type=['pptx'],
        key="step_d_template_uploader"
    )
    
    template_layout_names = []
    default_layouts = {
        "title": "ã‚¿ã‚¤ãƒˆãƒ« ã‚¹ãƒ©ã‚¤ãƒ‰",
        "agenda": "ã‚»ã‚¯ã‚·ãƒ§ãƒ³è¦‹å‡ºã—",
        "content_text": "ã‚¿ã‚¤ãƒˆãƒ«ã¨ã‚³ãƒ³ãƒ†ãƒ³ãƒ„",
        "content_image": "2ã¤ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„"
    }
    
    if template_file:
        try:
            template_file_id = f"{template_file.name}_{template_file.size}"
            
            if st.session_state.get('step_d_template_file_id') != template_file_id:
                logger.info(f"Step D: æ–°ã—ã„ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ {template_file_id} ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚")
                template_file.seek(0)
                template_bytes = template_file.getvalue()
                prs = Presentation(BytesIO(template_bytes))
                template_layout_names = [layout.name for layout in prs.slide_layouts]
                
                st.success(f"ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã€Œ{template_file.name}ã€ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")
                st.session_state.step_d_template_file = BytesIO(template_bytes)
                st.session_state.step_d_template_file_id = template_file_id
                st.session_state.step_d_layout_map = {} 
            else:
                st.session_state.step_d_template_file.seek(0)
                prs = Presentation(st.session_state.step_d_template_file)
                template_layout_names = [layout.name for layout in prs.slide_layouts]

        except Exception as e:
            st.error(f"ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
            template_layout_names = []
            st.session_state.step_d_template_file = None
            st.session_state.step_d_template_file_id = None
            
    else:
        if st.session_state.step_d_template_file is not None:
             st.session_state.step_d_template_file = None
             st.session_state.step_d_template_file_id = None
             st.session_state.step_d_layout_map = {}


    # --- 2. Step C åˆ†æçµæœã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ ---
    st.header("Step 2: Step C åˆ†æãƒ¬ãƒãƒ¼ãƒˆ (JSON) ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
    st.info("Step C ã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã—ãŸ `report_for_powerpoint.json` ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
    report_file = st.file_uploader(
        "åˆ†æãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ« (report_for_powerpoint.json)",
        type=['json'],
        key="step_d_report_uploader"
    )

    if report_file:
        try:
            current_report_file_id = f"{report_file.name}_{report_file.size}"
            
            if ('step_d_report_data' not in st.session_state or 
                not st.session_state.step_d_report_data or 
                st.session_state.get('current_report_file_id_D') != current_report_file_id):
                
                logger.info(f"Step D: æ–°ã—ã„ãƒ¬ãƒãƒ¼ãƒˆ {current_report_file_id} ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚")
                report_json_string = report_file.getvalue().decode('utf-8')
                report_data = json.loads(report_json_string)
            
                if isinstance(report_data, list) and all(isinstance(item, dict) for item in report_data):
                    st.success(f"åˆ†æãƒ¬ãƒãƒ¼ãƒˆã€Œ{report_file.name}ã€ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ ({len(report_data)}ã‚¹ãƒ©ã‚¤ãƒ‰)ã€‚")
                    st.session_state.step_d_report_data = report_data
                    st.session_state.current_report_file_id_D = current_report_file_id
                    st.session_state.step_d_generated_pptx = None
                else:
                    st.error("ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸJSONãŒæœŸå¾…ã™ã‚‹å½¢å¼ï¼ˆã‚¹ãƒ©ã‚¤ãƒ‰ã®ãƒªã‚¹ãƒˆï¼‰ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
                    st.session_state.step_d_report_data = []
                    st.session_state.current_report_file_id_D = None

        except Exception as e:
            logger.error(f"Step D JSONãƒ¬ãƒãƒ¼ãƒˆèª­è¾¼ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            st.error(f"åˆ†æãƒ¬ãƒãƒ¼ãƒˆã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            st.session_state.step_d_report_data = []
            st.session_state.current_report_file_id_D = None
    
    if not st.session_state.step_d_report_data:
        st.session_state.step_d_report_data = []
        st.session_state.step_d_generated_pptx = None
        st.session_state.current_report_file_id_D = None
        st.warning("PowerPointã‚’ç”Ÿæˆã™ã‚‹ã«ã¯ã€Step C ã§ç”Ÿæˆã—ãŸ JSON ãƒ¬ãƒãƒ¼ãƒˆã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
        return

    # --- 3. (â˜…) ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆå‰²ã‚Šå½“ã¦---
    st.header("Step 3: ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã®å‰²ã‚Šå½“ã¦")
    
    if not st.session_state.step_d_template_file:
        st.info("Step 1 ã§ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã¨ã€ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆåã‚’é¸æŠã§ãã¾ã™ã€‚ï¼ˆç¾åœ¨ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šï¼‰")
        layout_options = list(default_layouts.values())
    else:
        st.info("ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‹ã‚‰èª­ã¿è¾¼ã‚“ã ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆåã‚’ã€å„ã‚¹ãƒ©ã‚¤ãƒ‰ã‚¿ã‚¤ãƒ—ã«å‰²ã‚Šå½“ã¦ã¦ãã ã•ã„ã€‚")
        layout_options = template_layout_names
        
    if not layout_options:
         st.error("ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
         layout_options = list(default_layouts.values())

    def get_default_index(default_name_key):
        if default_name_key in st.session_state.step_d_layout_map:
            saved_name = st.session_state.step_d_layout_map[default_name_key]
            if saved_name in layout_options:
                return layout_options.index(saved_name)
        
        target_name = default_layouts[default_name_key]
        if target_name in layout_options:
            return layout_options.index(target_name)
            
        for i, opt in enumerate(layout_options):
            if default_name_key in opt.lower():
                return i
            if target_name.split(' ')[0] in opt:
                return i
        return 0

    layout_map = {}
    col1, col2 = st.columns(2)
    with col1:
        layout_map["title"] = st.selectbox(
            "1. è¡¨ç´™ (Title) ã‚¹ãƒ©ã‚¤ãƒ‰:", layout_options, 
            index=get_default_index("title"), key="layout_select_title"
        )
        layout_map["agenda"] = st.selectbox(
            "2. ç›®æ¬¡ (Agenda) ã‚¹ãƒ©ã‚¤ãƒ‰:", layout_options, 
            index=get_default_index("agenda"), key="layout_select_agenda"
        )
    with col2:
        layout_map["content_text"] = st.selectbox(
            "3. åˆ†æ (ãƒ†ã‚­ã‚¹ãƒˆã®ã¿) ã‚¹ãƒ©ã‚¤ãƒ‰:", layout_options, 
            index=get_default_index("content_text"), key="layout_select_text"
        )
        layout_map["content_image"] = st.selectbox(
            "4. åˆ†æ (ãƒ†ã‚­ã‚¹ãƒˆ+ç”»åƒ) ã‚¹ãƒ©ã‚¤ãƒ‰:", layout_options, 
            index=get_default_index("content_image"), key="layout_select_image"
        )
    
    st.session_state.step_d_layout_map = layout_map


    # --- 4. ã‚¹ãƒ©ã‚¤ãƒ‰æ§‹æˆã®ç·¨é›† ---
    st.header("Step 4: ã‚¹ãƒ©ã‚¤ãƒ‰æ§‹æˆã®ç¢ºèªãƒ»ç·¨é›†")
    st.info("ï¼ˆ(â˜…) ãƒã‚¦ã‚¹ã®ãƒ‰ãƒ©ãƒƒã‚°ï¼†ãƒ‰ãƒ­ãƒƒãƒ—ã§ã‚¹ãƒ©ã‚¤ãƒ‰ã®é †ç•ªã‚’å…¥ã‚Œæ›¿ãˆã‚‹ã“ã¨ãŒã§ãã¾ã™ï¼‰")

    try:
        headers_list = []
        header_to_item_map = {}
        
        if not st.session_state.step_d_report_data:
             st.warning("JSONãƒ‡ãƒ¼ã‚¿ãŒç©ºã‹ã€æ­£ã—ãèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
             return

        for i, item in enumerate(st.session_state.step_d_report_data):
            if not isinstance(item, dict):
                st.error(f"ãƒ‡ãƒ¼ã‚¿å½¢å¼ã‚¨ãƒ©ãƒ¼: {item} ã¯è¾æ›¸ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
                continue
            
            title = item.get('slide_title', 'ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ãªã—ï¼‰')
            layout = item.get('slide_layout', 'N/A')
            has_image = "ğŸ–¼ï¸" if (item.get("image_base64")) else "ğŸ“„"
            header_str = f"**{i+1}: {title}** (Layout: `{layout}`, {has_image})"
            headers_list.append(header_str)
            header_to_item_map[header_str] = item

        if not all(isinstance(h, str) for h in headers_list):
            st.error("å†…éƒ¨ã‚¨ãƒ©ãƒ¼: ãƒ˜ãƒƒãƒ€ãƒ¼ãƒªã‚¹ãƒˆã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
            return

        sorted_headers = sort_items(
            items=headers_list,
            key="sortable_slides_v4"
        )
        
        cleaned_sorted_data = []
        for header in sorted_headers:
            if header in header_to_item_map:
                cleaned_sorted_data.append(header_to_item_map[header])
            else:
                logger.error(f"ãƒãƒƒãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: ã‚½ãƒ¼ãƒˆå¾Œã®ãƒ˜ãƒƒãƒ€ãƒ¼ '{header}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            
        st.session_state.step_d_report_data = cleaned_sorted_data
        
    except Exception as e:
        logger.error(f"streamlit-sortables å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        st.error(f"ã‚¹ãƒ©ã‚¤ãƒ‰ç·¨é›†UIã®æç”»ã«å¤±æ•—: {e}ã€‚")


    # --- 5. AIã«ã‚ˆã‚‹ä¿®æ­£æŒ‡ç¤º  ---
    st.header("Step 5: (ã‚ªãƒ—ã‚·ãƒ§ãƒ³) AIã«ã‚ˆã‚‹å†…å®¹ã®ä¿®æ­£æŒ‡ç¤º")
    st.markdown(f"ï¼ˆ(â˜…) ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: `{MODEL_PRO}`ï¼‰")
    
    with st.expander("AIã«ã‚¹ãƒ©ã‚¤ãƒ‰å†…å®¹ã®ä¿®æ­£ã‚’æŒ‡ç¤ºã™ã‚‹"):
        correction_prompt = st.text_area(
            "ä¿®æ­£å†…å®¹ã‚’å…·ä½“çš„ã«æŒ‡ç¤ºã—ã¦ãã ã•ã„:",
            placeholder=(
                "ä¾‹: ã€Œã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ãƒ»ã‚µãƒãƒªãƒ¼ã€ã‚¹ãƒ©ã‚¤ãƒ‰ã®ç®‡æ¡æ›¸ãã‚’3ç‚¹ã«è¦ç´„ã—ã¦ã€‚\n"
                "ä¾‹: ã€Œå…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã€ã‚¹ãƒ©ã‚¤ãƒ‰ã‚’å‰Šé™¤ã—ã¦ã€‚"
            ),
            key="step_d_correction_prompt"
        )
        
        if st.button("AIã§ã‚¹ãƒ©ã‚¤ãƒ‰æ§‹æˆã‚’ä¿®æ­£", key="run_ai_correction_D", type="secondary"):
            if correction_prompt.strip():
                with st.spinner(f"AI ({MODEL_PRO}) ãŒã‚¹ãƒ©ã‚¤ãƒ‰æ§‹æˆ (JSON) ã‚’ä¿®æ­£ä¸­..."):
                    current_json_str = json.dumps(st.session_state.step_d_report_data, ensure_ascii=False)
                    corrected_json_str = run_step_d_ai_correction(current_json_str, correction_prompt)
                    
                    try:
                        corrected_data = json.loads(corrected_json_str)
                        if isinstance(corrected_data, list):
                            st.session_state.step_d_report_data = corrected_data
                            st.success("AIã«ã‚ˆã‚‹ã‚¹ãƒ©ã‚¤ãƒ‰æ§‹æˆã®ä¿®æ­£ãŒå®Œäº†ã—ã¾ã—ãŸã€‚Step 4 ã®æ§‹æˆãŒæ›´æ–°ã•ã‚Œã¦ã„ã¾ã™ã€‚")
                            st.rerun()
                        else:
                            st.error("AIãŒãƒªã‚¹ãƒˆå½¢å¼ã§ãªã„ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã—ã¾ã—ãŸã€‚ä¿®æ­£ã¯ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸã€‚")
                    except Exception as e:
                        st.error(f"AIã®å›ç­”ã®ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—: {e}ã€‚ä¿®æ­£ã¯ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸã€‚")
            else:
                st.warning("ä¿®æ­£æŒ‡ç¤ºã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

    # --- 6. PowerPointç”Ÿæˆ ---
    st.header("Step 6: PowerPointã®ç”Ÿæˆã¨ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ")
    
    tip_placeholder_d = st.empty()
    
    if st.button("PowerPointã‚’ç”Ÿæˆ (Step 6)", key="generate_pptx_D", type="primary", use_container_width=True):
        st.session_state.step_d_generated_pptx = None
        
        if not st.session_state.tips_list or len(st.session_state.tips_list) <= 1:
            with st.spinner("åˆ†æTIPSã‚’AIã§ç”Ÿæˆä¸­..."):
                st.session_state.tips_list = get_analysis_tips_list_from_ai()
                st.session_state.current_tip_index = random.randint(0, len(st.session_state.tips_list) - 1) if st.session_state.tips_list else 0
                st.session_state.last_tip_time = time.time()
        
        with st.spinner("PowerPointãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆä¸­..."):
            now = time.time()
            if (now - st.session_state.last_tip_time > 10):
                if len(st.session_state.tips_list) > 1:
                    st.session_state.current_tip_index = (st.session_state.current_tip_index + 1) % len(st.session_state.tips_list)
                st.session_state.last_tip_time = now
            if st.session_state.tips_list:
                try:
                    current_tip = st.session_state.tips_list[st.session_state.current_tip_index]
                    tip_placeholder_d.info(f"ğŸ’¡ ãƒ‡ãƒ¼ã‚¿åˆ†æTIPS: {current_tip}")
                except IndexError:
                    st.session_state.current_tip_index = 0

            generated_file_stream = create_powerpoint_presentation(
                st.session_state.step_d_template_file,
                st.session_state.step_d_report_data,
                st.session_state.step_d_layout_map
            )
            
            tip_placeholder_d.empty()
            
            if generated_file_stream:
                st.session_state.step_d_generated_pptx = generated_file_stream.getvalue()
                st.success("PowerPointãƒ•ã‚¡ã‚¤ãƒ«ã®ç”ŸæˆãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
            else:
                st.error("PowerPointãƒ•ã‚¡ã‚¤ãƒ«ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚")

    if st.session_state.step_d_generated_pptx:
        st.download_button(
            label="ç”Ÿæˆã•ã‚ŒãŸ PowerPoint ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            data=st.session_state.step_d_generated_pptx,
            file_name="AI_Analysis_Report_v3.pptx",
            mime="application/vnd.openxmlformats-officedocument.presentationml.presentation",
            use_container_width=True
        )
        st.balloons()

# --- 11. (â˜…) Mainé–¢æ•° (ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ) ---
def main():
    """Streamlitã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    
    # (â˜…) --- st.set_page_config() ã‚’æœ€åˆã«å®Ÿè¡Œ ---
    st.set_page_config(page_title="AI Data Analysis App", layout="wide")
    
    if 'log_messages' not in st.session_state:
        st.session_state.log_messages = []
    if 'current_step' not in st.session_state:
        st.session_state.current_step = 'A' # åˆæœŸã‚¹ãƒ†ãƒƒãƒ—
    if 'tips_list' not in st.session_state:
        st.session_state.tips_list = []
    if 'current_tip_index' not in st.session_state:
        st.session_state.current_tip_index = 0
    if 'last_tip_time' not in st.session_state:
        st.session_state.last_tip_time = time.time()

    # (â˜…) --- .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã‚€ ---
    try:
        load_dotenv()
        logger.info(".env ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿è©¦è¡Œå®Œäº†ã€‚")
    except Exception as e:
        logger.warning(f".env ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}") 
    
    # --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ (ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³) ---
    with st.sidebar:
        st.title("AI ãƒ¬ãƒãƒ¼ãƒ†ã‚£ãƒ³ã‚° App")
        st.markdown("---")
        
        st.header("âš™ï¸ AI è¨­å®š")
        
        if not os.getenv("GOOGLE_API_KEY"):
            st.warning(
                "Google APIã‚­ãƒ¼ãŒ.envãƒ•ã‚¡ã‚¤ãƒ«ã«è¨­å®šã•ã‚Œã¦ã„ãªã„ã‹ã€èª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸã€‚\n\n"
                "(.envãƒ•ã‚¡ã‚¤ãƒ«ã« `GOOGLE_API_KEY='ã‚ãªãŸã®APIã‚­ãƒ¼'` ã¨è¨˜è¼‰ã—ã¦ãã ã•ã„)"
            )
        else:
            st.success("Google APIã‚­ãƒ¼ èª­è¾¼å®Œäº†")
            # (â˜…) ã‚¢ãƒ—ãƒªèµ·å‹•æ™‚ã«LLMã¨spaCyã®ãƒ­ãƒ¼ãƒ‰ã‚’è©¦ã¿ã‚‹
            if 'llm_checked' not in st.session_state:
                if get_llm(MODEL_FLASH_LITE) is None: 
                    st.error("LLMã®åˆæœŸåŒ–ã«å¤±æ•—ã€‚APIã‚­ãƒ¼ãŒæ­£ã—ã„ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
                if load_spacy_model() is None:
                    st.error("spaCyãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã€‚")
                st.session_state.llm_checked = True # (â˜…) æ¯ãƒªãƒ©ãƒ³æ™‚ã«ãƒã‚§ãƒƒã‚¯ã—ãªã„ã‚ˆã†
        
        st.markdown("---")
        
        # (â˜…) --- Step Aã€œD ã®ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ãƒœã‚¿ãƒ³ ---
        st.header("ğŸ”„ ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³")
        current_step = st.session_state.current_step
        
        if st.button(
            "Step A: AIã‚¿ã‚°ä»˜ã‘", key="nav_A", use_container_width=True,
            type=("primary" if current_step == 'A' else "secondary")
        ):
            if st.session_state.current_step != 'A':
                st.session_state.current_step = 'A'; st.rerun()

        if st.button(
            "Step B: åˆ†æå®Ÿè¡Œ", key="nav_B", use_container_width=True,
            type=("primary" if current_step == 'B' else "secondary")
        ):
            if st.session_state.current_step != 'B':
                st.session_state.current_step = 'B'; st.rerun()

        if st.button(
            "Step C: AIãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ", key="nav_C", use_container_width=True,
            type=("primary" if current_step == 'C' else "secondary")
        ):
            if st.session_state.current_step != 'C':
                st.session_state.current_step = 'C'; st.rerun()

        if st.button(
            "Step D: PowerPointç”Ÿæˆ", key="nav_D", use_container_width=True,
            type=("primary" if current_step == 'D' else "secondary")
        ):
            if st.session_state.current_step != 'D':
                st.session_state.current_step = 'D'; st.rerun()
                
    # (â˜…) æ—¢å­˜ã® main() é–¢æ•°ã®ãƒ­ã‚¸ãƒƒã‚¯ (ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆåˆæœŸåŒ–) ã‚’ç§»å‹•
    if 'llm_checked' not in st.session_state:
        if os.getenv("GOOGLE_API_KEY"):
            if get_llm(MODEL_FLASH_LITE) is None: 
                pass # (â˜…) ã‚¨ãƒ©ãƒ¼ã¯ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§è¡¨ç¤º
            if load_spacy_model() is None:
                pass # (â˜…) ã‚¨ãƒ©ãƒ¼ã¯ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§è¡¨ç¤º
        st.session_state.llm_checked = True


    # --- ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ (ã‚¹ãƒ†ãƒƒãƒ—ã«å¿œã˜ã¦æç”») ---
    if st.session_state.current_step == 'A':
        render_step_a()
    elif st.session_state.current_step == 'B':
        render_step_b()
    elif st.session_state.current_step == 'C':
        render_step_c()
    elif st.session_state.current_step == 'D':
        render_step_d()
    else:
        st.error("ä¸æ˜ãªã‚¹ãƒ†ãƒƒãƒ—ã§ã™ã€‚Step Aã«æˆ»ã‚Šã¾ã™ã€‚")
        st.session_state.current_step = 'A'; st.rerun()

if __name__ == "__main__":
    main()