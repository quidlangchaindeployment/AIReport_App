import streamlit as st
import pandas as pd
import numpy as np
import os
import re
import json
import logging
import time
import spacy
import altair as alt  # L11: Altair (L630ã‹ã‚‰ç§»å‹•)
from io import StringIO, BytesIO
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser

# L17-L22: å¤–éƒ¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒª ( requirements.txt ã«å¿…è¦ )
# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒª (Excel)
try:
    import openpyxl
except ImportError:
    st.error("Excel (openpyxl) ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚`pip install openpyxl` ã—ã¦ãã ã•ã„ã€‚")
# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒª (spaCy)
try:
    import ja_core_news_sm
except ImportError:
    st.error("spaCyæ—¥æœ¬èªãƒ¢ãƒ‡ãƒ« (ja_core_news_sm) ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚`python -m spacy download ja_core_news_sm` ã—ã¦ãã ã•ã„ã€‚")

# L27: å®šæ•° (KISS)
# AIãƒ¢ãƒ‡ãƒ«ã‚’å®šæ•°åŒ– (KISS)
# ( gemini-1.5-flash-latest ã‚„ gemini-2.5-flash-lite ãªã©)
AI_MODEL_NAME = "gemini-2.5-flash-lite"
# L31: ãƒãƒƒãƒã‚µã‚¤ã‚ºã¨å¾…æ©Ÿæ™‚é–“ã‚‚å®šæ•°åŒ– (KISS)
FILTER_BATCH_SIZE = 50
FILTER_SLEEP_TIME = 4.1  # 15 RPM (60s / 15)
TAGGING_BATCH_SIZE = 10
TAGGING_SLEEP_TIME = 4.1  # 15 RPM

# L37: åœ°åè¾æ›¸
# geography_db.py ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã®ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚° (KISS)
try:
    from geography_db import JAPAN_GEOGRAPHY_DB
except ImportError:
    st.error("åœ°åè¾æ›¸ãƒ•ã‚¡ã‚¤ãƒ« (geography_db.py) ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
    JAPAN_GEOGRAPHY_DB = {}  # å®Ÿè¡Œæ™‚ã‚¨ãƒ©ãƒ¼ã‚’é¿ã‘ã‚‹ãŸã‚ã€ç©ºã®è¾æ›¸ã‚’å®šç¾©

# --- L42-L59: ãƒ­ã‚¬ãƒ¼è¨­å®š ---
class StreamlitLogHandler(logging.Handler):
    """Streamlitã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã«ãƒ­ã‚°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ ã™ã‚‹ãƒãƒ³ãƒ‰ãƒ©"""
    def __init__(self):
        super().__init__()
        if 'log_messages' not in st.session_state:
            st.session_state.log_messages = []

    def emit(self, record):
        """ãƒ­ã‚°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã«è¿½åŠ """
        log_entry = self.format(record)
        st.session_state.log_messages.append(log_entry)
        # ãƒ­ã‚°ãŒæºœã¾ã‚Šã™ããªã„ã‚ˆã†ã«åˆ¶å¾¡ (ä¾‹: æœ€æ–°500ä»¶)
        st.session_state.log_messages = st.session_state.log_messages[-500:]

logger = logging.getLogger(__name__)
if not logger.handlers:
    logger.setLevel(logging.INFO)
    handler = StreamlitLogHandler()
    handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
    logger.addHandler(handler)

# --- L63: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ (KISS / SRP) ---
# LLMã¨spaCyãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã‚’ @st.cache_resource ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚‹
# ã“ã‚Œã«ã‚ˆã‚Šã€æ‰‹å‹•ã§ã® session_state ç®¡ç† (L1385ãªã©) ãŒä¸è¦ã«ãªã‚‹

@st.cache_resource  # ã‚­ãƒ£ãƒƒã‚·ãƒ¥
def get_llm():
    """LLM (Google Gemini) ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ãƒ»ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚‹"""
    try:
        api_key = os.getenv("GOOGLE_API_KEY")
        if not api_key:
            logger.error("GOOGLE_API_KEY ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            # st.error("APIã‚­ãƒ¼ãŒã‚ã‚Šã¾ã›ã‚“") # é–¢æ•°å†…ã§ã®UIè¡¨ç¤ºã¯é¿ã‘ã‚‹ (SRP)
            return None
            
        llm = ChatGoogleGenerativeAI(
            model=AI_MODEL_NAME, 
            temperature=0.0,
            convert_system_message_to_human=True,
            api_key=api_key
        )
        logger.info(f"LLM Model ({AI_MODEL_NAME}) loaded successfully.")
        return llm
    except Exception as e:
        logger.error(f"LLMã®åˆæœŸåŒ–ã«å¤±æ•—: {e}", exc_info=True)
        return None

@st.cache_resource  # ã‚­ãƒ£ãƒƒã‚·ãƒ¥
def load_spacy_model():
    """spaCyã®æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«(ja_core_news_sm)ã‚’ãƒ­ãƒ¼ãƒ‰ãƒ»ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚‹"""
    try:
        logger.info("Loading spaCy model (ja_core_news_sm)...")
        nlp = spacy.load("ja_core_news_sm")
        logger.info("spaCy model loaded successfully.")
        return nlp
    except Exception as e:
        logger.error(f"Failed to load spaCy model: {e}", exc_info=True)
        # st.error ã¯ main / render é–¢æ•°ã§è¡Œã† (SRP)
        return None

# --- L106-L138: ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ãƒ˜ãƒ«ãƒ‘ãƒ¼ (read_file) ---
# (æ—¢å­˜ã® L106-L138 ã‚’ãã®ã¾ã¾ã“ã“ã«è²¼ã‚Šä»˜ã‘)
def read_file(file):
    """ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«(Excel/CSV)ã‚’Pandas DataFrameã¨ã—ã¦èª­ã¿è¾¼ã‚€"""
    file_name = file.name
    logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿é–‹å§‹: {file_name}")
    try:
        if file_name.endswith('.csv'):
            # æ–‡å­—ã‚³ãƒ¼ãƒ‰ã‚’è‡ªå‹•åˆ¤åˆ¥ (KISS)
            try:
                # æœ€åˆã«UTF-8-SIG (BOMä»˜ã) ã‚’è©¦ã™
                content = file.getvalue().decode('utf-8-sig')
                df = pd.read_csv(StringIO(content))
            except UnicodeDecodeError:
                # Shift_JIS (CP932) ã§å†è©¦è¡Œ
                logger.warning(f"UTF-8-SIGãƒ‡ã‚³ãƒ¼ãƒ‰å¤±æ•—ã€‚CP932ã§å†è©¦è¡Œ: {file_name}")
                content = file.getvalue().decode('cp932')
                df = pd.read_csv(StringIO(content))
        elif file_name.endswith(('.xlsx', '.xls')):
            # BytesIO ã‚’ä½¿ç”¨ (KISS)
            df = pd.read_excel(BytesIO(file.getvalue()), engine='openpyxl')
        else:
            logger.warning(f"ã‚µãƒãƒ¼ãƒˆå¤–ã®ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼: {file_name}")
            return None, f"ã‚µãƒãƒ¼ãƒˆå¤–ã®ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼: {file_name}"
        logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿æˆåŠŸ: {file_name}")
        return df, None
    except Exception as e:
        logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ ({file_name}): {e}", exc_info=True)
        st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ã€Œ{file_name}ã€ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        return None, f"èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}"

# --- L140: AIé–¢æ•° (ã‚­ãƒ£ãƒƒã‚·ãƒ¥åˆ©ç”¨ç‰ˆ) ---

def get_dynamic_categories(analysis_prompt):  # llm å¼•æ•°ã‚’å‰Šé™¤ (SRP)
    """
    ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®åˆ†ææŒ‡é‡ã«åŸºã¥ãã€AIãŒå‹•çš„ãªã‚«ãƒ†ã‚´ãƒªã‚’JSONå½¢å¼ã§ç”Ÿæˆã™ã‚‹ã€‚
    """
    llm = get_llm()  # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸLLMã‚’ç›´æ¥å‘¼ã³å‡ºã—
    if llm is None:
        logger.error("get_dynamic_categories: LLM is not available.")
        st.error("AIãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§APIã‚­ãƒ¼ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
        return None  #
        
    logger.info("å‹•çš„ã‚«ãƒ†ã‚´ãƒªç”ŸæˆAIã‚’å‘¼ã³å‡ºã—...")
    prompt = PromptTemplate.from_template(
        """
        ã‚ãªãŸã¯ãƒ‡ãƒ¼ã‚¿åˆ†æã®ã‚¹ã‚­ãƒ¼ãƒè¨­è¨ˆè€…ã§ã™ã€‚ã€Œåˆ†ææŒ‡é‡ã€ã‚’èª­ã¿ã€ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æŠ½å‡ºã™ã‚‹ã¹ãã€Œãƒˆãƒ”ãƒƒã‚¯ã®ã‚«ãƒ†ã‚´ãƒªã€ã‚’è€ƒæ¡ˆã—ã¦ãã ã•ã„ã€‚ã€Œå¸‚åŒºç”ºæ‘ã€ã¯å¿…é ˆã‚«ãƒ†ã‚´ãƒªã¨ã—ã¦è‡ªå‹•ã§è¿½åŠ ã•ã‚Œã‚‹ãŸã‚ã€ãã‚Œä»¥å¤–ã®ã‚«ãƒ†ã‚´ãƒªã‚’å®šç¾©ã—ã¦ãã ã•ã„ã€‚
        # æŒ‡ç¤º: 1.ã€Œåˆ†ææŒ‡é‡ã€ã®ãƒˆãƒ”ãƒƒã‚¯ã‚’ã‚«ãƒ†ã‚´ãƒªåŒ– 2.å„ã‚«ãƒ†ã‚´ãƒªã®èª¬æ˜è¨˜è¿° 3.å³æ ¼ãªJSONè¾æ›¸å‡ºåŠ› 4.åœ°åã‚«ãƒ†ã‚´ãƒªç¦æ­¢ 5.è©²å½“ãªã‘ã‚Œã°ç©ºJSON
        # åˆ†ææŒ‡é‡:{user_prompt}
        # å›ç­” (JSONè¾æ›¸å½¢å¼):
        """
    )
    chain = prompt | llm | StrOutputParser()
    try:
        response_str = chain.invoke({"user_prompt": analysis_prompt})
        logger.debug(f"AIã‚«ãƒ†ã‚´ãƒªå®šç¾©(ç”Ÿ): {response_str}")
        # ( ... æ—¢å­˜ã® L161-L176 ã®ãƒ‘ãƒ¼ã‚¹ãƒ­ã‚¸ãƒƒã‚¯ ... )
        match = re.search(r'\{.*\}', response_str, re.DOTALL)
        if not match:
            logger.warning("AIãŒJSONå½¢å¼ã§å¿œç­”ã—ã¾ã›ã‚“ã§ã—ãŸã€‚")
            return None
        json_str = match.group(0).replace("'", '"')
        try:
            categories = json.loads(json_str)
            return categories
        except json.JSONDecodeError as json_e:
            logger.error(f"AIå¿œç­”ã®JSONãƒ‘ãƒ¼ã‚¹å¤±æ•—: {json_e} - Raw: {json_str}")
            return None
    except Exception as e:
        logger.error(f"AIã‚«ãƒ†ã‚´ãƒªç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        st.error(f"AIã‚«ãƒ†ã‚´ãƒªç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return None

def filter_relevant_data_by_ai(df_batch, analysis_prompt):  # llm å¼•æ•°ã‚’å‰Šé™¤ (SRP)
    """
    AIã‚’ä½¿ã„ã€åˆ†ææŒ‡é‡ã¨ç„¡é–¢ä¿‚ãªè¡Œã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹ (relevant: true/false)ã€‚
    """
    llm = get_llm()  # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸLLMã‚’ç›´æ¥å‘¼ã³å‡ºã—
    if llm is None:
        logger.error("filter_relevant_data_by_ai: LLM is not available.")
        st.error("AIãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚APIã‚­ãƒ¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        return pd.DataFrame()  # ç©ºã®DF (ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¤±æ•—)

    logger.debug(f"{len(df_batch)}ä»¶ AIé–¢é€£æ€§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°é–‹å§‹...")
    
    # ( ... æ—¢å­˜ã® L209-L248 ã®ãƒ­ã‚¸ãƒƒã‚¯ (input_texts_jsonl, prompt, chain.invoke, ãƒ‘ãƒ¼ã‚¹å‡¦ç†) ... )
    input_texts_jsonl = df_batch.apply(lambda row: json.dumps({"id": row['id'], "text": str(row['ANALYSIS_TEXT_COLUMN'])[:500]}, ensure_ascii=False), axis=1).tolist()
    prompt = PromptTemplate.from_template(
        """
        ã‚ãªãŸã¯ãƒ‡ãƒ¼ã‚¿åˆ†æã®ã‚­ãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼ã§ã™ã€‚ã€Œåˆ†ææŒ‡é‡ã€ã«åŸºã¥ãã€ã€Œãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿(JSONL)ã€ã®å„è¡ŒãŒåˆ†æå¯¾è±¡ã¨ã—ã¦ã€é–¢é€£ã—ã¦ã„ã‚‹ã‹ (relevant: true)ã€‘ã€ã€ç„¡é–¢ä¿‚ã‹ (relevant: false)ã€‘ã‚’åˆ¤å®šã—ã¦ãã ã•ã„ã€‚
        # åˆ†ææŒ‡é‡ (Analysis Scope):
        {analysis_prompt}
        # æŒ‡ç¤º:
        1. ã€Œåˆ†ææŒ‡é‡ã€ã¨ã€å¼·ãé–¢é€£ã€‘ã™ã‚‹æŠ•ç¨¿ã®ã¿ã‚’ `true` ã¨ã™ã‚‹ã€‚
        2. å˜ãªã‚‹å®£ä¼ï¼ˆä¾‹: "ã‚»ãƒ¼ãƒ«é–‹å‚¬ä¸­ï¼"ï¼‰ã€æŒ¨æ‹¶ã®ã¿ï¼ˆä¾‹: "ã‚ã‘ã¾ã—ã¦ãŠã‚ã§ã¨ã†"ï¼‰ã€æŒ‡é‡ã¨ç„¡é–¢ä¿‚ãªåœ°åŸŸã®è¨€åŠï¼ˆä¾‹: æŒ‡é‡ãŒã€Œåºƒå³¶ã€ãªã®ã«ã€ŒåŒ—æµ·é“ã€ã®è©±ã®ã¿ï¼‰ã¯ `false` ã¨ã™ã‚‹ã€‚
        3. å‡ºåŠ›ã¯ã€JSONLå½¢å¼ã®ã¿ã€‘ï¼ˆid ã¨ relevant (boolean) ã‚’å«ã‚€è¾æ›¸ï¼‰ã€‚
        # ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ (JSONL):
        {text_data_jsonl}
        # å›ç­” (JSONLå½¢å¼ã®ã¿):
        """
    )
    chain = prompt | llm | StrOutputParser()
    try:
        invoke_params = {
            "analysis_prompt": analysis_prompt,
            "text_data_jsonl": "\n".join(input_texts_jsonl)
        }
        logger.debug(f"AI Filtering - Invoking LLM...")
        response_str = chain.invoke(invoke_params)
        logger.debug(f"AI Filtering - Raw response received.")
        results = []
        match = re.search(r'```(?:jsonl|json)?\s*([\s\S]*?)\s*```', response_str, re.DOTALL)
        jsonl_content = match.group(1).strip() if match else response_str.strip()
        for line in jsonl_content.strip().split('\n'):
            cleaned_line = line.strip()
            if not cleaned_line: continue
            try:
                data = json.loads(cleaned_line)
                if isinstance(data.get("relevant"), bool):
                    results.append({"id": data.get("id"), "relevant": data.get("relevant")})
                else:
                    results.append({"id": data.get("id"), "relevant": str(data.get("relevant")).lower() == 'true'})
            except json.JSONDecodeError as json_e:
                logger.warning(f"AIãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å›ç­”ãƒ‘ãƒ¼ã‚¹å¤±æ•—: {cleaned_line} - Error: {json_e}")
                id_match = re.search(r'"id":\s*(\d+)', cleaned_line)
                if id_match:
                    results.append({"id": int(id_match.group(1)), "relevant": True})
        return pd.DataFrame(results) if results else pd.DataFrame(columns=['id', 'relevant'])
    except Exception as e:
        logger.error(f"AIãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãƒãƒƒãƒå‡¦ç†ä¸­ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        st.error(f"AIãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        return df_batch[['id']].copy().assign(relevant=True)

def perform_ai_tagging(df_batch, categories_to_tag, analysis_prompt=""):  # llm å¼•æ•°ã‚’å‰Šé™¤ (SRP)
    """ãƒ†ã‚­ã‚¹ãƒˆã®ãƒãƒƒãƒã‚’å—ã‘å–ã‚Šã€AIãŒã€æŒ‡å®šã•ã‚ŒãŸã‚«ãƒ†ã‚´ãƒªå®šç¾©ã€‘ã«åŸºã¥ã„ã¦ç›´æ¥ã‚¿ã‚°ä»˜ã‘ã‚’è¡Œã†"""
    llm = get_llm()  # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸLLMã‚’ç›´æ¥å‘¼ã³å‡ºã—
    if llm is None:
        logger.error("perform_ai_tagging: LLM is not available.")
        st.error("AIãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚APIã‚­ãƒ¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        return pd.DataFrame()  # ç©ºã®DF (ã‚¿ã‚°ä»˜ã‘å¤±æ•—)

    logger.debug(f"AI Tagging - Received categories: {json.dumps(categories_to_tag, ensure_ascii=False)}")
    logger.info(f"{len(df_batch)}ä»¶ AIã‚¿ã‚°ä»˜ã‘é–‹å§‹ (ã‚«ãƒ†ã‚´ãƒª: {list(categories_to_tag.keys())})")
    
    # ( ... æ—¢å­˜ã® L258-L321 ã®ãƒ­ã‚¸ãƒƒã‚¯ (geography_context, input_texts_jsonl, prompt, chain.invoke, ãƒ‘ãƒ¼ã‚¹å‡¦ç†) ... )
    relevant_geo_db = {}
    if JAPAN_GEOGRAPHY_DB:
        prompt_lower = analysis_prompt.lower()
        keys_found = [
            key for key in JAPAN_GEOGRAPHY_DB.keys() 
            if any(hint in key for hint in [
                "åºƒå³¶", "ç¦å²¡", "å¤§é˜ª", "æ±äº¬", "åŒ—æµ·é“", "æ„›çŸ¥", "å®®åŸ", "æœ­å¹Œ", "æ¨ªæµœ", "åå¤å±‹", "äº¬éƒ½", "ç¥æˆ¸", "ä»™å°"
            ]) and any(hint in prompt_lower for hint in [
                "åºƒå³¶", "ç¦å²¡", "å¤§é˜ª", "æ±äº¬", "åŒ—æµ·é“", "æ„›çŸ¥", "å®®åŸ", "æœ­å¹Œ", "æ¨ªæµœ", "åå¤å±‹", "äº¬éƒ½", "ç¥æˆ¸", "ä»™å°"
            ])
        ]
        if "åºƒå³¶" in prompt_lower: keys_found.extend(["åºƒå³¶çœŒ", "åºƒå³¶å¸‚"])
        if "æ±äº¬" in prompt_lower: keys_found.extend(["æ±äº¬éƒ½", "æ±äº¬23åŒº"])
        if "å¤§é˜ª" in prompt_lower: keys_found.extend(["å¤§é˜ªåºœ", "å¤§é˜ªå¸‚"])
        for key in set(keys_found):
            if key in JAPAN_GEOGRAPHY_DB:
                relevant_geo_db[key] = JAPAN_GEOGRAPHY_DB[key]
        if not relevant_geo_db:
            logger.warning("åœ°åè¾æ›¸ã®çµã‚Šè¾¼ã¿ãƒ’ãƒ³ãƒˆãªã—ã€‚ä¸»è¦éƒ½å¸‚ã®ã¿æ¸¡ã—ã¾ã™ã€‚")
            default_keys = ["æ±äº¬éƒ½", "æ±äº¬23åŒº", "å¤§é˜ªåºœ", "å¤§é˜ªå¸‚", "åºƒå³¶çœŒ", "åºƒå³¶å¸‚"]
            for key in default_keys:
                 if key in JAPAN_GEOGRAPHY_DB:
                     relevant_geo_db[key] = JAPAN_GEOGRAPHY_DB[key]
        geo_context_str = json.dumps(relevant_geo_db, ensure_ascii=False, indent=2)
        if len(geo_context_str) > 5000:
            logger.warning(f"åœ°åè¾æ›¸ãŒå¤§ãã™ã ({len(geo_context_str)}B)ã€‚ã‚­ãƒ¼ã®ã¿ã«ç¸®å°ã€‚")
            geo_context_str = json.dumps(list(relevant_geo_db.keys()), ensure_ascii=False)
    else:
        geo_context_str = "{}"
    logger.info(f"AIã«æ¸¡ã™åœ°åè¾æ›¸(çµè¾¼æ¸ˆ): {list(relevant_geo_db.keys())}")
    
    input_texts_jsonl = df_batch.apply(lambda row: json.dumps({"id": row['id'], "text": str(row['ANALYSIS_TEXT_COLUMN'])[:500]}, ensure_ascii=False), axis=1).tolist()
    logger.debug(f"AI Tagging - Input sample: {input_texts_jsonl[0] if input_texts_jsonl else 'None'}")
    
    prompt = PromptTemplate.from_template(
        """
        ã‚ãªãŸã¯é«˜ç²¾åº¦ãƒ‡ãƒ¼ã‚¿åˆ†æã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ã€Œã‚«ãƒ†ã‚´ãƒªå®šç¾©ã€ã€Œåœ°åè¾æ›¸ã€ã€Œåˆ†ææŒ‡é‡ã€ã«åŸºã¥ãã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºã—ã¾ã™ã€‚
        # åˆ†ææŒ‡é‡ (Analysis Scope): {analysis_prompt}
        # åœ°åè¾æ›¸ (JAPAN_GEOGRAPHY_DB): {geo_context}
        # ã‚«ãƒ†ã‚´ãƒªå®šç¾© (categories): {categories}
        # ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ (JSONL): {text_data_jsonl}
        # æŒ‡ç¤º:
        1. ã€Œãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿(JSONL)ã€ã®å„è¡Œã‚’å‡¦ç†ã™ã‚‹ã€‚
        2. ã€Œã‚«ãƒ†ã‚´ãƒªå®šç¾©ã€ã®ã‚­ãƒ¼åã‚’ã€å³æ ¼ã«ã€‘ä½¿ç”¨ã—ã€å…¨ã‚«ãƒ†ã‚´ãƒªã‚’æŠ½å‡ºã™ã‚‹ã€‚
        3. ã€"å¸‚åŒºç”ºæ‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰" ä»¥å¤–ã®ã‚«ãƒ†ã‚´ãƒªã€‘:
           - å€¤ã¯å¿…ãšã€ãƒªã‚¹ãƒˆå½¢å¼ã€‘ã§å‡ºåŠ›ï¼ˆè©²å½“ãªã‘ã‚Œã°ç©ºãƒªã‚¹ãƒˆ []ï¼‰ã€‚
        4. ã€"å¸‚åŒºç”ºæ‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰" (æœ€é‡è¦ãƒ»å˜ä¸€å›ç­”)ã€‘:
           - å€¤ã¯ã€å˜ä¸€ã®æ–‡å­—åˆ—ã€‘ã§å‡ºåŠ›ã™ã‚‹ (è©²å½“ãªã‘ã‚Œã°ç©ºæ–‡å­—åˆ— "")ã€‚ãƒªã‚¹ãƒˆå½¢å¼ã¯ã€å³ç¦ã€‘ã€‚
           - æŠ½å‡ºãƒ«ãƒ¼ãƒ«:
             a. ã€Œåœ°åè¾æ›¸ã€ã®ã€å€¤ã€‘(ä¾‹: "å‘‰å¸‚", "å»¿æ—¥å¸‚å¸‚", "ä¸­åŒº") ã¾ãŸã¯ã€ã‚­ãƒ¼ã€‘(ä¾‹: "åºƒå³¶å¸‚") ã«ä¸€è‡´ã™ã‚‹ã€æœ€ã‚‚æ–‡è„ˆã«é–¢é€£æ€§ã®é«˜ã„ã‚‚ã®ã‚’ã€1ã¤ã ã‘ã€‘é¸ã¶ã€‚
             b. (ä¾‹: "åºƒå³¶å¸‚" ã¨ "ä¸­åŒº" ãŒä¸¡æ–¹è¨€åŠã•ã‚Œã¦ã„ã‚Œã°ã€ã‚ˆã‚Šè©³ç´°ãª "ä¸­åŒº" ã‚’å„ªå…ˆã™ã‚‹)
             c. "å®®å³¶" ã®ã‚ˆã†ãªãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯åã¯ã€ãã‚ŒãŒå±ã™ã‚‹ã€Œåœ°åè¾æ›¸ã€ã®å¸‚åŒºç”ºæ‘å (ä¾‹: "å»¿æ—¥å¸‚å¸‚") ã«ã€å¿…ãšå¤‰æ›ã€‘ã—ã¦å›ç­”ã™ã‚‹ã€‚
             d. "åºƒå³¶" ã®ã‚ˆã†ãªæ›–æ˜§ãªè¡¨ç¾ã¯ã€æ–‡è„ˆã‹ã‚‰ (a) ã®ã„ãšã‚Œã‹ã«ç‰¹å®šã§ãã‚‹å ´åˆã®ã¿ (ä¾‹: "åºƒå³¶å¸‚") æŠ½å‡ºã—ã€ç‰¹å®šã§ããªã‘ã‚Œã°ã€ç©ºæ–‡å­—åˆ— ""ã€‘ã¨ã™ã‚‹ã€‚
             e. éƒ½é“åºœçœŒå (ä¾‹: "åºƒå³¶çœŒ")ã€ãŠã‚ˆã³ã€Œè¦³å…‰åœ°ã€ã®ã‚ˆã†ãªåœ°åä»¥å¤–ã®å˜èªã¯ã€çµ¶å¯¾ã«æŠ½å‡ºã—ãªã„ã€‘ã€‚
             f. ã€Œåˆ†ææŒ‡é‡ã€ã¨ç„¡é–¢ä¿‚ãªåœ°åŸŸã®åœ°åï¼ˆä¾‹: æŒ‡é‡ãŒã€Œåºƒå³¶ã€ãªã®ã«ãƒ†ã‚­ã‚¹ãƒˆãŒã€Œæ»‹è³€çœŒã€ï¼‰ã¯ã€æŠ½å‡ºã—ãªã„ã€‘ã€‚
        5. ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆæƒ…å ±ã®æé€ ï¼‰ç¦æ­¢ã€‚
        6. å‡ºåŠ›ã¯ã€JSONLå½¢å¼ã®ã¿ã€‘ï¼ˆid ã¨ categories ã‚’å«ã‚€è¾æ›¸ï¼‰ã€‚
        # å›ç­” (JSONLå½¢å¼ã®ã¿):
        """
    )
    chain = prompt | llm | StrOutputParser()
    try:
        invoke_params = {
            "categories": json.dumps(categories_to_tag, ensure_ascii=False), 
            "geo_context": geo_context_str,
            "text_data_jsonl": "\n".join(input_texts_jsonl),
            "analysis_prompt": analysis_prompt
        }
        logger.debug(f"AI Tagging - Invoking LLM...")
        logger.info(f"Attempting AI call for ID: {df_batch.iloc[0]['id']}...")
        response_str = chain.invoke(invoke_params)
        logger.debug(f"AI Tagging - Raw response received.")
        
        results = []
        expected_keys = list(categories_to_tag.keys())
        match = re.search(r'```(?:jsonl|json)?\s*([\s\S]*?)\s*```', response_str, re.DOTALL)
        jsonl_content = match.group(1).strip() if match else response_str.strip()

        for line in jsonl_content.strip().split('\n'):
            cleaned_line = line.strip()
            if not cleaned_line: continue
            try:
                data = json.loads(cleaned_line)
                row_result = {"id": data.get("id")}
                tag_source = data.get('categories', data)
                
                for key in expected_keys:
                    found_key = None
                    for resp_key in tag_source.keys():
                        if resp_key.strip() == key:
                            found_key = resp_key
                            break
                    raw_value = tag_source.get(found_key) if found_key else None
                    
                    if key == "å¸‚åŒºç”ºæ‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰":
                        processed_value = ""
                        if isinstance(raw_value, list) and raw_value:
                            processed_value = str(raw_value[0]).strip()
                        elif raw_value is not None and str(raw_value).strip():
                            processed_value = str(raw_value).strip()
                        if processed_value.lower() in ["è©²å½“ãªã—", "none", "null", ""]:
                            row_result[key] = "" 
                        else:
                            row_result[key] = processed_value
                    else:
                        processed_values = [] 
                        if isinstance(raw_value, list):
                            processed_values = sorted(list(set(str(val).strip() for val in raw_value if str(val).strip())))
                        elif raw_value is not None and str(raw_value).strip():
                            processed_values = [str(raw_value).strip()]
                        row_result[key] = processed_values
                results.append(row_result)
            except json.JSONDecodeError as json_e:
                logger.warning(f"AIã‚¿ã‚°ä»˜ã‘å›ç­”ãƒ‘ãƒ¼ã‚¹å¤±æ•—: {cleaned_line} - Error: {json_e}")
                id_match = re.search(r'"id":\s*(\d+)', cleaned_line)
                if id_match:
                    results.append({"id": int(id_match.group(1))})
        return pd.DataFrame(results) if results else pd.DataFrame(columns=['id'] + list(expected_keys))
    except Exception as e:
        logger.error(f"AIã‚¿ã‚°ä»˜ã‘ãƒãƒƒãƒå‡¦ç†ä¸­ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        st.error(f"AIã‚¿ã‚°ä»˜ã‘å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        return pd.DataFrame() # å¤±æ•—æ™‚ã¯ç©ºã®DFã‚’è¿”ã™

# --- L322-L438: Step B (åˆ†ææ‰‹æ³•ææ¡ˆ) ---
# (æ—¢å­˜ã® L322-L438 (suggest_analysis_techniques é–¢æ•°) ã‚’ãã®ã¾ã¾ã“ã“ã«è²¼ã‚Šä»˜ã‘)
def suggest_analysis_techniques(df):
    """
    ãƒ•ãƒ©ã‚°ä»˜ããƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’åˆ†æã—ã€é©åˆ‡ãªåˆ†ææ‰‹æ³•ã‚’å„ªå…ˆåº¦é †ã«ææ¡ˆã™ã‚‹ã€‚
    """
    suggestions = []
    if df is None or df.empty: # ç©ºã®DFã‚‚ãƒã‚§ãƒƒã‚¯
        logger.error("suggest_analysis_techniques ã« None ã¾ãŸã¯ç©ºã®DataFrame"); return suggestions
    try:
        # ãƒ‡ãƒ¼ã‚¿å‹ã®å†ç¢ºèªã¨åˆ—ã®ç‰¹å®š (ã‚ˆã‚Šç¢ºå®Ÿã«)
        numeric_cols = df.select_dtypes(include=np.number).columns.tolist()
        object_cols = df.select_dtypes(include='object').columns.tolist() # objectå‹ã‚’ã¾ãšå–å¾—
        datetime_cols = []
        possible_dt_cols = [col for col in object_cols] # objectåˆ—ã‹ã‚‰å€™è£œã‚’æ¢ã™
        # æ—¥ä»˜å‹ã¸ã®å¤‰æ›ã‚’è©¦ã¿ã‚‹ (æ¬ æãŒå¤šã„åˆ—ã¯é™¤å¤–)
        for col in possible_dt_cols:
             if df[col].isnull().sum() / len(df) > 0.5: continue # æ¬ æãŒ5å‰²è¶…ãªã‚‰ã‚¹ã‚­ãƒƒãƒ—
             sample = df[col].dropna().head(50)
             if sample.empty: continue
             try:
                 pd.to_datetime(sample, errors='raise')
                 # å¤‰æ›æˆåŠŸ â†’ å…¨ä½“ã‚’å¤‰æ›ã—ã¦ç¢ºèª
                 temp_dt = pd.to_datetime(df[col], errors='coerce').dropna()
                 # å¹´æœˆæ—¥ã®ã„ãšã‚Œã‹ãŒè¤‡æ•°å­˜åœ¨ã™ã‚‹ã‹ã€ç‰¹å®šã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€ã‹ãªã©ã§åˆ¤æ–­
                 if not temp_dt.empty and (temp_dt.dt.year.nunique() > 1 or temp_dt.dt.month.nunique() > 1 or temp_dt.dt.day.nunique() > 1 or col.lower() in ['date', 'time', 'timestamp', 'æ—¥ä»˜', 'æ—¥æ™‚']):
                     datetime_cols.append(col)
                     logger.info(f"åˆ— '{col}' ã‚’æ—¥æ™‚åˆ—ã¨ã—ã¦èªè­˜ã—ã¾ã—ãŸã€‚")
             except (ValueError, TypeError, OverflowError, pd.errors.ParserError): pass # ã‚¨ãƒ©ãƒ¼ãŒå‡ºã¦ã‚‚ç„¡è¦–

        numeric_cols = [col for col in numeric_cols if col != 'id'] # idåˆ—é™¤å¤–
        # ANALYSIS_TEXT_COLUMN ã¨æ—¥æ™‚åˆ—ã‚’é™¤ã„ãŸã‚‚ã®ãŒã‚«ãƒ†ã‚´ãƒªåˆ—å€™è£œ
        categorical_cols = [col for col in object_cols if col != 'ANALYSIS_TEXT_COLUMN' and col not in datetime_cols]
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ—ï¼ˆãƒ•ãƒ©ã‚°åˆ—ï¼‰ã‚’ç‰¹å®š
        flag_cols = [col for col in categorical_cols if col.endswith('ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰')]
        other_categorical = [col for col in categorical_cols if not col.endswith('ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰')]
        logger.info(f"ææ¡ˆåˆ†æ - æ•°å€¤:{numeric_cols}, ã‚«ãƒ†ã‚´ãƒª(ãƒ•ãƒ©ã‚°):{flag_cols}, ã‚«ãƒ†ã‚´ãƒª(ä»–):{other_categorical}, æ—¥æ™‚:{datetime_cols}")

        # --- ææ¡ˆãƒªã‚¹ãƒˆ (å„ªå…ˆåº¦é †) ---
        potential_suggestions = []

        # å„ªå…ˆåº¦1: åŸºæœ¬é›†è¨ˆ (ã»ã¼å¿…é ˆ)
        if flag_cols:
            potential_suggestions.append({
                "priority": 1, "name": "å˜ç´”é›†è¨ˆï¼ˆé »åº¦åˆ†æï¼‰",
                "description": "å„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆã‚«ãƒ†ã‚´ãƒªï¼‰ãŒã©ã®ãã‚‰ã„ã®é »åº¦ã§å‡ºç¾ã—ãŸã‹ãƒˆãƒƒãƒ—Nã‚’è¡¨ç¤ºã—ã€å…¨ä½“åƒã‚’æŠŠæ¡ã—ã¾ã™ã€‚",
                "reason": f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ—({len(flag_cols)}å€‹)ã‚ã‚Šã€‚ã¾ãšè¦‹ã‚‹ã¹ãåŸºæœ¬æŒ‡æ¨™ã§ã™ã€‚",
                "suitable_cols": flag_cols
            })
        if numeric_cols:
             potential_suggestions.append({
                 "priority": 1, "name": "åŸºæœ¬çµ±è¨ˆé‡",
                 "description": f"æ•°å€¤ãƒ‡ãƒ¼ã‚¿({', '.join(numeric_cols)})ã®å¹³å‡ã€ä¸­å¤®å€¤ã€æœ€å¤§/æœ€å°å€¤ãªã©ã‚’ç®—å‡ºã—ã€ãƒ‡ãƒ¼ã‚¿ã®åˆ†å¸ƒã‚’ç¢ºèªã—ã¾ã™ã€‚",
                 "reason": f"æ•°å€¤åˆ—({len(numeric_cols)}å€‹)ã‚ã‚Šã€‚ãƒ‡ãƒ¼ã‚¿ã®åŸºæœ¬ç‰¹æ€§æŠŠæ¡ã«ã€‚",
                 "suitable_cols": numeric_cols
             })

        # å„ªå…ˆåº¦2: é–¢ä¿‚æ€§ã®åˆ†æ (ã‚¯ãƒ­ã‚¹é›†è¨ˆ)
        if len(flag_cols) >= 2:
            potential_suggestions.append({
                "priority": 2, "name": "ã‚¯ãƒ­ã‚¹é›†è¨ˆï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é–“ï¼‰",
                "description": "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é–“ã®çµ„ã¿åˆã‚ã›ã§å¤šãå‡ºç¾ã™ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¢ã‚Šã¾ã™ï¼ˆä¾‹: ç‰¹å®šã®å¸‚åŒºç”ºæ‘ã¨è¦³å…‰åœ°ã®çµ„ã¿åˆã‚ã›ï¼‰ã€‚",
                "reason": f"è¤‡æ•°ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ—({len(flag_cols)}å€‹)ã‚ã‚Šã€é–¢é€£æ€§ã®ç™ºè¦‹ã«ã€‚",
                "suitable_cols": flag_cols
            })
        if flag_cols and other_categorical:
             potential_suggestions.append({
                "priority": 2, "name": "ã‚¯ãƒ­ã‚¹é›†è¨ˆï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰Ã—å±æ€§ï¼‰",
                "description": f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰({flag_cols[0]}ãªã©)ã¨ä»–ã®å±æ€§({', '.join(other_categorical)})ã®é–¢ä¿‚æ€§ã‚’åˆ†æã—ã¾ã™ï¼ˆä¾‹: å¹´ä»£åˆ¥ã«ã‚ˆãå‡ºã‚‹è¦³å…‰åœ°ï¼‰ã€‚",
                "reason": f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ—ã¨ä»–ã‚«ãƒ†ã‚´ãƒªåˆ—({len(other_categorical)}å€‹)ã‚ã‚Šã€å±æ€§ã”ã¨ã®å‚¾å‘æŠŠæ¡ã«ã€‚",
                "suitable_cols": flag_cols + other_categorical
            })

        # å„ªå…ˆåº¦3: å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆ†æ (L438ã®æŒ‡ç¤º)
        if len(flag_cols) >= 2:
            potential_suggestions.append({
                "priority": 3, "name": "å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆ†æ",
                "description": "ãƒ†ã‚­ã‚¹ãƒˆå†…ã§åŒæ™‚ã«å‡ºç¾ã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆä¾‹: ã€Œåºƒå³¶å¸‚ã€ã¨ã€Œå³å³¶ç¥ç¤¾ã€ï¼‰ã®é–¢ä¿‚æ€§ã‚’ç·šã§çµã³ã€ã©ã®å˜èªãŒä¸­å¿ƒçš„ãªå½¹å‰²ã‚’æœãŸã—ã¦ã„ã‚‹ã‹ã‚’å¯è¦–åŒ–ã—ã¾ã™ã€‚",
                "reason": f"è¤‡æ•°ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ—({len(flag_cols)}å€‹)ã‚ã‚Šã€‚å˜èªé–“ã®éš ã‚ŒãŸã¤ãªãŒã‚Šã‚’ç™ºè¦‹ã§ãã¾ã™ã€‚",
                "suitable_cols": flag_cols
            })

        # å„ªå…ˆåº¦4: ã‚°ãƒ«ãƒ¼ãƒ—æ¯”è¼ƒ (L438ã®æŒ‡ç¤º)
        if numeric_cols and flag_cols:
            potential_suggestions.append({
                "priority": 4, "name": "ã‚«ãƒ†ã‚´ãƒªåˆ¥é›†è¨ˆï¼ˆã‚°ãƒ«ãƒ¼ãƒ—æ¯”è¼ƒï¼‰",
                "description": f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚«ãƒ†ã‚´ãƒªï¼ˆ{flag_cols[0]}ãªã©ï¼‰ã”ã¨ã«æ•°å€¤ãƒ‡ãƒ¼ã‚¿({numeric_cols[0]}ãªã©)ã®å¹³å‡å€¤ã‚„åˆè¨ˆå€¤ã«å·®ãŒã‚ã‚‹ã‹æ¯”è¼ƒã—ã¾ã™ã€‚",
                "reason": f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ—ã¨æ•°å€¤åˆ—({len(numeric_cols)}å€‹)ã‚ã‚Šã€ã‚°ãƒ«ãƒ¼ãƒ—é–“ã®ç‰¹å¾´æ¯”è¼ƒã«ã€‚",
                "suitable_cols": {"numeric": numeric_cols, "grouping": flag_cols}
            })

        # å„ªå…ˆåº¦5: æ™‚ç³»åˆ—åˆ†æ (L438ã®æŒ‡ç¤º)
        if datetime_cols and flag_cols:
             potential_suggestions.append({
                "priority": 5, "name": "æ™‚ç³»åˆ—ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æ",
                "description": f"ç‰¹å®šã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å‡ºç¾æ•°ãŒæ™‚é–“ï¼ˆ{datetime_cols[0]}ãªã©ï¼‰ã¨ã¨ã‚‚ã«ã©ã†å¤‰åŒ–ã—ãŸã‹ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’å¯è¦–åŒ–ã—ã¾ã™ã€‚",
                "reason": f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ—ã¨æ—¥æ™‚åˆ—({len(datetime_cols)}å€‹)ã‚ã‚Šã€æ™‚é–“å¤‰åŒ–ã®æŠŠæ¡ã«ã€‚",
                "suitable_cols": {"datetime": datetime_cols, "keywords": flag_cols}
            })

        # å„ªå…ˆåº¦6: ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚° (L438ã®æŒ‡ç¤º)
        potential_suggestions.append({
            "priority": 6, "name": "ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°ï¼ˆé »å‡ºå˜èªãªã©ï¼‰",
            "description": "åŸæ–‡ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰é »å‡ºã™ã‚‹å˜èªã‚’æŠ½å‡ºã—ã€ã©ã®ã‚ˆã†ãªè¨€è‘‰ãŒå¤šãä½¿ã‚ã‚Œã¦ã„ã‚‹ã‹å…¨ä½“åƒã‚’æŠŠæ¡ã—ã¾ã™ã€‚",
            "reason": "åŸæ–‡ãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚Šã€ã‚¿ã‚°ä»˜ã‘ä»¥å¤–ã®è¦³ç‚¹ã‹ã‚‰ã®ã‚¤ãƒ³ã‚µã‚¤ãƒˆç™ºè¦‹ã«ã€‚",
            "suitable_cols": ['ANALYSIS_TEXT_COLUMN']
        })

        # å„ªå…ˆåº¦7: å¤šå¤‰é‡è§£æ (L438ã®æŒ‡ç¤º)
        if len(numeric_cols) >= 3:
             potential_suggestions.append({
                 "priority": 7, "name": "ä¸»æˆåˆ†åˆ†æ (PCA) / å› å­åˆ†æ",
                 "description": f"è¤‡æ•°ã®æ•°å€¤ãƒ‡ãƒ¼ã‚¿({', '.join(numeric_cols)})é–“ã®ç›¸é–¢é–¢ä¿‚ã‹ã‚‰ã€èƒŒå¾Œã«ã‚ã‚‹å…±é€šã®è¦å› ï¼ˆä¸»æˆåˆ†/å› å­ï¼‰ã‚’æ¢ã‚Šã¾ã™ã€‚",
                 "reason": f"è¤‡æ•°æ•°å€¤åˆ—({len(numeric_cols)}å€‹)ãŒã‚ã‚Šã€å¤‰æ•°é–“ã®è¤‡é›‘ãªé–¢ä¿‚æ€§ã®ç¸®ç´„ã‚„è§£é‡ˆã«ã€‚",
                 "suitable_cols": numeric_cols
             })

        # å„ªå…ˆåº¦ã§ã‚½ãƒ¼ãƒˆã—ã€ä¸Šä½8ä»¶ç¨‹åº¦ã‚’è¿”ã™ (L438ã®æŒ‡ç¤º)
        suggestions = sorted(potential_suggestions, key=lambda x: x['priority'])
        logger.info(f"ææ¡ˆæ‰‹æ³•(ã‚½ãƒ¼ãƒˆå¾Œ): {[s['name'] for s in suggestions]}")
        return suggestions[:8] # ä¸Šé™ã‚’ 8 ã«å¤‰æ›´

    except Exception as e:
        logger.error(f"åˆ†ææ‰‹æ³•ææ¡ˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True); st.warning(f"åˆ†ææ‰‹æ³•ææ¡ˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
    return suggestions

def get_suggestions_from_prompt(user_prompt, df, existing_suggestions):  # llm å¼•æ•°ã‚’å‰Šé™¤ (SRP)
    """
    ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è‡ªç”±è¨˜è¿°ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã«åŸºã¥ãã€AIãŒè¿½åŠ ã®åˆ†ææ‰‹æ³•ã‚’ææ¡ˆã™ã‚‹ã€‚
    """
    logger.info("AIãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ™ãƒ¼ã‚¹ã®åˆ†æææ¡ˆã‚’é–‹å§‹...")
    llm = get_llm()  # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸLLMã‚’ç›´æ¥å‘¼ã³å‡ºã—
    if llm is None:
        logger.error("get_suggestions_from_prompt: LLM is not available.")
        return []
    
    try:
        # ( ... æ—¢å­˜ã® L439-L498 ã®ãƒ­ã‚¸ãƒƒã‚¯ (column_info_str, prompt, chain.invoke) ... )
        col_info = []
        for col in df.columns:
            col_info.append(f"- {col} (å‹: {df[col].dtype})")
        column_info_str = "\n".join(col_info)
        existing_names = [s['name'] for s in existing_suggestions]
        prompt = PromptTemplate.from_template(
            """
            ã‚ãªãŸã¯ãƒ‡ãƒ¼ã‚¿åˆ†æã®ã‚¹ã‚­ãƒ¼ãƒè¨­è¨ˆè€…ã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã€Œåˆ†ææŒ‡ç¤ºã€ã‚’è§£é‡ˆã—ã€ãã‚Œã‚’JSONãƒªã‚¹ãƒˆå½¢å¼ã®ã€Œåˆ†ææ‰‹æ³•ã€ã«å¤‰æ›ã—ã¦ãã ã•ã„ã€‚
            # ãƒ‡ãƒ¼ã‚¿æ§‹é€  (åˆ©ç”¨å¯èƒ½ãªåˆ—å):
            {column_info}
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®åˆ†ææŒ‡ç¤º (ã“ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’è§£é‡ˆå¯¾è±¡ã¨ã—ã¾ã™):
            {user_prompt}
            # æŒ‡ç¤º:
            1. ã€Œãƒ¦ãƒ¼ã‚¶ãƒ¼ã®åˆ†ææŒ‡ç¤ºã€ã«å«ã¾ã‚Œã‚‹åˆ†æé …ç›®ã‚’ã€1ã¤ãšã¤ã€‘è§£é‡ˆã—ã€ãã‚Œãã‚Œã‚’ã€Œåˆ†ææ‰‹æ³•ã€ã¨ã—ã¦å®šç¾©ã™ã‚‹ã€‚ (ä¾‹: ã€ŒæŠ•ç¨¿æ•°åˆ†æã€ã¯ã€ŒæŠ•ç¨¿æ•°åˆ†æã€ã¨ã„ã†åå‰ã®æ‰‹æ³•ã«ã™ã‚‹)
            2. å„ææ¡ˆã« `priority` (å„ªå…ˆåº¦: 6å›ºå®š), `name` (æ‰‹æ³•å), `description` (æ‰‹æ³•ã®ç°¡æ½”ãªèª¬æ˜), `reason` (ææ¡ˆç†ç”±: ã€Œãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡ç¤ºã«åŸºã¥ãã€ã¨è¨˜è¿°) ã‚’å«ã‚€JSONãƒªã‚¹ãƒˆå½¢å¼ã§å›ç­”ã™ã‚‹ã€‚(â˜…)
            3. æŒ‡ç¤ºãŒç©ºã€ã¾ãŸã¯è§£é‡ˆä¸èƒ½ãªå ´åˆã¯ã€ç©ºãƒªã‚¹ãƒˆ [] ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚
            """
        )
        chain = prompt | llm | StrOutputParser()
        response_str = chain.invoke({
            "column_info": column_info_str,
            "user_prompt": user_prompt
        })
        
        # ( ... æ—¢å­˜ã® L502-L534 ã®ãƒ­ã‚¸ãƒƒã‚¯ (ãƒ‘ãƒ¼ã‚¹å‡¦ç†) ... )
        logger.info(f"AIè¿½åŠ ææ¡ˆ(ç”Ÿ): {response_str}")
        match = re.search(r'\[.*\]', response_str, re.DOTALL)
        if not match:
            logger.warning("AIãŒJSONãƒªã‚¹ãƒˆå½¢å¼ã§å¿œç­”ã—ã¾ã›ã‚“ã§ã—ãŸã€‚")
            return []
        json_str = match.group(0)
        ai_suggestions = json.loads(json_str)
        for suggestion in ai_suggestions:
            suggestion['priority'] = 6 # ãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡ç¤ºã¯å„ªå…ˆåº¦ã‚’ä½ãè¨­å®š
        logger.info(f"AIè¿½åŠ ææ¡ˆ(ãƒ‘ãƒ¼ã‚¹æ¸ˆ): {len(ai_suggestions)}ä»¶")
        return ai_suggestions
        
    except Exception as e:
        logger.error(f"AIè¿½åŠ ææ¡ˆã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        st.warning(f"AIè¿½åŠ ææ¡ˆã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return []

# --- L468: Step B (ææ¡ˆè¡¨ç¤ºUI) ---
def display_suggestions(suggestions, df):
    """
    ææ¡ˆã•ã‚ŒãŸåˆ†ææ‰‹æ³•ã‚’è¡¨ç¤ºã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒé¸æŠã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹ (â˜… ãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ç‰ˆ)
    """
    if not suggestions:
        st.info("ææ¡ˆå¯èƒ½ãªåˆ†ææ‰‹æ³•ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return

    st.subheader("ææ¡ˆã•ã‚ŒãŸåˆ†ææ‰‹æ³•:")
    st.markdown("---")
    
    # L497 ã®ãƒ­ã‚¸ãƒƒã‚¯ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ5ä»¶é¸æŠ)
    default_selection_names = [s['name'] for s in suggestions[:min(len(suggestions), 5)]] 
    
    st.markdown("å®Ÿè¡Œã—ãŸã„åˆ†ææ‰‹æ³•ã‚’é¸æŠï¼ˆè¤‡æ•°å¯ï¼‰:")
    selected_technique_names = []
    
    for suggestion in suggestions:
        name = suggestion['name']
        is_default_checked = name in default_selection_names
        is_checked = st.checkbox(
            name, 
            value=is_default_checked, 
            key=f"cb_{name}"
        )
        if is_checked:
            selected_technique_names.append(name)
    
    # L515-L519: ä¸è¦ãªã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã‚’å‰Šé™¤ (KISS)
    
    if selected_technique_names:
        st.markdown("---")
        st.subheader("é¸æŠã•ã‚ŒãŸæ‰‹æ³•ã®è©³ç´°:")
        selected_suggestions = [s for s in suggestions if s['name'] in selected_technique_names]
        
        for suggestion in selected_suggestions:
            with st.expander(f"{suggestion['name']} (å„ªå…ˆåº¦: {suggestion['priority']})"):
                st.markdown(f"**<èª¬æ˜>**\n{suggestion['description']}")
                st.markdown(f"**<ææ¡ˆç†ç”±>**\n{suggestion['reason']}")
    
    st.markdown("---")

    # L525: ã‚­ãƒ¼åå¤‰æ›´æ¸ˆã¿ã®ãƒœã‚¿ãƒ³ (execute_button_C_v2)
    if st.button("é¸æŠã—ãŸæ‰‹æ³•ã§åˆ†æã‚’å®Ÿè¡Œ (Step Cã¸)", key="execute_button_C_v2", disabled=not selected_technique_names, type="primary"):
         if selected_technique_names:
             st.session_state.chosen_analysis_list = selected_technique_names
             st.session_state.current_step = 'C'
             st.rerun()
         else:
             st.error("åˆ†æã‚’å®Ÿè¡Œã™ã‚‹ã«ã¯ã€å°‘ãªãã¨ã‚‚1ã¤ã®æ‰‹æ³•ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")

# --- L537: Step C (AIã‚µãƒãƒªãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ) ---
# (æ—¢å­˜ã® L537-L578 (generate_ai_summary_prompt é–¢æ•°) ã‚’ãã®ã¾ã¾ã“ã“ã«è²¼ã‚Šä»˜ã‘)
def generate_ai_summary_prompt(results_dict, df):
    """
    Step C-1 ã§å¾—ã‚‰ã‚ŒãŸåˆ†æçµæœ(DataFrame)ã‚’AIç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¤‰æ›ã™ã‚‹ã€‚
    """
    logger.info("AIã‚µãƒãƒªãƒ¼ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ç”Ÿæˆé–‹å§‹...")
    if not results_dict:
        logger.warning("AIã‚µãƒãƒªãƒ¼ã®å…ƒã«ãªã‚‹åˆ†æçµæœãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return "ã‚¨ãƒ©ãƒ¼: AIã‚µãƒãƒªãƒ¼ã®å…ƒã«ãªã‚‹åˆ†æçµæœãŒã‚ã‚Šã¾ã›ã‚“ã€‚Step C-1ã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚"
    
    context_str = f"## åˆ†æå¯¾è±¡ãƒ‡ãƒ¼ã‚¿ã®æ¦‚è¦\n"
    context_str += f"- ç·è¡Œæ•°: {len(df)}\n"
    context_str += f"- åˆ—ãƒªã‚¹ãƒˆ: {', '.join(df.columns.tolist())}\n\n"
    context_str += "## å€‹åˆ¥åˆ†æã®çµæœã‚µãƒãƒªãƒ¼\n"
    context_str += "ï¼ˆæ³¨ï¼šãƒˆãƒ¼ã‚¯ãƒ³æ•°ç¯€ç´„ã®ãŸã‚ã€å„åˆ†æçµæœã¯æœ€å¤§5ä»¶ã®ã¿æŠœç²‹ã—ã¦ã„ã¾ã™ï¼‰\n\n"
    
    for name, data in results_dict.items():
        context_str += f"### {name}\n"
        if isinstance(data, (pd.DataFrame, pd.Series)):
            if data.empty:
                context_str += "(ãƒ‡ãƒ¼ã‚¿ãªã—)\n\n"
            else:
                if len(data) > 5:
                    context_str += f"ä¸Šä½5ä»¶:\n{data.head(5).to_string()}\n\n"
                else:
                    context_str += f"å…¨ä»¶:\n{data.to_string()}\n\n"
        else:
            context_str += f"{str(data)}\n\n"
    
    final_prompt = f"""
ã‚ãªãŸã¯å„ªç§€ãªãƒ‡ãƒ¼ã‚¿ã‚¢ãƒŠãƒªã‚¹ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã®ã€Œåˆ†æå¯¾è±¡ãƒ‡ãƒ¼ã‚¿ã®æ¦‚è¦ã€ã¨ã€Œå€‹åˆ¥åˆ†æã®çµæœã‚µãƒãƒªãƒ¼ã€ã‚’èª­ã¿è§£ãã€ãƒ—ãƒ­ã®è¦–ç‚¹ã‹ã‚‰ç·åˆçš„ãªã€Œåˆ†æã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã€ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
# æŒ‡ç¤º:
1. å„åˆ†æçµæœã‚’æ¨ªæ–­çš„ã«è§£é‡ˆã—ã€é‡è¦ãªã‚¤ãƒ³ã‚µã‚¤ãƒˆï¼ˆæ´å¯Ÿï¼‰ã‚’æŠ½å‡ºã™ã‚‹ã€‚
2. å˜ãªã‚‹çµæœã®ç¾…åˆ—ã§ã¯ãªãã€ãƒ“ã‚¸ãƒã‚¹ä¸Šã®ç¤ºå”†ï¼ˆä¾‹: ã©ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒé‡è¦ã‹ã€ã©ã®å±æ€§ã«æ³¨ç›®ã™ã¹ãã‹ï¼‰ã‚’å°ãå‡ºã™ã€‚
3. ãƒ¬ãƒãƒ¼ãƒˆã¯æ—¥æœ¬ã®ãƒ“ã‚¸ãƒã‚¹ãƒãƒ³å‘ã‘ã«ã€è¦‹ã‚„ã™ã„ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ï¼ˆè¦‹å‡ºã—ã€ç®‡æ¡æ›¸ãï¼‰ã§æ§‹æˆã™ã‚‹ã€‚
4. çµè«–ã‹ã‚‰å…ˆã«è¿°ã¹ã€ãã®å¾Œã«è©³ç´°ãªæ ¹æ‹ ã‚’èª¬æ˜ã™ã‚‹ã€‚
---
[åˆ†æã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ]
{context_str}
---
[ã‚ãªãŸã®å›ç­”]
# åˆ†æã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆ
"""
    logger.info("AIã‚µãƒãƒªãƒ¼ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆå®Œäº†ã€‚")
    return final_prompt

# --- L580: Step C (å¯è¦–åŒ–ãƒ˜ãƒ«ãƒ‘ãƒ¼) ---
def run_simple_count(df, flag_cols):
    """å˜ç´”é›†è¨ˆï¼ˆé »åº¦åˆ†æï¼‰ã‚’å®Ÿè¡Œã—ã€Streamlitã§å¯è¦–åŒ–ã™ã‚‹"""
    if not flag_cols:
        st.warning("é›†è¨ˆå¯¾è±¡ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ—ï¼ˆsuitable_colsï¼‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return None #
    
    col_to_analyze = st.selectbox(
        "é›†è¨ˆã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ—ã‚’é¸æŠ:", 
        flag_cols, 
        key=f"sc_select_{flag_cols[0]}"
    )
    
    if not col_to_analyze or col_to_analyze not in df.columns:
        st.error(f"åˆ— '{col_to_analyze}' ãŒãƒ‡ãƒ¼ã‚¿ã«å­˜åœ¨ã—ã¾ã›ã‚“ã€‚")
        return None #
    try:
        s = df[col_to_analyze].astype(str).str.split(', ').explode()
        s = s[s.str.strip() != ''] # ç©ºç™½ã‚’é™¤å»
        s = s.str.strip() # å‰å¾Œã®ç©ºç™½ã‚’é™¤å»
        
        if s.empty:
            st.info("é›†è¨ˆå¯¾è±¡ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            return None #
            
        counts = s.value_counts().head(20) # ä¸Šä½20ä»¶
        st.bar_chart(counts)
        with st.expander("è©³ç´°ãƒ‡ãƒ¼ã‚¿ï¼ˆä¸Šä½20ä»¶ï¼‰"):
            st.dataframe(counts)
        return counts # 
            
    except Exception as e:
        st.error(f"å˜ç´”é›†è¨ˆã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        logger.error(f"run_simple_count error: {e}", exc_info=True)
    return None #

def run_basic_stats(df, numeric_cols):
    """åŸºæœ¬çµ±è¨ˆé‡ã‚’å®Ÿè¡Œã—ã€Streamlitã§è¡¨ç¤ºã™ã‚‹"""
    if not numeric_cols:
        st.warning("é›†è¨ˆå¯¾è±¡ã®æ•°å€¤åˆ—ï¼ˆsuitable_colsï¼‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return None #
    
    existing_cols = [col for col in numeric_cols if col in df.columns]
    if not existing_cols:
        st.error("æŒ‡å®šã•ã‚ŒãŸæ•°å€¤åˆ—ãŒãƒ‡ãƒ¼ã‚¿ã«å­˜åœ¨ã—ã¾ã›ã‚“ã€‚")
        return None #
        
    stats_df = df[existing_cols].describe()
    st.dataframe(stats_df)
    return stats_df #

def run_crosstab(df, suitable_cols):
    """ã‚¯ãƒ­ã‚¹é›†è¨ˆã‚’å®Ÿè¡Œã—ã€Streamlitã§è¡¨ç¤ºã™ã‚‹"""
    if not suitable_cols or len(suitable_cols) < 2:
        st.warning("ã‚¯ãƒ­ã‚¹é›†è¨ˆã«ã¯2ã¤ä»¥ä¸Šã®åˆ—ãŒå¿…è¦ã§ã™ã€‚")
        return None #

    existing_cols = [col for col in suitable_cols if col in df.columns]
    if len(existing_cols) < 2:
        st.error(f"ãƒ‡ãƒ¼ã‚¿å†…ã«å­˜åœ¨ã™ã‚‹åˆ†æå¯¾è±¡åˆ—ãŒ2ã¤æœªæº€ã§ã™: {existing_cols}")
        return None #

    st.info(f"åˆ†æå¯èƒ½ãªåˆ—: {', '.join(existing_cols)}")
    
    key_base = suitable_cols[0]
    col1 = st.selectbox("è¡Œ (Index) ã«è¨­å®šã™ã‚‹åˆ—:", existing_cols, key=f"ct_idx_{key_base}")
    
    options_col2 = [c for c in existing_cols if c != col1]
    if not options_col2:
        st.error("2ã¤ç›®ã®åˆ—ã‚’é¸æŠã§ãã¾ã›ã‚“ã€‚")
        return None #
        
    col2 = st.selectbox("åˆ— (Column) ã«è¨­å®šã™ã‚‹åˆ—:", options_col2, key=f"ct_col_{key_base}")

    if not col1 or not col2:
        return None #

    try:
        crosstab_df = pd.crosstab(df[col1].astype(str), df[col2].astype(str))
        
        if crosstab_df.empty:
            st.info("ã‚¯ãƒ­ã‚¹é›†è¨ˆã®çµæœã€ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            return None
        
        st.dataframe(crosstab_df)
        
        if st.checkbox("ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã§è¡¨ç¤º", key=f"ct_heatmap_{key_base}"):    
            return crosstab_df # 
    except Exception as e:
        st.error(f"ã‚¯ãƒ­ã‚¹é›†è¨ˆã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        logger.error(f"run_crosstab error: {e}", exc_info=True)
    return None #

def run_timeseries(df, suitable_cols_dict):
    """æ™‚ç³»åˆ—åˆ†æã‚’å®Ÿè¡Œã—ã€Streamlitã§å¯è¦–åŒ–ã™ã‚‹"""
    if not isinstance(suitable_cols_dict, dict) or 'datetime' not in suitable_cols_dict or 'keywords' not in suitable_cols_dict:
        st.warning("æ™‚ç³»åˆ—åˆ†æã®ãŸã‚ã®åˆ—æƒ…å ±ï¼ˆdatetime, keywordsï¼‰ãŒä¸ååˆ†ã§ã™ã€‚")
        return None #
        
    dt_cols = [col for col in suitable_cols_dict['datetime'] if col in df.columns]
    kw_cols = [col for col in suitable_cols_dict['keywords'] if col in df.columns]

    if not dt_cols: st.error("æ—¥æ™‚åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"); return None #
    if not kw_cols: st.error("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"); return None #

    key_base = dt_cols[0]
    dt_col = st.selectbox("ä½¿ç”¨ã™ã‚‹æ—¥æ™‚åˆ—:", dt_cols, key=f"ts_dt_{key_base}")
    kw_col = st.selectbox("é›†è¨ˆã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ—:", kw_cols, key=f"ts_kw_{key_base}")

    if not dt_col or not kw_col:
        return None #

    try:
        df_copy = df[[dt_col, kw_col]].copy()
        
        df_copy[dt_col] = pd.to_datetime(df_copy[dt_col], errors='coerce')
        df_copy = df_copy.dropna(subset=[dt_col])
        if df_copy.empty: st.info("æœ‰åŠ¹ãªæ—¥æ™‚ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚"); return None #

        df_copy[kw_col] = df_copy[kw_col].astype(str)
        df_copy = df_copy[df_copy[kw_col].str.strip() != ''] 
        if df_copy.empty: st.info(f"ã€Œ{kw_col}ã€ã«æœ‰åŠ¹ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"); return None #

        time_df = df_copy.set_index(dt_col).resample('D').size().rename("æŠ•ç¨¿æ•°")
        
        if time_df.empty: st.info("æ™‚ç³»åˆ—é›†è¨ˆã®çµæœã€ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"); return None #
        
        time_df.index.name = "æ—¥æ™‚"
        
        st.line_chart(time_df)
        with st.expander("è©³ç´°ãƒ‡ãƒ¼ã‚¿"):
            st.dataframe(time_df)
        
        return time_df # 
            
    except Exception as e:
        st.error(f"æ™‚ç³»åˆ—åˆ†æã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        logger.error(f"run_timeseries error: {e}", exc_info=True)
    return None #

def run_text_mining(df, text_col='ANALYSIS_TEXT_COLUMN'):
    """
    spaCyã‚’ä½¿ç”¨ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°ï¼ˆé »å‡ºå˜èªåˆ†æï¼‰ã‚’å®Ÿè¡Œã—ã€å¯è¦–åŒ–ã™ã‚‹ã€‚
    APIã¯ä½¿ç”¨ã—ãªã„ã€‚
    """
    if text_col not in df.columns or df[text_col].empty:
        st.warning(f"åˆ†æå¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆåˆ— '{text_col}' ãŒãªã„ã‹ã€ç©ºã§ã™ã€‚")
        return None #

    nlp = load_spacy_model() # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’ç›´æ¥å‘¼ã³å‡ºã—
    if nlp is None:
        st.error("spaCyæ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
        return None
            
    st.info("ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°å‡¦ç†ä¸­ï¼ˆãƒ‡ãƒ¼ã‚¿é‡ã«ã‚ˆã£ã¦æ™‚é–“ãŒã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ï¼‰...")

    try:
        texts = df[text_col].dropna().astype(str)
        if texts.empty:
            st.warning("åˆ†æå¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            return None #
            
        words = []
        target_pos = {'NOUN', 'PROPN', 'ADJ'}
        stop_words = {
            'ã®', 'ã«', 'ã¯', 'ã‚’', 'ãŒ', 'ã§', 'ã¦', 'ã§ã™', 'ã¾ã™', 'ã“ã¨', 'ã‚‚ã®', 'ãã‚Œ', 'ã‚ã‚Œ',
            'ã“ã‚Œ', 'ãŸã‚', 'ã„ã‚‹', 'ã™ã‚‹', 'ã‚ã‚‹', 'ãªã„', 'ã„ã†', 'ã‚ˆã†', 'ãã†', 'ãªã©', 'ã•ã‚“',
            'çš„', 'çš„', 'çš„', 'çš„', 'äºº', 'è‡ªåˆ†', 'ç§', 'åƒ•', 'ä½•', 'ãã®', 'ã“ã®', 'ã‚ã®'
        }
        for doc in nlp.pipe(texts, disable=["parser", "ner"]):
            for token in doc:
                if (token.pos_ in target_pos) and (not token.is_stop) and (token.lemma_ not in stop_words) and (len(token.lemma_) > 1):
                    words.append(token.lemma_)

        if not words:
            st.warning("æŠ½å‡ºå¯èƒ½ãªæœ‰åŠ¹ãªå˜èªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            return None #

        word_counts = pd.Series(words).value_counts().head(30) # ä¸Šä½30ä»¶

        st.subheader("é »å‡ºå˜èª Top 30")
        st.bar_chart(word_counts)
        with st.expander("è©³ç´°ãƒ‡ãƒ¼ã‚¿ï¼ˆTop 30ï¼‰"):
            st.dataframe(word_counts.reset_index(name="å‡ºç¾å›æ•°").rename(columns={"index": "å˜èª"}))

        # L727: é‡è¤‡ã—ãŸ dataframe å‘¼ã³å‡ºã—ã‚’å‰Šé™¤ (KISS)
        
        return word_counts # 
    except Exception as e:
        st.error(f"ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        logger.error(f"run_text_mining error: {e}", exc_info=True)
    return None #

# --- L752: Part 2 (renderé–¢æ•°, main) ã¯æ¬¡ã®ãƒãƒ£ãƒƒãƒˆã§ææ¡ˆã—ã¾ã™ ---
# --- L752: UIæ›´æ–°ãƒ˜ãƒ«ãƒ‘ãƒ¼ (DRYåŸå‰‡) ---
def update_progress_ui(progress_placeholder, log_placeholder, processed_rows, total_rows, message_prefix):
    """
    Step A ã®é€²æ—ãƒãƒ¼ã¨ãƒ­ã‚°ã‚¨ãƒªã‚¢ã‚’æ›´æ–°ã™ã‚‹ (DRY)
    """
    try:
        progress_percent = min(processed_rows / total_rows, 1.0)
        progress_text = f"[{message_prefix}] å‡¦ç†ä¸­: {processed_rows}/{total_rows} ä»¶ ({progress_percent:.0%})"
        progress_placeholder.progress(progress_percent, text=progress_text)
        
        # ãƒ­ã‚°è¡¨ç¤º (æœ€æ–°50ä»¶)
        log_text_for_ui = "\n".join(st.session_state.log_messages[-50:])
        log_placeholder.text_area("å®Ÿè¡Œãƒ­ã‚° (æœ€æ–°50ä»¶):", log_text_for_ui, height=200, key=f"log_update_{message_prefix}_{processed_rows}", disabled=True)
    except Exception as e:
        logger.warning(f"UI update failed: {e}") # UIã‚¨ãƒ©ãƒ¼ã¯å‡¦ç†ã‚’æ­¢ã‚ãªã„

# --- L752: Step A (ã‚¿ã‚°ä»˜ã‘UI) ---
def render_step_a():
    """Step A: ã‚¿ã‚°ä»˜ã‘å‡¦ç†ã®UIã‚’æç”»ã™ã‚‹"""
    st.title("ğŸ·ï¸ ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®AIã‚¿ã‚°ä»˜ã‘ (Step A)")

    # Step A å›ºæœ‰ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã‚’ã“ã“ã§åˆæœŸåŒ– (SRP)
    if 'cancel_analysis' not in st.session_state: st.session_state.cancel_analysis = False
    if 'generated_categories' not in st.session_state: st.session_state.generated_categories = {}
    if 'selected_categories' not in st.session_state: st.session_state.selected_categories = set()
    if 'api_key_A' not in st.session_state: st.session_state.api_key_A = "" # L1096 (æ—§ L1383) ã‹ã‚‰ç§»å‹•
    if 'analysis_prompt_A' not in st.session_state: st.session_state.analysis_prompt_A = "" # L1092 (æ—§ L1379) ã‹ã‚‰ç§»å‹•
    if 'selected_text_col' not in st.session_state: st.session_state.selected_text_col = {} # L1094 (æ—§ L1381) ã‹ã‚‰ç§»å‹•
    if 'tagged_df_A' not in st.session_state: st.session_state.tagged_df_A = pd.DataFrame() # L1090 (æ—§ L1377) ã‹ã‚‰ç§»å‹•

    # L754-L757: ä¸è¦ãªã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã‚’å‰Šé™¤ (KISS)
    
    st.header("Step 1: åˆ†æå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
    uploaded_files = st.file_uploader("åˆ†æã—ãŸã„ Excel / CSV ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆè¤‡æ•°å¯ï¼‰", type=['csv', 'xlsx', 'xls'], accept_multiple_files=True, key="uploader_A")
    
    if not uploaded_files:
        st.info("åˆ†æã‚’é–‹å§‹ã™ã‚‹ã«ã¯ã€Excelã¾ãŸã¯CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
        return # ãƒ•ã‚¡ã‚¤ãƒ«ãŒãªã‘ã‚Œã°ã“ã“ã§çµ‚äº† (KISS)
    
    valid_files_data = {}
    error_messages = []
    for f in uploaded_files:
        df, err = read_file(f)
        if err: error_messages.append(f"**{f.name}**: {err}")
        else: valid_files_data[f.name] = df
    if error_messages: st.error("ä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯èª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸ:\n" + "\n".join(error_messages))
    if not valid_files_data: st.warning("èª­ã¿è¾¼ã¿å¯èƒ½ãªãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“ã€‚"); return

    st.header("Step 2: åˆ†ææŒ‡é‡ã®å…¥åŠ›")
    analysis_prompt = st.text_area(
        "AIãŒã‚¿ã‚°ä»˜ã‘ã¨ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°ã‚’è¡Œã†éš›ã®æŒ‡é‡ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆå¿…é ˆï¼‰:",
        value=st.session_state.analysis_prompt_A,
        height=100,
        placeholder="ä¾‹: åºƒå³¶çœŒã®è¦³å…‰ã«é–¢ã™ã‚‹Instagramã®æŠ•ç¨¿ã€‚ç„¡é–¢ä¿‚ãªåœ°åŸŸã®æŠ•ç¨¿ã‚„ã€å˜ãªã‚‹æŒ¨æ‹¶ãƒ»å®£ä¼ã¯é™¤å¤–ã—ãŸã„ã€‚",
        key="analysis_prompt_input_A"
    )
    st.session_state.analysis_prompt_A = analysis_prompt # L781: ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ä¿å­˜

    if not analysis_prompt.strip():
        st.warning("åˆ†ææŒ‡é‡ã¯å¿…é ˆã§ã™ã€‚AIãŒãƒ‡ãƒ¼ã‚¿ã‚’ç†è§£ã™ã‚‹ãŸã‚ã«ç›®çš„ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        return # æŒ‡é‡ãŒãªã‘ã‚Œã°ã“ã“ã§çµ‚äº† (KISS)

    st.header("Step 3: AIã«ã‚ˆã‚‹ã‚«ãƒ†ã‚´ãƒªå€™è£œã®ç”Ÿæˆ")
    if st.button("AIã«ã‚«ãƒ†ã‚´ãƒªå€™è£œã‚’ç”Ÿæˆã•ã›ã‚‹", key="gen_cat_button", type="primary"):
        if not os.getenv("GOOGLE_API_KEY"):
            st.error("Google APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ï¼ˆã‚µã‚¤ãƒ‰ãƒãƒ¼ã§è¨­å®šã—ã¦ãã ã•ã„ï¼‰")
        else:
            with st.spinner("AIãŒåˆ†ææŒ‡é‡ã‚’èª­ã¿è§£ãã€ã‚«ãƒ†ã‚´ãƒªã‚’è€ƒæ¡ˆä¸­..."):
                logger.info("AIã‚«ãƒ†ã‚´ãƒªç”Ÿæˆãƒœã‚¿ãƒ³ã‚¯ãƒªãƒƒã‚¯")
                st.session_state.generated_categories = {"å¸‚åŒºç”ºæ‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰": "åœ°åè¾æ›¸(JAPAN_GEOGRAPHY_DB)ã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸå¸‚åŒºç”ºæ‘å"}
                # L796: ã‚­ãƒ£ãƒƒã‚·ãƒ¥åˆ©ç”¨ç‰ˆ (llmå¼•æ•°å‰Šé™¤)
                ai_categories = get_dynamic_categories(analysis_prompt) 
                if ai_categories:
                    st.session_state.generated_categories.update(ai_categories)
                    logger.info(f"AIã‚«ãƒ†ã‚´ãƒªç”ŸæˆæˆåŠŸ: {list(ai_categories.keys())}")
                    st.success("AIã«ã‚ˆã‚‹ã‚«ãƒ†ã‚´ãƒªå€™è£œã®ç”ŸæˆãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
                else:
                    st.error("AIã«ã‚ˆã‚‹ã‚«ãƒ†ã‚´ãƒªç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚AIã®å¿œç­”ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

    st.header("Step 4: åˆ†æã‚«ãƒ†ã‚´ãƒªã®é¸æŠ")
    if not st.session_state.generated_categories:
        st.info("Step 3 ã§ã‚«ãƒ†ã‚´ãƒªã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚")
        return
    st.markdown("ã‚¿ã‚°ä»˜ã‘ã—ãŸã„ã‚«ãƒ†ã‚´ãƒªã‚’ä»¥ä¸‹ã‹ã‚‰é¸æŠã—ã¦ãã ã•ã„ï¼ˆã€Œå¸‚åŒºç”ºæ‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€ã¯å¿…é ˆã§ã™ï¼‰")
    selected_cats = []
    cols = st.columns(3)
    categories_to_show = st.session_state.generated_categories.items()
    for i, (cat, desc) in enumerate(categories_to_show):
        with cols[i % 3]:
            is_checked = st.checkbox(
                cat, 
                value=(cat == "å¸‚åŒºç”ºæ‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰" or cat in st.session_state.selected_categories), 
                help=desc, 
                key=f"cat_cb_{cat}",
                disabled=(cat == "å¸‚åŒºç”ºæ‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰") # å¿…é ˆé …ç›®ã¯ç„¡åŠ¹åŒ–
            )
            if is_checked:
                selected_cats.append(cat)
    st.session_state.selected_categories = set(selected_cats)

    st.header("Step 5: åˆ†æå¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆåˆ—ã®æŒ‡å®š")
    selected_text_col_map = {}
    st.markdown("ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã«ã€ã‚¿ã‚°ä»˜ã‘å¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆãŒå«ã¾ã‚Œã‚‹åˆ—ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚")
    for f_name, df in valid_files_data.items():
        cols_list = list(df.columns)
        default_index = 0
        if st.session_state.selected_text_col.get(f_name) in cols_list:
            default_index = cols_list.index(st.session_state.selected_text_col.get(f_name))
        elif 'ANALYSIS_TEXT_COLUMN' in cols_list:
             default_index = cols_list.index('ANALYSIS_TEXT_COLUMN')
        selected_col = st.selectbox(f"**{f_name}** ã®ãƒ†ã‚­ã‚¹ãƒˆåˆ—:", cols_list, index=default_index, key=f"col_select_{f_name}")
        selected_text_col_map[f_name] = selected_col
    st.session_state.selected_text_col = selected_text_col_map

    st.header("Step 6: åˆ†æå®Ÿè¡Œ")
    if st.button("ã‚­ãƒ£ãƒ³ã‚»ãƒ«", key="cancel_button_A"):
        st.session_state.cancel_analysis = True
        logger.warning("åˆ†æã‚­ãƒ£ãƒ³ã‚»ãƒ«ãƒœã‚¿ãƒ³ãŒæŠ¼ã•ã‚Œã¾ã—ãŸã€‚")
        st.warning("æ¬¡ã®ãƒãƒƒãƒå‡¦ç†å¾Œã«åˆ†æã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã—ã¾ã™...")
        
    if st.button("åˆ†æå®Ÿè¡Œ", type="primary", key="run_analysis_A"):
        st.session_state.cancel_analysis = False
        st.session_state.log_messages = [] # ãƒ­ã‚°ãƒªã‚»ãƒƒãƒˆ
        st.session_state.tagged_df_A = pd.DataFrame() # çµæœãƒªã‚»ãƒƒãƒˆ
        
        try:
            with st.spinner("Step A: AIåˆ†æå‡¦ç†ä¸­..."):
                logger.info("Step A åˆ†æå®Ÿè¡Œãƒœã‚¿ãƒ³ã‚¯ãƒªãƒƒã‚¯")
                progress_placeholder = st.progress(0.0, text="å‡¦ç†å¾…æ©Ÿä¸­...")
                log_placeholder = st.empty()
                
                temp_dfs = []
                for f_name, df in valid_files_data.items():
                    col_name = selected_text_col_map[f_name]
                    temp_df = df.rename(columns={col_name: 'ANALYSIS_TEXT_COLUMN'})
                    temp_dfs.append(temp_df)
                
                logger.info(f"{len(temp_dfs)} å€‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’çµåˆ..."); 
                master_df = pd.concat(temp_dfs, ignore_index=True, sort=False); 
                master_df['id'] = master_df.index; 
                total_rows = len(master_df); 
                logger.info(f"çµåˆå®Œäº†ã€‚ç·è¡Œæ•°: {total_rows}")
                if master_df.empty: logger.error("çµåˆå¾ŒDFç©º"); raise Exception("åˆ†æå¯¾è±¡ãƒ‡ãƒ¼ã‚¿ç©º")

                logger.info("Step A-2: é‡è¤‡å‰Šé™¤ é–‹å§‹...")
                initial_row_count = len(master_df)
                master_df.drop_duplicates(subset=['ANALYSIS_TEXT_COLUMN'], keep='first', inplace=True)
                deduped_row_count = len(master_df)
                logger.info(f"é‡è¤‡å‰Šé™¤ å®Œäº†ã€‚ {initial_row_count}è¡Œ -> {deduped_row_count}è¡Œ ({initial_row_count - deduped_row_count}è¡Œå‰Šé™¤)")
                
                logger.info("Step A-3: AIé–¢é€£æ€§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° é–‹å§‹...")
                total_filter_rows = len(master_df)
                total_filter_batches = (total_filter_rows + FILTER_BATCH_SIZE - 1) // FILTER_BATCH_SIZE
                all_filtered_results = []
                
                for i in range(0, total_filter_rows, FILTER_BATCH_SIZE): # L1033: å®šæ•°
                    if st.session_state.cancel_analysis: logger.warning(f"ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ£ãƒ³ã‚»ãƒ« (ãƒãƒƒãƒ {i//FILTER_BATCH_SIZE + 1})"); st.warning("åˆ†æã‚­ãƒ£ãƒ³ã‚»ãƒ«"); break
                    
                    batch_df = master_df.iloc[i:i+FILTER_BATCH_SIZE] # L1036: å®šæ•°
                    current_batch_num = i // FILTER_BATCH_SIZE + 1 # L1037: å®šæ•°
                    logger.info(f"AIãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° ãƒãƒƒãƒ {current_batch_num}/{total_filter_batches} å‡¦ç†ä¸­...")
                    
                    # L1048: UIæ›´æ–°ã‚’ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ã§å‘¼ã³å‡ºã— (DRY)
                    update_progress_ui(
                        progress_placeholder, log_placeholder, 
                        min(i + FILTER_BATCH_SIZE, total_filter_rows), total_filter_rows, 
                        "AIãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"
                    )
                    
                    # L1053: ã‚­ãƒ£ãƒƒã‚·ãƒ¥åˆ©ç”¨ç‰ˆ (llmå¼•æ•°å‰Šé™¤)
                    filtered_df = filter_relevant_data_by_ai(batch_df, analysis_prompt)
                    if filtered_df is not None and not filtered_df.empty:
                        all_filtered_results.append(filtered_df)
                    else:
                        logger.warning(f"AIãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° ãƒãƒƒãƒ {current_batch_num} çµæœç©º")
                        
                    time.sleep(FILTER_SLEEP_TIME) # L1060: å®šæ•°
                
                if st.session_state.cancel_analysis:
                    logger.warning("AIãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å‡¦ç†ãŒã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸã€‚")
                    raise Exception("åˆ†æãŒã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ") 
                if not all_filtered_results:
                    logger.error("å…¨ãƒãƒƒãƒAIãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¤±æ•—"); raise Exception("AIãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å‡¦ç†å¤±æ•—")
                logger.info("å…¨AIãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°çµæœçµåˆ...");
                filter_results_df = pd.concat(all_filtered_results, ignore_index=True)
                relevant_ids = filter_results_df[filter_results_df['relevant'] == True]['id']
                filtered_master_df = master_df[master_df['id'].isin(relevant_ids)].copy()
                filtered_row_count = len(filtered_master_df)
                logger.info(f"AIãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° å®Œäº†ã€‚ {deduped_row_count}è¡Œ -> {filtered_row_count}è¡Œ ({deduped_row_count - filtered_row_count}è¡Œå‰Šé™¤)")
                if filtered_master_df.empty:
                    logger.error("AIãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã€ãƒ‡ãƒ¼ã‚¿ãŒ0ä»¶ã«ãªã‚Šã¾ã—ãŸã€‚"); raise Exception("åˆ†æå¯¾è±¡ãƒ‡ãƒ¼ã‚¿ç©º")
                
                logger.info("Step A-4: AIã‚¿ã‚°ä»˜ã‘å‡¦ç†é–‹å§‹..."); 
                selected_category_definitions = { cat: desc for cat, desc in st.session_state.generated_categories.items() if cat in st.session_state.selected_categories }; 
                logger.info(f"é¸æŠã‚«ãƒ†ã‚´ãƒª: {list(selected_category_definitions.keys())}")
                
                master_df_for_tagging = filtered_master_df
                total_rows = len(master_df_for_tagging) # L1082: ç·è¡Œæ•°ã‚’æ›´æ–°
                
                all_tagged_results = []; 
                total_batches = (total_rows + TAGGING_BATCH_SIZE - 1) // TAGGING_BATCH_SIZE; 
                logger.info(f"ãƒãƒƒãƒã‚µã‚¤ã‚º {TAGGING_BATCH_SIZE}, ç·ãƒãƒƒãƒæ•°: {total_batches}")
                
                for i in range(0, total_rows, TAGGING_BATCH_SIZE): # L1085: å®šæ•°
                    if st.session_state.cancel_analysis: logger.warning(f"ãƒ«ãƒ¼ãƒ—ã‚­ãƒ£ãƒ³ã‚»ãƒ« (ãƒãƒƒãƒ {i//TAGGING_BATCH_SIZE + 1})"); st.warning("åˆ†æã‚­ãƒ£ãƒ³ã‚»ãƒ«"); break
                    
                    batch_df = master_df_for_tagging.iloc[i:i+TAGGING_BATCH_SIZE]; # L1088: å®šæ•°
                    current_batch_num = i // TAGGING_BATCH_SIZE + 1; 
                    logger.info(f"ãƒãƒƒãƒ {current_batch_num}/{total_batches} å‡¦ç†ä¸­...")
                    
                    # L1089: UIæ›´æ–°ã‚’ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ã§å‘¼ã³å‡ºã— (DRY)
                    update_progress_ui(
                        progress_placeholder, log_placeholder, 
                        min(i + TAGGING_BATCH_SIZE, total_rows), total_rows, 
                        "AIã‚¿ã‚°ä»˜ã‘"
                    )
                    
                    logger.info(f"Calling perform_ai_tagging batch {current_batch_num}...")
                    # L1094: ã‚­ãƒ£ãƒƒã‚·ãƒ¥åˆ©ç”¨ç‰ˆ (llmå¼•æ•°å‰Šé™¤)
                    tagged_df = perform_ai_tagging(batch_df, selected_category_definitions, analysis_prompt)
                    logger.info(f"Finished perform_ai_tagging batch {current_batch_num}.")
                    if tagged_df is not None and not tagged_df.empty: all_tagged_results.append(tagged_df)
                    
                    time.sleep(TAGGING_SLEEP_TIME) # L1098: å®šæ•°
                
                if st.session_state.cancel_analysis:
                    logger.warning("AIã‚¿ã‚°ä»˜ã‘å‡¦ç†ãŒã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸã€‚")
                    raise Exception("åˆ†æãŒã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ")
                if not all_tagged_results: logger.error("å…¨ãƒãƒƒãƒAIã‚¿ã‚°ä»˜ã‘å¤±æ•—"); raise Exception("AIã‚¿ã‚°ä»˜ã‘å‡¦ç†å¤±æ•—")
                
                logger.info("å…¨AIã‚¿ã‚°ä»˜ã‘çµæœçµåˆ..."); 
                tagged_results_df = pd.concat(all_tagged_results, ignore_index=True)
                
                logger.info("æœ€çµ‚ãƒãƒ¼ã‚¸å‡¦ç†é–‹å§‹..."); 
                cols_to_drop_from_master = [col for col in tagged_results_df.columns if col in master_df_for_tagging.columns and col != 'id']
                if cols_to_drop_from_master: 
                    logger.warning(f"é‡è¤‡åˆ—å‰Šé™¤: {cols_to_drop_from_master}"); 
                    master_df_for_merge = master_df_for_tagging.drop(columns=cols_to_drop_from_master)
                else: 
                    master_df_for_merge = master_df_for_tagging
                
                final_df = pd.merge(master_df_for_merge, tagged_results_df, on='id', how='right')
                st.session_state.tagged_df_A = final_df
                logger.info("åˆ†æå‡¦ç† æ­£å¸¸çµ‚äº†"); 
                st.success("AIã«ã‚ˆã‚‹åˆ†æå‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚"); 
                progress_placeholder.progress(1.0, text="å‡¦ç†å®Œäº†")
                log_text_for_ui = "\n".join(st.session_state.log_messages)
                log_placeholder.text_area("å®Ÿè¡Œãƒ­ã‚°:", log_text_for_ui, height=200, key=f"log_update_A_final", disabled=True)
                
        except Exception as e:
            logger.error(f"Step A åˆ†æå®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            st.error(f"åˆ†æå®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            if 'progress_placeholder' in locals():
                progress_placeholder.progress(1.0, text="ã‚¨ãƒ©ãƒ¼ã«ã‚ˆã‚Šå‡¦ç†ä¸­æ–­")
    
    if st.session_state.cancel_analysis:
        st.session_state.cancel_analysis = False # L1126: çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆ
    
    if not st.session_state.tagged_df_A.empty:
        st.header("Step 7: åˆ†æçµæœã®ç¢ºèªã¨ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ")
        st.dataframe(st.session_state.tagged_df_A.head(50))
        
        @st.cache_data
        def convert_df_to_csv(df):
            return df.to_csv(encoding="utf-8-sig", index=False).encode("utf-8-sig")
        csv_data = convert_df_to_csv(st.session_state.tagged_df_A)
        st.download_button(
            label="åˆ†æçµæœCSVã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            data=csv_data,
            file_name="keyword_extraction_result.csv",
            mime="text/csv",
        )

# --- L833: Step C (å¯è¦–åŒ–UI) ---
def render_step_c():
    """Step C: åˆ†æçµæœã®å¯è¦–åŒ–ã‚’æç”»ã™ã‚‹"""
    st.title("ğŸ”¬ åˆ†æçµæœã®å¯è¦–åŒ– (Step C)")
    
    # Step C å›ºæœ‰ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã‚’ã“ã“ã§åˆæœŸåŒ– (SRP)
    if 'step_c_results' not in st.session_state: st.session_state.step_c_results = {}
    if 'ai_summary_prompt' not in st.session_state: st.session_state.ai_summary_prompt = None
    if 'ai_summary_result' not in st.session_state: st.session_state.ai_summary_result = None

    if 'chosen_analysis_list' not in st.session_state or not st.session_state.chosen_analysis_list:
        st.warning("å®Ÿè¡Œã™ã‚‹åˆ†æãŒé¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚Step Bã«æˆ»ã£ã¦ãã ã•ã„ã€‚")
        if st.button("Step B ã«æˆ»ã‚‹"):
            st.session_state.current_step = 'B'; st.rerun()
        return

    if 'df_flagged_B' not in st.session_state or st.session_state.df_flagged_B.empty:
        st.warning("åˆ†æå¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚Step Bã§CSVã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
        if st.button("Step B ã«æˆ»ã‚‹"):
            st.session_state.current_step = 'B'; st.rerun()
        return
        
    if 'suggestions_B' not in st.session_state or not st.session_state.suggestions_B:
        st.warning("åˆ†ææ‰‹æ³•ã®ææ¡ˆãƒªã‚¹ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚Step Bã§å†ææ¡ˆã—ã¦ãã ã•ã„ã€‚")
        if st.button("Step B ã«æˆ»ã‚‹"):
            st.session_state.current_step = 'B'; st.rerun()
        return

    df = st.session_state.df_flagged_B
    selected_names = st.session_state.chosen_analysis_list
    all_suggestions = st.session_state.suggestions_B
    
    analyses_to_run = [s for s in all_suggestions if s['name'] in selected_names]
    
    st.info(f"**å®Ÿè¡Œã™ã‚‹åˆ†æ:** {', '.join(selected_names)}")
    st.markdown("---")

    st.session_state.step_c_results = {}
    
    for suggestion in analyses_to_run:
        name = suggestion['name']
        cols = suggestion.get('suitable_cols', []) 
        
        with st.container(border=True):
            st.subheader(f"ğŸ“ˆ åˆ†æçµæœ: {name}")
            
            try:
                result_data = None # çµæœæ ¼ç´ç”¨
                if name == "å˜ç´”é›†è¨ˆï¼ˆé »åº¦åˆ†æï¼‰":
                    result_data = run_simple_count(df, cols) 
                elif name == "åŸºæœ¬çµ±è¨ˆé‡":
                    result_data = run_basic_stats(df, cols) 
                elif name == "ã‚¯ãƒ­ã‚¹é›†è¨ˆï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é–“ï¼‰":
                    result_data = run_crosstab(df, cols) 
                elif name == "ã‚¯ãƒ­ã‚¹é›†è¨ˆï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰Ã—å±æ€§ï¼‰":
                    result_data = run_crosstab(df, cols)
                elif name == "å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆ†æ":
                    st.warning("ã€Œå…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆ†æã€ã¯ç¾åœ¨å®Ÿè£…ä¸­ã§ã™ã€‚") #
                elif name == "ã‚«ãƒ†ã‚´ãƒªåˆ¥é›†è¨ˆï¼ˆã‚°ãƒ«ãƒ¼ãƒ—æ¯”è¼ƒï¼‰":
                    if isinstance(cols, dict) and 'numeric' in cols and 'grouping' in cols:
                         grouping_cols = cols['grouping']
                         numeric_cols_to_desc = [col for col in cols['numeric'] if col in df.columns]
                         
                         if not numeric_cols_to_desc: st.warning("åˆ†æå¯¾è±¡ã®æ•°å€¤åˆ—ãŒãƒ‡ãƒ¼ã‚¿ã«ã‚ã‚Šã¾ã›ã‚“ã€‚")
                         elif not grouping_cols: st.warning("åˆ†æå¯¾è±¡ã®ã‚°ãƒ«ãƒ¼ãƒ—åˆ—ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
                         else:
                             if not isinstance(grouping_cols, list):
                                 grouping_cols = [grouping_cols]
                             existing_grouping_cols = [col for col in grouping_cols if col in df.columns]
                             if not existing_grouping_cols:
                                 st.warning(f"ã‚°ãƒ«ãƒ¼ãƒ—åŒ–åˆ— {grouping_cols} ãŒãƒ‡ãƒ¼ã‚¿ã«å­˜åœ¨ã—ã¾ã›ã‚“ã€‚")
                             else:
                                 try:
                                     df_copy = df.copy()
                                     for col in existing_grouping_cols:
                                         df_copy[col] = df_copy[col].astype(str)
                                         
                                     # L874: è‡´å‘½çš„ãƒã‚° (NameError) ä¿®æ­£
                                     # L874 (æ—§) ã‚’ L871 ã®å‰ã«ç§»å‹•
                                     result_df = df_copy.groupby(existing_grouping_cols)[numeric_cols_to_desc].describe()
                                     
                                     flat_cols = []
                                     for col in result_df.columns:
                                         flat_cols.append(f"{col[0]}_{col[1]}") 
                                     result_df.columns = flat_cols
                                     
                                     final_result_df = result_df.reset_index()
                                     st.dataframe(final_result_df) 
                                     result_data = final_result_df # 
                                 except Exception as group_e:
                                     st.error(f"ã‚°ãƒ«ãƒ¼ãƒ—åˆ¥é›†è¨ˆã‚¨ãƒ©ãƒ¼: {group_e}")
                                     logger.error(f"Groupby describe error: {group_e}", exc_info=True)
                    else:
                         st.warning(f"ã€Œ{name}ã€ã®åˆ—å®šç¾©ãŒä¸é©åˆ‡ã§ã™: {cols}")
                elif name == "æ™‚ç³»åˆ—ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æ":
                    if isinstance(cols, dict) and 'datetime' in cols and 'keywords' in cols:
                        result_data = run_timeseries(df, cols)
                    else:
                         st.warning(f"ã€Œ{name}ã€ã®åˆ—å®šç¾©ãŒä¸é©åˆ‡ã§ã™: {cols}")
                elif name == "ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°ï¼ˆé »å‡ºå˜èªãªã©ï¼‰":
                    if cols and isinstance(cols, list) and cols[0] == 'ANALYSIS_TEXT_COLUMN':
                        result_data = run_text_mining(df, 'ANALYSIS_TEXT_COLUMN')
                    else:
                        st.warning(f"ã€Œ{name}ã€ã®åˆ—å®šç¾©ãŒä¸é©åˆ‡ã§ã™: {cols}")
                elif name == "ä¸»æˆåˆ†åˆ†æ (PCA) / å› å­åˆ†æ":
                    st.warning("ã€Œä¸»æˆåˆ†åˆ†æã€ã¯ç¾åœ¨å®Ÿè£…ä¸­ã§ã™ã€‚")
                else:
                    st.warning(f"ã€Œ{name}ã€ã®å¯è¦–åŒ–ãƒ­ã‚¸ãƒƒã‚¯ã¯ã¾ã å®Ÿè£…ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
                
                if result_data is not None and not result_data.empty:
                    st.session_state.step_c_results[name] = result_data
            
            except Exception as e:
                st.error(f"ã€Œ{name}ã€ã®åˆ†æä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                logger.error(f"Step C Analysis Error ({name}): {e}", exc_info=True)

    st.markdown("---")
    st.success("Step C-1 (å¯è¦–åŒ–) ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
    
    st.header("Step C-2: AIã«ã‚ˆã‚‹åˆ†æã‚µãƒãƒªãƒ¼")
    
    if not st.session_state.step_c_results:
        st.warning("AIã‚µãƒãƒªãƒ¼ã®å…ƒã«ãªã‚‹åˆ†æçµæœãŒã‚ã‚Šã¾ã›ã‚“ã€‚Step C-1ã§æœ‰åŠ¹ãªåˆ†æã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
    else:
        st.info("ä¸Šè¨˜ã§å®Ÿè¡Œã•ã‚ŒãŸåˆ†æçµæœã‚’AIã«å…¥åŠ›ã—ã€ç·åˆçš„ãªã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¾ã™ã€‚")

        if st.button("ğŸ¤– AIã‚µãƒãƒªãƒ¼ç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ", key="gen_prompt_c2"):
            st.session_state.ai_summary_prompt = generate_ai_summary_prompt(st.session_state.step_c_results, df)
            st.session_state.ai_summary_result = None 
            st.rerun()

        if st.session_state.ai_summary_prompt:
            st.subheader("AIã¸ã®æŒ‡ç¤ºãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆç¢ºèªãƒ»ç·¨é›†å¯ï¼‰")
            prompt_input = st.text_area(
                "ä»¥ä¸‹ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’AIã«é€ä¿¡ã—ã¾ã™:",
                value=st.session_state.ai_summary_prompt,
                height=300,
                key="ai_prompt_c2_input"
            )
            
            if st.button("ğŸš€ ã“ã®å†…å®¹ã§AIã«æŒ‡ç¤ºã‚’é€ä¿¡", key="send_prompt_c2", type="primary"):
                if not os.getenv("GOOGLE_API_KEY"):
                    st.error("AIã®å®Ÿè¡Œã«ã¯ Google APIã‚­ãƒ¼ ãŒå¿…è¦ã§ã™ã€‚ï¼ˆã‚µã‚¤ãƒ‰ãƒãƒ¼ã§è¨­å®šã—ã¦ãã ã•ã„ï¼‰")
                else:
                    with st.spinner("AIãŒã‚µãƒãƒªãƒ¼ã‚’ç”Ÿæˆä¸­... (Rate Limitã«æ³¨æ„)"):
                        llm = get_llm() # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸLLMã‚’å‘¼ã³å‡ºã—
                        if llm:
                            try:
                                response = llm.invoke(prompt_input) 
                                st.session_state.ai_summary_result = response.content
                            except Exception as e:
                                st.error(f"AIã®å‘¼ã³å‡ºã—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
                                logger.error(f"AI summary failed: {e}", exc_info=True)
                        else:
                            st.error("AIãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
            
        if st.session_state.ai_summary_result:
            st.subheader("AIã«ã‚ˆã‚‹åˆ†æã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆ")
            st.markdown(st.session_state.ai_summary_result)
    
    st.markdown("---")
    if st.button("â¬…ï¸ Step B ã«æˆ»ã‚‹", key="back_to_b_c2"):
        st.session_state.current_step = 'B'; st.rerun()

# --- L1002: Step B (åˆ†æææ¡ˆUI) ---
def render_step_b():
    """Step B: åˆ†ææ‰‹æ³•ã®ææ¡ˆUIã‚’æç”»ã™ã‚‹"""
    st.title("ğŸ“Š åˆ†ææ‰‹æ³•ã®ææ¡ˆ (Step B)")
    
    # Step B å›ºæœ‰ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã‚’ã“ã“ã§åˆæœŸåŒ– (SRP)
    if 'df_flagged_B' not in st.session_state: st.session_state.df_flagged_B = pd.DataFrame()
    if 'suggestions_B' not in st.session_state: st.session_state.suggestions_B = []
    if 'chosen_analysis_list' not in st.session_state: st.session_state.chosen_analysis_list = []
    
    st.header("Step 1: ãƒ•ãƒ©ã‚°ä»˜ãCSVã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
    uploaded_flagged_file = st.file_uploader("ãƒ•ãƒ©ã‚°ä»˜ã‘æ¸ˆã¿CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=['csv'], key="step_b_uploader")
    
    analysis_prompt_B = st.text_area(
        "ï¼ˆä»»æ„ï¼‰è¿½åŠ ã®åˆ†ææŒ‡ç¤º:", 
        placeholder="ä¾‹: ç‰¹å®šã®å¸‚åŒºç”ºæ‘ï¼ˆåºƒå³¶å¸‚ãªã©ï¼‰ã¨è¦³å…‰æ–½è¨­ã®ç›¸é–¢é–¢ä¿‚ã‚’æ·±æ˜ã‚Šã—ãŸã„ã€‚",
        key="step_b_prompt"
    )

    if uploaded_flagged_file:
        try:
            uploaded_flagged_file.seek(0)
            df_flagged = pd.read_csv(uploaded_flagged_file, encoding="utf-8-sig")
            st.session_state.df_flagged_B = df_flagged # L1017: Step C ã®ãŸã‚ã«ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ä¿å­˜
            st.success(f"ãƒ•ã‚¡ã‚¤ãƒ«ã€Œ{uploaded_flagged_file.name}ã€èª­è¾¼å®Œäº†")
            st.dataframe(df_flagged.head())

            if st.button("ğŸ’¡ åˆ†ææ‰‹æ³•ã‚’ææ¡ˆã•ã›ã‚‹", key="suggest_button_B"):
                with st.spinner("ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã¨æŒ‡ç¤ºå†…å®¹ã‚’åˆ†æã—ã€æ‰‹æ³•ã‚’ææ¡ˆä¸­..."):
                    base_suggestions = suggest_analysis_techniques(df_flagged)
                    
                    ai_suggestions = []
                    if analysis_prompt_B.strip():
                        # L1028: ã‚­ãƒ£ãƒƒã‚·ãƒ¥åˆ©ç”¨ç‰ˆ (llmå¼•æ•°å‰Šé™¤)
                        ai_suggestions = get_suggestions_from_prompt(
                            analysis_prompt_B, df_flagged, base_suggestions
                        )

                    base_suggestion_names = {s['name'] for s in base_suggestions} 
                    filtered_ai_suggestions = [
                        s for s in ai_suggestions if s['name'] not in base_suggestion_names 
                    ]
                    all_suggestions = sorted(base_suggestions + filtered_ai_suggestions, key=lambda x: x['priority']) 
                    st.session_state.suggestions_B = all_suggestions
                    # L1041: ææ¡ˆæ™‚ã«å¤ã„Cã®çµæœã‚’ã‚¯ãƒªã‚¢ (KISS)
                    st.session_state.step_c_results = {}
                    st.session_state.ai_summary_prompt = None
                    st.session_state.ai_summary_result = None

            if 'suggestions_B' in st.session_state and st.session_state.suggestions_B:
                display_suggestions(st.session_state.suggestions_B, df_flagged)
            
            # L1070-L1077: è‡´å‘½çš„ãƒã‚° (NameError) ä¿®æ­£
            # L1070 (æ—§ L1418) ã® if st.button(...) ãƒ–ãƒ­ãƒƒã‚¯å…¨ä½“ã‚’å‰Šé™¤

        except Exception as e:
            logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­è¾¼/åˆ†æææ¡ˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­è¾¼/åˆ†æææ¡ˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

# --- L1078: Main (ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ) ---
def main():
    """Streamlitã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    st.set_page_config(page_title="AI Data Analysis App", layout="wide")
    
    # L1082: ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã®ã¿åˆæœŸåŒ– (SRP)
    if 'current_step' not in st.session_state:
        st.session_state.current_step = 'A' # åˆæœŸã‚¹ãƒ†ãƒƒãƒ—
    if 'log_messages' not in st.session_state:
        st.session_state.log_messages = []

    # L1090-L1099 (æ—§ L1377-L1385): ã‚¹ãƒ†ãƒƒãƒ—å›ºæœ‰ã®åˆæœŸåŒ–ã‚’å‰Šé™¤ (SRP)

    with st.sidebar:
        st.title("Navigation")
        st.markdown("---")
        
        st.header("âš™ï¸ AI è¨­å®š")
        google_api_key = st.text_input("Google API Key", type="password", key="api_key_global")
        if google_api_key:
            os.environ["GOOGLE_API_KEY"] = google_api_key
        
        # L1109: APIã‚­ãƒ¼ãŒãªã„å ´åˆã®è­¦å‘Šã‚’å¼·åŒ– (KISS)
        if not os.getenv("GOOGLE_API_KEY"):
            st.warning("AIæ©Ÿèƒ½ã‚’åˆ©ç”¨ã™ã‚‹ã«ã¯ Google APIã‚­ãƒ¼ ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
        else:
            # L1113: ã‚¢ãƒ—ãƒªèµ·å‹•æ™‚ã«LLMã¨spaCyã®ãƒ­ãƒ¼ãƒ‰ã‚’è©¦ã¿ã‚‹ (KISS)
            if get_llm() is None:
                st.error("LLMã®åˆæœŸåŒ–ã«å¤±æ•—ã€‚APIã‚­ãƒ¼ãŒæ­£ã—ã„ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            if load_spacy_model() is None:
                st.error("spaCyãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã€‚Dockerã‚¤ãƒ¡ãƒ¼ã‚¸ã‚’å†ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        
        st.markdown("---")
        
        st.header("ğŸ”„ Step é¸æŠ")
        current_step = st.session_state.current_step
        
        if st.button("Step A: ã‚¿ã‚°ä»˜ã‘", key="nav_A", use_container_width=True, type=("primary" if current_step == 'A' else "secondary")):
            if st.session_state.current_step != 'A':
                st.session_state.current_step = 'A'; st.rerun()

        if st.button("Step B: åˆ†ææ‰‹æ³•ææ¡ˆ", key="nav_B", use_container_width=True, type=("primary" if current_step == 'B' else "secondary")):
            if st.session_state.current_step != 'B':
                st.session_state.current_step = 'B'; st.rerun()

    # --- ã‚¹ãƒ†ãƒƒãƒ—ã«å¿œã˜ã¦æç”»é–¢æ•°ã‚’å‘¼ã³å‡ºã— ---
    if st.session_state.current_step == 'A':
        render_step_a()
    elif st.session_state.current_step == 'B':
        render_step_b()
    elif st.session_state.current_step == 'C': 
        render_step_c() 

if __name__ == "__main__":
    main()