# ---
# app.py (AI Data Analysis App - Refactored Version)
#
# ã“ã®ã‚³ãƒ¼ãƒ‰ã¯ã€å•†ç”¨åˆ©ç”¨ãŒå®¹æ˜“ãªå¯›å®¹ãªï¼ˆpermissiveï¼‰ãƒ©ã‚¤ã‚»ãƒ³ã‚¹
# (ä¾‹: MIT, Apache License 2.0, BSD) ã®ä¸‹ã§åˆ©ç”¨å¯èƒ½ãª
# ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã€ã¾ãŸã¯ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã«ä¾å­˜ã—ãªã„ã‚³ãƒ¼ãƒ‰ã®ã¿ã‚’ä½¿ç”¨ã—ã¦å®Ÿè£…ã•ã‚Œã¦ã„ã¾ã™ã€‚
# GPL, AGPL, SSPLãªã©ã®ã‚³ãƒ”ãƒ¼ãƒ¬ãƒ•ãƒˆåŠ¹æœã‚’æŒã¤ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¯ä½¿ç”¨ã—ã¦ã„ã¾ã›ã‚“ã€‚
# ---

# --- 1. ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ---
import streamlit as st
import pandas as pd
import numpy as np
import os
import re
import json
import logging
import time
import spacy
import altair as alt
import networkx as nx
from networkx.algorithms import community
from pyvis.network import Network
import streamlit.components.v1 as components
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from io import StringIO, BytesIO
from typing import Optional, Dict, List, Any, Union
import random  # (â˜…) Tipsãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã®ãŸã‚ã«è¿½åŠ 
from dotenv import load_dotenv  # (â˜…) .envèª­ã¿è¾¼ã¿ã®ãŸã‚ã«è¿½åŠ 
import matplotlib
matplotlib.use('Agg') # (â˜…) Streamlitã®ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã§å‹•ä½œã•ã›ã‚‹ãŸã‚ã®ãŠã¾ã˜ãªã„
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import base64

# (â˜…) --- matplotlib æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š ---
# Dockerfileã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ãŸIPAãƒ•ã‚©ãƒ³ãƒˆã®ãƒ‘ã‚¹ã‚’æŒ‡å®š
# (â˜…) ã”æ³¨æ„: ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§å®Ÿè¡Œã™ã‚‹å ´åˆã€ã“ã®ãƒ•ã‚©ãƒ³ãƒˆãƒ‘ã‚¹ãŒç•°ãªã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
# (â˜…) Dockerç’°å¢ƒ (Debianãƒ™ãƒ¼ã‚¹) ã‚’å‰æã¨ã—ã¦ã„ã¾ã™ã€‚
try:
    # (â˜…) Dockerfileå†…ã®ãƒ‘ã‚¹
    font_path = '/usr/share/fonts/opentype/ipafont-gothic/ipagp.ttf' 
    if os.path.exists(font_path):
        fm.fontManager.addfont(font_path)
        plt.rcParams['font.family'] = 'IPAGothic'
        # logger.info(f"æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆ '{font_path}' ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")
    else:
        # (â˜…) ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ (ç’°å¢ƒã«ã‚ˆã£ã¦èª¿æ•´ãŒå¿…è¦)
        logger.warning(f"æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆ '{font_path}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚°ãƒ©ãƒ•ã®æ—¥æœ¬èªãŒæ–‡å­—åŒ–ã‘ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
        # (â˜…) ä»£æ›¿ãƒ•ã‚©ãƒ³ãƒˆã®æ¤œç´¢ (ã‚„ã‚„æ™‚é–“ãŒã‹ã‹ã‚‹ãŒå …ç‰¢)
        try:
            jp_font = fm.findfont(fm.FontProperties(family='IPAexGothic'))
            plt.rcParams['font.family'] = 'IPAexGothic'
            logger.info(f"ä»£æ›¿ãƒ•ã‚©ãƒ³ãƒˆ '{jp_font}' ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
        except:
             logger.error("ä»£æ›¿ã®æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã‚‚è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
             plt.rcParams['font.family'] = 'sans-serif'
except Exception as e:
    logger.error(f"matplotlibæ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®šã‚¨ãƒ©ãƒ¼: {e}")

# (â˜…) LangChain / Google Generative AI ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
# ãƒ©ã‚¤ã‚»ãƒ³ã‚¹: Apache License 2.0
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser

# (â˜…) Step D (PowerPointç”Ÿæˆ) ã§å¿…è¦ã¨ãªã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
# ãƒ©ã‚¤ã‚»ãƒ³ã‚¹: MIT License
try:
    import pptx
    from pptx import Presentation
    from pptx.util import Inches, Pt
except ImportError:
    st.error(
        "PowerPointç”Ÿæˆãƒ©ã‚¤ãƒ–ãƒ©ãƒª(python-pptx)ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
        "pip install python-pptx ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚"
    )

# (â˜…) Step D (ãƒ‰ãƒ©ãƒƒã‚°ï¼†ãƒ‰ãƒ­ãƒƒãƒ—UI) ã§å¿…è¦ã¨ãªã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
# ãƒ©ã‚¤ã‚»ãƒ³ã‚¹: MIT License
try:
    from streamlit_sortables import sort_items
except ImportError:
    st.error(
        "UIãƒ©ã‚¤ãƒ–ãƒ©ãƒª(streamlit-sortables)ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
        "pip install streamlit-sortables ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚"
    )

# æ—¢å­˜ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒª (openpyxl, ja_core_news_sm) ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    import openpyxl
except ImportError:
    st.error("Excel (openpyxl) ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚`pip install openpyxl` ã—ã¦ãã ã•ã„ã€‚")
try:
    import ja_core_news_sm
except ImportError:
    st.error("spaCyæ—¥æœ¬èªãƒ¢ãƒ‡ãƒ« (ja_core_news_sm) ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚`python -m spacy download ja_core_news_sm` ã—ã¦ãã ã•ã„ã€‚")

# --- 2. (â˜…) å®šæ•°å®šç¾© ---

# (â˜…) è¦ä»¶ã«åŸºã¥ãã€ä½¿ç”¨ã™ã‚‹AIãƒ¢ãƒ‡ãƒ«ã‚’å®šæ•°ã¨ã—ã¦å®šç¾©
MODEL_FLASH_LITE = "gemini-2.5-flash-lite" # Step A, B (é«˜é€Ÿãƒ»åŠ¹ç‡çš„)
MODEL_FLASH = "gemini-2.5-flash"         # Step D (ä»£æ›¿)
MODEL_PRO = "gemini-2.5-pro"             # Step C, D (é«˜å“è³ª)

# ãƒãƒƒãƒã‚µã‚¤ã‚ºã¨å¾…æ©Ÿæ™‚é–“ (KISS)
FILTER_BATCH_SIZE = 50
FILTER_SLEEP_TIME = 6.1  # Rate Limit å¯¾ç­– (10 requests per 60 seconds)
TAGGING_BATCH_SIZE = 10
TAGGING_SLEEP_TIME = 6.1  # Rate Limit å¯¾ç­–

# åœ°åè¾æ›¸
try:
    from geography_db import JAPAN_GEOGRAPHY_DB
except ImportError:
    st.error("åœ°åè¾æ›¸ãƒ•ã‚¡ã‚¤ãƒ« (geography_db.py) ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
    JAPAN_GEOGRAPHY_DB = {}


# --- 3. ãƒ­ã‚¬ãƒ¼è¨­å®š ---
class StreamlitLogHandler(logging.Handler):
    """Streamlitã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã«ãƒ­ã‚°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ ã™ã‚‹ãƒãƒ³ãƒ‰ãƒ©"""
    def __init__(self):
        super().__init__()
        if 'log_messages' not in st.session_state:
            st.session_state.log_messages = []

    def emit(self, record):
        log_entry = self.format(record)
        st.session_state.log_messages.append(log_entry)
        st.session_state.log_messages = st.session_state.log_messages[-500:]

logger = logging.getLogger(__name__)
if not logger.handlers:
    logger.setLevel(logging.INFO)
    handler = StreamlitLogHandler()
    handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
    logger.addHandler(handler)


# --- 4. (â˜…) AIãƒ¢ãƒ‡ãƒ«ãƒ»NLPãƒ¢ãƒ‡ãƒ«ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç† ---

# (â˜…) è¦ä»¶ã«åŸºã¥ãã€ç•°ãªã‚‹ãƒ¢ãƒ‡ãƒ«åã‚’æŒ‡å®šã—ã¦LLMã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹é–¢æ•°ã«åˆ·æ–°
@st.cache_resource(ttl=3600)  # 1æ™‚é–“ã‚­ãƒ£ãƒƒã‚·ãƒ¥
def get_llm(model_name: str, temperature: float = 0.0) -> Optional[ChatGoogleGenerativeAI]:
    """
    æŒ‡å®šã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«åã¨æ¸©åº¦ã§LLM (Google Gemini) ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ãƒ»ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚‹ã€‚
    """
    try:
        api_key = os.getenv("GOOGLE_API_KEY")
        if not api_key:
            logger.error(f"get_llm: GOOGLE_API_KEY ãŒã‚ã‚Šã¾ã›ã‚“ (Model: {model_name})")
            return None

        llm = ChatGoogleGenerativeAI(
            model=model_name,
            temperature=temperature,
            convert_system_message_to_human=True,
            api_key=api_key
        )
        logger.info(f"LLM Model ({model_name}) loaded successfully.")
        return llm
    except Exception as e:
        logger.error(f"LLM ({model_name}) ã®åˆæœŸåŒ–ã«å¤±æ•—: {e}", exc_info=True)
        st.error(f"AIãƒ¢ãƒ‡ãƒ« ({model_name}) ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return None

@st.cache_resource
def load_spacy_model() -> Optional[spacy.language.Language]:
    """spaCyã®æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«(ja_core_news_sm)ã‚’ãƒ­ãƒ¼ãƒ‰ãƒ»ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚‹"""
    try:
        logger.info("Loading spaCy model (ja_core_news_sm)...")
        nlp = spacy.load("ja_core_news_sm")
        logger.info("spaCy model loaded successfully.")
        return nlp
    except Exception as e:
        logger.error(f"Failed to load spaCy model: {e}", exc_info=True)
        return None

@st.cache_data(ttl=3600) # 1æ™‚é–“ã‚­ãƒ£ãƒƒã‚·ãƒ¥
def get_analysis_tips_list_from_ai() -> List[str]:
    """
    (â˜…) å¾…æ©Ÿæ™‚é–“ä¸­ã«è¡¨ç¤ºã™ã‚‹ã€Œãƒ‡ãƒ¼ã‚¿åˆ†æã«é–¢ã™ã‚‹Tipsã€ã‚’AIã§ç”Ÿæˆã™ã‚‹ã€‚
    ãƒ¢ãƒ‡ãƒ«: MODEL_FLASH_LITE (gemini-2.5-flash-lite)
    """
    logger.info("get_analysis_tips_list_from_ai: AI (Flash Lite) ã§TIPSã‚’ç”Ÿæˆã—ã¾ã™ã€‚")
    llm = get_llm(model_name=MODEL_FLASH_LITE, temperature=0.5)
    if llm is None:
        return ["AIãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚"]

    prompt = PromptTemplate.from_template(
        """
        ã‚ãªãŸã¯ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ãƒ†ã‚£ã‚¹ãƒˆã®ãƒ¡ãƒ³ã‚¿ãƒ¼ã§ã™ã€‚
        ãƒ‡ãƒ¼ã‚¿åˆ†æã®åˆå¿ƒè€…ã‹ã‚‰ä¸­ç´šè€…ã«å‘ã‘ã¦ã€å½¹ç«‹ã¤ã€Œãƒ’ãƒ³ãƒˆã‚„TIPSã€ã‚’ã€10å€‹ã€‘ã€JSONã®ãƒªã‚¹ãƒˆå½¢å¼ã§ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
        
        # æŒ‡ç¤º:
        1. å„TIPSã¯ã€1ã€œ2æ–‡ã®ç°¡æ½”ãªæ—¥æœ¬èªã®æ–‡å­—åˆ—ã«ã™ã‚‹ã“ã¨ã€‚
        2. ä¾‹: ã€Œ'å¹³å‡å€¤'ã ã‘ã§ãªã'ä¸­å¤®å€¤'ã‚‚è¦‹ã‚‹ã“ã¨ã§ã€å¤–ã‚Œå€¤ã®å½±éŸ¿ã‚’æŠŠæ¡ã§ãã¾ã™ã€‚ã€
        3. ä¾‹: ã€Œãƒ‡ãƒ¼ã‚¿ã‚’å¯è¦–åŒ–ã™ã‚‹å‰ã«ã€ã¾ãšãƒ‡ãƒ¼ã‚¿ã®'æ¬ æå€¤'ã¨'å‹'ã‚’ç¢ºèªã—ã¾ã—ã‚‡ã†ã€‚ã€
        4. å‡ºåŠ›ã¯JSONãƒªã‚¹ãƒˆå½¢å¼ï¼ˆ ["TIPS1", "TIPS2", ...] ï¼‰ã®ã¿ã€‚
        
        # å›ç­” (JSONãƒªã‚¹ãƒˆå½¢å¼ã®ã¿):
        """
    )
    chain = prompt | llm | StrOutputParser()
    
    try:
        response_str = chain.invoke({})
        logger.debug(f"AI TIPSç”Ÿæˆ(ç”Ÿ): {response_str}")
        
        # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚„ä¸è¦ãªãƒ†ã‚­ã‚¹ãƒˆã‚’é™¤å»ã—ã€JSONã®ã¿ã‚’æŠ½å‡º
        match = re.search(r'\[.*\]', response_str, re.DOTALL)
        if not match:
            logger.warning("AIãŒTIPSãƒªã‚¹ãƒˆ(JSON)ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
            return ["åˆ†æTIPSã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚"]
        
        json_str = match.group(0)
        tips_list = json.loads(json_str)
        
        if isinstance(tips_list, list) and all(isinstance(tip, str) for tip in tips_list):
            logger.info(f"AI TIPS {len(tips_list)}ä»¶ã®ç”Ÿæˆã«æˆåŠŸã€‚")
            return tips_list
        else:
            raise Exception("AIã®å›ç­”ãŒæ–‡å­—åˆ—ã®ãƒªã‚¹ãƒˆå½¢å¼ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
            
    except Exception as e:
        logger.error(f"AI TIPSç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        return [
            "TIPSã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚",
            "ãƒ‡ãƒ¼ã‚¿åˆ†æã¯ã€ã¾ãšç›®çš„ï¼ˆKGI/KPIï¼‰ã‚’æ˜ç¢ºã«ã™ã‚‹ã“ã¨ã‹ã‚‰å§‹ã¾ã‚Šã¾ã™ã€‚",
            "ã€Œãªãœï¼Ÿã€ã‚’5å›ç¹°ã‚Šè¿”ã™ã“ã¨ã§ã€åˆ†æã®çœŸã®ç›®çš„ã«ãŸã©ã‚Šç€ãã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚",
            "è‰¯ã„åˆ†æã¯ã€è‰¯ã„ã€Œå•ã„ã€ã‹ã‚‰ç”Ÿã¾ã‚Œã¾ã™ã€‚",
            "ãƒ‡ãƒ¼ã‚¿ã¯ã€Œé›†ã‚ã‚‹ã€ã“ã¨ã‚ˆã‚Šã€Œã©ã†ä½¿ã†ã‹ã€ãŒé‡è¦ã§ã™ã€‚"
        ]

# --- 5. ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ãƒ˜ãƒ«ãƒ‘ãƒ¼ ---
def read_file(file: st.runtime.uploaded_file_manager.UploadedFile) -> (Optional[pd.DataFrame], Optional[str]):
    """ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«(Excel/CSV)ã‚’Pandas DataFrameã¨ã—ã¦èª­ã¿è¾¼ã‚€"""
    file_name = file.name
    logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿é–‹å§‹: {file_name}")
    try:
        if file_name.endswith('.csv'):
            # æ–‡å­—ã‚³ãƒ¼ãƒ‰ã‚’è‡ªå‹•åˆ¤åˆ¥
            try:
                content = file.getvalue().decode('utf-8-sig')
            except UnicodeDecodeError:
                logger.warning(f"UTF-8-SIGãƒ‡ã‚³ãƒ¼ãƒ‰å¤±æ•—ã€‚CP932ã§å†è©¦è¡Œ: {file_name}")
                content = file.getvalue().decode('cp932')
            df = pd.read_csv(StringIO(content))

        elif file_name.endswith(('.xlsx', '.xls')):
            df = pd.read_excel(BytesIO(file.getvalue()), engine='openpyxl')
        else:
            msg = f"ã‚µãƒãƒ¼ãƒˆå¤–ã®ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼: {file_name}"
            logger.warning(msg)
            return None, msg

        logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿æˆåŠŸ: {file_name}")
        return df, None
    except Exception as e:
        logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ ({file_name}): {e}", exc_info=True)
        st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ã€Œ{file_name}ã€ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        return None, f"èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}"


# --- 6. (â˜…) Step A: AIã‚¿ã‚°ä»˜ã‘é–¢é€£é–¢æ•° ---
# (è¦ä»¶: Step Aã¯ gemini-2.5-flash-lite ã‚’ä½¿ç”¨)

def get_dynamic_categories(analysis_prompt: str) -> Optional[Dict[str, str]]:
    """
    (Step A) ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®åˆ†ææŒ‡é‡ã«åŸºã¥ãã€AIãŒå‹•çš„ãªã‚«ãƒ†ã‚´ãƒªã‚’JSONå½¢å¼ã§ç”Ÿæˆã™ã‚‹ã€‚
    (â˜…) ãƒ¢ãƒ‡ãƒ«: MODEL_FLASH_LITE (gemini-2.5-flash-lite)
    """
    # (â˜…) Step A ã®è¦ä»¶ã«åŸºã¥ãã€FLASH_LITE ãƒ¢ãƒ‡ãƒ«ã‚’æ˜ç¤ºçš„ã«æŒ‡å®š
    llm = get_llm(model_name=MODEL_FLASH_LITE, temperature=0.0)
    if llm is None:
        logger.error("get_dynamic_categories: LLM (Flash Lite) ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")
        st.error("AIãƒ¢ãƒ‡ãƒ«(Flash Lite)ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§APIã‚­ãƒ¼ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
        return None

    logger.info("å‹•çš„ã‚«ãƒ†ã‚´ãƒªç”ŸæˆAI (Flash Lite) ã‚’å‘¼ã³å‡ºã—...")
    prompt = PromptTemplate.from_template(
        """
        ã‚ãªãŸã¯ãƒ‡ãƒ¼ã‚¿åˆ†æã®ã‚¹ã‚­ãƒ¼ãƒè¨­è¨ˆè€…ã§ã™ã€‚ã€Œåˆ†ææŒ‡é‡ã€ã‚’èª­ã¿ã€ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æŠ½å‡ºã™ã‚‹ã¹ãã€Œãƒˆãƒ”ãƒƒã‚¯ã®ã‚«ãƒ†ã‚´ãƒªã€ã‚’è€ƒæ¡ˆã—ã¦ãã ã•ã„ã€‚ã€Œå¸‚åŒºç”ºæ‘ã€ã¯å¿…é ˆã‚«ãƒ†ã‚´ãƒªã¨ã—ã¦è‡ªå‹•ã§è¿½åŠ ã•ã‚Œã‚‹ãŸã‚ã€ãã‚Œä»¥å¤–ã®ã‚«ãƒ†ã‚´ãƒªã‚’å®šç¾©ã—ã¦ãã ã•ã„ã€‚
        # æŒ‡ç¤º: 1.ã€Œåˆ†ææŒ‡é‡ã€ã®ãƒˆãƒ”ãƒƒã‚¯ã‚’ã‚«ãƒ†ã‚´ãƒªåŒ– 2.å„ã‚«ãƒ†ã‚´ãƒªã®èª¬æ˜è¨˜è¿° 3.å³æ ¼ãªJSONè¾æ›¸å‡ºåŠ› 4.åœ°åã‚«ãƒ†ã‚´ãƒªç¦æ­¢ 5.è©²å½“ãªã‘ã‚Œã°ç©ºJSON
        # åˆ†ææŒ‡é‡:{user_prompt}
        # å›ç­” (JSONè¾æ›¸å½¢å¼):
        """
    )
    chain = prompt | llm | StrOutputParser()
    try:
        response_str = chain.invoke({"user_prompt": analysis_prompt})
        logger.debug(f"AIã‚«ãƒ†ã‚´ãƒªå®šç¾©(ç”Ÿ): {response_str}")

        # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚„ä¸è¦ãªãƒ†ã‚­ã‚¹ãƒˆã‚’é™¤å»ã—ã€JSONã®ã¿ã‚’æŠ½å‡º
        match = re.search(r'\{.*\}', response_str, re.DOTALL)
        if not match:
            logger.warning("AIãŒJSONå½¢å¼ã§å¿œç­”ã—ã¾ã›ã‚“ã§ã—ãŸã€‚")
            return None
        
        json_str = match.group(0).replace("'", '"')
        try:
            categories = json.loads(json_str)
            return categories
        except json.JSONDecodeError as json_e:
            logger.error(f"AIå¿œç­”ã®JSONãƒ‘ãƒ¼ã‚¹å¤±æ•—: {json_e} - Raw: {json_str}")
            return None
            
    except Exception as e:
        logger.error(f"AIã‚«ãƒ†ã‚´ãƒªç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        st.error(f"AIã‚«ãƒ†ã‚´ãƒªç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return None

def filter_relevant_data_by_ai(df_batch: pd.DataFrame, analysis_prompt: str) -> pd.DataFrame:
    """
    (Step A) AIã‚’ä½¿ã„ã€åˆ†ææŒ‡é‡ã¨ç„¡é–¢ä¿‚ãªè¡Œã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹ (relevant: true/false)ã€‚
    (â˜…) ãƒ¢ãƒ‡ãƒ«: MODEL_FLASH_LITE (gemini-2.5-flash-lite)
    (â˜…) è¦ä»¶: é€²æ—è¡¨ç¤º (ã“ã®é–¢æ•°ã¯ãƒãƒƒãƒå‡¦ç†ã®ä¸€éƒ¨ã¨ã—ã¦å‘¼ã°ã‚Œã€å‘¼ã³å‡ºã—å…ƒã®
          `render_step_a` å†…ã® `update_progress_ui` ã§é€²æ—ãŒè¡¨ç¤ºã•ã‚Œã‚‹)
    """
    # (â˜…) Step A ã®è¦ä»¶ã«åŸºã¥ãã€FLASH_LITE ãƒ¢ãƒ‡ãƒ«ã‚’æ˜ç¤ºçš„ã«æŒ‡å®š
    llm = get_llm(model_name=MODEL_FLASH_LITE, temperature=0.0)
    if llm is None:
        logger.error("filter_relevant_data_by_ai: LLM (Flash Lite) ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")
        st.error("AIãƒ¢ãƒ‡ãƒ«(Flash Lite)ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚APIã‚­ãƒ¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        return pd.DataFrame()  # ç©ºã®DF

    logger.debug(f"{len(df_batch)}ä»¶ AIé–¢é€£æ€§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° (Flash Lite) é–‹å§‹...")

    # ãƒ†ã‚­ã‚¹ãƒˆãŒé•·ã™ãã‚‹å ´åˆã€å…ˆé ­500æ–‡å­—ã«åˆ‡ã‚Šè©°ã‚ã‚‹
    input_texts_jsonl = df_batch.apply(
        lambda row: json.dumps(
            {"id": row['id'], "text": str(row['ANALYSIS_TEXT_COLUMN'])[:500]},
            ensure_ascii=False
        ),
        axis=1
    ).tolist()

    prompt = PromptTemplate.from_template(
        """
        ã‚ãªãŸã¯ãƒ‡ãƒ¼ã‚¿åˆ†æã®ã‚­ãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼ã§ã™ã€‚ã€Œåˆ†ææŒ‡é‡ã€ã«åŸºã¥ãã€ã€Œãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿(JSONL)ã€ã®å„è¡ŒãŒåˆ†æå¯¾è±¡ã¨ã—ã¦ã€é–¢é€£ã—ã¦ã„ã‚‹ã‹ (relevant: true)ã€‘ã€ã€ç„¡é–¢ä¿‚ã‹ (relevant: false)ã€‘ã‚’åˆ¤å®šã—ã¦ãã ã•ã„ã€‚
        # åˆ†ææŒ‡é‡ (Analysis Scope):
        {analysis_prompt}
        # æŒ‡ç¤º:
        1. ã€Œåˆ†ææŒ‡é‡ã€ã¨ã€å¼·ãé–¢é€£ã€‘ã™ã‚‹æŠ•ç¨¿ã®ã¿ã‚’ `true` ã¨ã™ã‚‹ã€‚
        2. å˜ãªã‚‹å®£ä¼ã€æŒ¨æ‹¶ã®ã¿ã€æŒ‡é‡ã¨ç„¡é–¢ä¿‚ãªåœ°åŸŸã®è¨€åŠã¯ `false` ã¨ã™ã‚‹ã€‚
        3. å‡ºåŠ›ã¯ã€JSONLå½¢å¼ã®ã¿ã€‘ï¼ˆid ã¨ relevant (boolean) ã‚’å«ã‚€è¾æ›¸ï¼‰ã€‚
        # ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ (JSONL):
        {text_data_jsonl}
        # å›ç­” (JSONLå½¢å¼ã®ã¿):
        """
    )
    chain = prompt | llm | StrOutputParser()
    
    try:
        invoke_params = {
            "analysis_prompt": analysis_prompt,
            "text_data_jsonl": "\n".join(input_texts_jsonl)
        }
        response_str = chain.invoke(invoke_params)
        
        results = []
        # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ ````jsonl ... ``` ã‚’é™¤å»
        match = re.search(r'```(?:jsonl|json)?\s*([\s\S]*?)\s*```', response_str, re.DOTALL)
        jsonl_content = match.group(1).strip() if match else response_str.strip()

        for line in jsonl_content.strip().split('\n'):
            cleaned_line = line.strip()
            if not cleaned_line: continue
            try:
                data = json.loads(cleaned_line)
                # relevant ãŒ "true" (str) ã‚„ true (bool) ãªã©æºã‚‰ããŒã‚ã‚‹ãŸã‚å …ç‰¢ã«å‡¦ç†
                is_relevant = False
                if isinstance(data.get("relevant"), bool):
                    is_relevant = data.get("relevant")
                elif isinstance(data.get("relevant"), str):
                    is_relevant = data.get("relevant").lower() == 'true'
                
                results.append({"id": data.get("id"), "relevant": is_relevant})
            except (json.JSONDecodeError, AttributeError) as json_e:
                logger.warning(f"AIãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å›ç­”ãƒ‘ãƒ¼ã‚¹å¤±æ•—: {cleaned_line} - Error: {json_e}")
                # ãƒ‘ãƒ¼ã‚¹å¤±æ•—æ™‚ã¯ã€IDãŒç‰¹å®šã§ãã‚Œã°é–¢é€£ã‚ã‚Š(True)ã¨ã—ã¦ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                id_match = re.search(r'"id":\s*(\d+)', cleaned_line)
                if id_match:
                    results.append({"id": int(id_match.group(1)), "relevant": True})

        return pd.DataFrame(results) if results else pd.DataFrame(columns=['id', 'relevant'])
        
    except Exception as e:
        logger.error(f"AIãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãƒãƒƒãƒå‡¦ç†ä¸­ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        st.error(f"AIãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å®‰å…¨å´ã«å€’ã—ã€ã™ã¹ã¦é–¢é€£ã‚ã‚Š(True)ã¨ã—ã¦è¿”ã™
        return df_batch[['id']].copy().assign(relevant=True)

def perform_ai_tagging(
    df_batch: pd.DataFrame,
    categories_to_tag: Dict[str, str],
    analysis_prompt: str = ""
) -> pd.DataFrame:
    """
    (Step A) ãƒ†ã‚­ã‚¹ãƒˆã®ãƒãƒƒãƒã‚’å—ã‘å–ã‚Šã€AIãŒã€æŒ‡å®šã•ã‚ŒãŸã‚«ãƒ†ã‚´ãƒªå®šç¾©ã€‘ã«åŸºã¥ã„ã¦ç›´æ¥ã‚¿ã‚°ä»˜ã‘ã‚’è¡Œã†
    (â˜…) ãƒ¢ãƒ‡ãƒ«: MODEL_FLASH_LITE (gemini-2.5-flash-lite)
    (â˜…) è¦ä»¶: é€²æ—è¡¨ç¤º (ã“ã®é–¢æ•°ã¯ãƒãƒƒãƒå‡¦ç†ã®ä¸€éƒ¨ã¨ã—ã¦å‘¼ã°ã‚Œã€å‘¼ã³å‡ºã—å…ƒã®
          `render_step_a` å†…ã® `update_progress_ui` ã§é€²æ—ãŒè¡¨ç¤ºã•ã‚Œã‚‹)
    """
    # (â˜…) Step A ã®è¦ä»¶ã«åŸºã¥ãã€FLASH_LITE ãƒ¢ãƒ‡ãƒ«ã‚’æ˜ç¤ºçš„ã«æŒ‡å®š
    llm = get_llm(model_name=MODEL_FLASH_LITE, temperature=0.0)
    if llm is None:
        logger.error("perform_ai_tagging: LLM (Flash Lite) ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")
        st.error("AIãƒ¢ãƒ‡ãƒ«(Flash Lite)ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚APIã‚­ãƒ¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        return pd.DataFrame()

    logger.info(f"{len(df_batch)}ä»¶ AIã‚¿ã‚°ä»˜ã‘ (Flash Lite) é–‹å§‹ (ã‚«ãƒ†ã‚´ãƒª: {list(categories_to_tag.keys())})")

    # åœ°åè¾æ›¸ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æº–å‚™ (åˆ†ææŒ‡é‡ã«é–¢é€£ã™ã‚‹åœ°åã®ã¿ã‚’AIã«æ¸¡ã™)
    geo_context_str = "{}"
    if JAPAN_GEOGRAPHY_DB and "å¸‚åŒºç”ºæ‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰" in categories_to_tag:
        try:
            relevant_geo_db = {}
            prompt_lower = analysis_prompt.lower()
            
            # (åœ°åè¾æ›¸ã®ã‚­ãƒ¼ã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ä¸¡æ–¹ã«å«ã¾ã‚Œã‚‹ä¸»è¦ãªãƒ’ãƒ³ãƒˆ)
            hints = ["åºƒå³¶", "ç¦å²¡", "å¤§é˜ª", "æ±äº¬", "åŒ—æµ·é“", "æ„›çŸ¥", "å®®åŸ", "æœ­å¹Œ", "æ¨ªæµœ", "åå¤å±‹", "äº¬éƒ½", "ç¥æˆ¸", "ä»™å°"]
            keys_found = [
                key for key in JAPAN_GEOGRAPHY_DB.keys()
                if any(h in key.lower() for h in hints) and any(h in prompt_lower for h in hints)
            ]
            # ç‰¹å®šã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€é–¢é€£ã™ã‚‹ã‚­ãƒ¼ã‚’å¼·åˆ¶çš„ã«è¿½åŠ 
            if "åºƒå³¶" in prompt_lower: keys_found.extend(["åºƒå³¶çœŒ", "åºƒå³¶å¸‚"])
            if "æ±äº¬" in prompt_lower: keys_found.extend(["æ±äº¬éƒ½", "æ±äº¬23åŒº"])
            if "å¤§é˜ª" in prompt_lower: keys_found.extend(["å¤§é˜ªåºœ", "å¤§é˜ªå¸‚"])

            for key in set(keys_found): # é‡è¤‡å‰Šé™¤
                if key in JAPAN_GEOGRAPHY_DB:
                    relevant_geo_db[key] = JAPAN_GEOGRAPHY_DB[key]
            
            #  relevant_geo_db ãŒç©ºã®å ´åˆã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ (ä¸»è¦éƒ½å¸‚)
            if not relevant_geo_db:
                logger.warning("åœ°åè¾æ›¸ã®çµã‚Šè¾¼ã¿ãƒ’ãƒ³ãƒˆãªã—ã€‚ä¸»è¦éƒ½å¸‚ã®ã¿æ¸¡ã—ã¾ã™ã€‚")
                default_keys = ["æ±äº¬éƒ½", "æ±äº¬23åŒº", "å¤§é˜ªåºœ", "å¤§é˜ªå¸‚", "åºƒå³¶çœŒ", "åºƒå³¶å¸‚", "ç¦å²¡çœŒ", "ç¦å²¡å¸‚"]
                for key in default_keys:
                    if key in JAPAN_GEOGRAPHY_DB:
                        relevant_geo_db[key] = JAPAN_GEOGRAPHY_DB[key]

            geo_context_str = json.dumps(relevant_geo_db, ensure_ascii=False, indent=2)
            
            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒå¤§ãã™ãã‚‹å ´åˆã€ã‚­ãƒ¼ã®ã¿ã«ç¸®å°
            if len(geo_context_str) > 5000:
                logger.warning(f"åœ°åè¾æ›¸ãŒå¤§ãã™ã ({len(geo_context_str)}B)ã€‚ã‚­ãƒ¼ã®ã¿ã«ç¸®å°ã€‚")
                geo_context_str = json.dumps(list(relevant_geo_db.keys()), ensure_ascii=False)
                
            logger.info(f"AIã«æ¸¡ã™åœ°åè¾æ›¸(çµè¾¼æ¸ˆ): {list(relevant_geo_db.keys())}")
        except Exception as e:
            logger.error(f"åœ°åè¾æ›¸ã®æº–å‚™ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            geo_context_str = "{}" # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ç©ºã®è¾æ›¸

    # ãƒ†ã‚­ã‚¹ãƒˆãŒé•·ã™ãã‚‹å ´åˆã€å…ˆé ­500æ–‡å­—ã«åˆ‡ã‚Šè©°ã‚ã‚‹
    input_texts_jsonl = df_batch.apply(
        lambda row: json.dumps(
            {"id": row['id'], "text": str(row['ANALYSIS_TEXT_COLUMN'])[:500]},
            ensure_ascii=False
        ),
        axis=1
    ).tolist()

    prompt = PromptTemplate.from_template(
        """
        ã‚ãªãŸã¯é«˜ç²¾åº¦ãƒ‡ãƒ¼ã‚¿åˆ†æã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ã€Œã‚«ãƒ†ã‚´ãƒªå®šç¾©ã€ã€Œåœ°åè¾æ›¸ã€ã€Œåˆ†ææŒ‡é‡ã€ã«åŸºã¥ãã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºã—ã¾ã™ã€‚
        # åˆ†ææŒ‡é‡ (Analysis Scope): {analysis_prompt}
        # åœ°åè¾æ›¸ (JAPAN_GEOGRAPHY_DB): {geo_context}
        # ã‚«ãƒ†ã‚´ãƒªå®šç¾© (categories): {categories}
        # ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ (JSONL): {text_data_jsonl}
        # æŒ‡ç¤º:
        1. ã€Œãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿(JSONL)ã€ã®å„è¡Œã‚’å‡¦ç†ã™ã‚‹ã€‚
        2. ã€Œã‚«ãƒ†ã‚´ãƒªå®šç¾©ã€ã®ã‚­ãƒ¼åã‚’ã€å³æ ¼ã«ã€‘ä½¿ç”¨ã—ã€å…¨ã‚«ãƒ†ã‚´ãƒªã‚’æŠ½å‡ºã™ã‚‹ã€‚
        3. ã€"å¸‚åŒºç”ºæ‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰" ä»¥å¤–ã®ã‚«ãƒ†ã‚´ãƒªã€‘:
           - å€¤ã¯å¿…ãšã€ãƒªã‚¹ãƒˆå½¢å¼ã€‘ã§å‡ºåŠ›ï¼ˆè©²å½“ãªã‘ã‚Œã°ç©ºãƒªã‚¹ãƒˆ []ï¼‰ã€‚
        4. ã€"å¸‚åŒºç”ºæ‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰" (æœ€é‡è¦ãƒ»å˜ä¸€å›ç­”)ã€‘:
           - å€¤ã¯ã€å˜ä¸€ã®æ–‡å­—åˆ—ã€‘ã§å‡ºåŠ›ã™ã‚‹ (è©²å½“ãªã‘ã‚Œã°ç©ºæ–‡å­—åˆ— "")ã€‚ãƒªã‚¹ãƒˆå½¢å¼ã¯ã€å³ç¦ã€‘ã€‚
           - æŠ½å‡ºãƒ«ãƒ¼ãƒ«:
             a. ã€Œåœ°åè¾æ›¸ã€ã®ã€å€¤ã€‘(ä¾‹: "å‘‰å¸‚", "ä¸­åŒº") ã¾ãŸã¯ã€ã‚­ãƒ¼ã€‘(ä¾‹: "åºƒå³¶å¸‚") ã«ä¸€è‡´ã™ã‚‹ã€æœ€ã‚‚æ–‡è„ˆã«é–¢é€£æ€§ã®é«˜ã„ã‚‚ã®ã‚’ã€1ã¤ã ã‘ã€‘é¸ã¶ã€‚
             b. (ä¾‹: "åºƒå³¶å¸‚" ã¨ "ä¸­åŒº" ãŒä¸¡æ–¹è¨€åŠã•ã‚Œã¦ã„ã‚Œã°ã€ã‚ˆã‚Šè©³ç´°ãª "ä¸­åŒº" ã‚’å„ªå…ˆã™ã‚‹)
             c. "å®®å³¶" ã®ã‚ˆã†ãªãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯åã¯ã€ãã‚ŒãŒå±ã™ã‚‹ã€Œåœ°åè¾æ›¸ã€ã®å¸‚åŒºç”ºæ‘å (ä¾‹: "å»¿æ—¥å¸‚å¸‚") ã«ã€å¿…ãšå¤‰æ›ã€‘ã—ã¦å›ç­”ã™ã‚‹ã€‚
             d. "åºƒå³¶" ã®ã‚ˆã†ãªæ›–æ˜§ãªè¡¨ç¾ã¯ã€æ–‡è„ˆã‹ã‚‰ (a) ã®ã„ãšã‚Œã‹ã«ç‰¹å®šã§ãã‚‹å ´åˆã®ã¿ (ä¾‹: "åºƒå³¶å¸‚") æŠ½å‡ºã—ã€ç‰¹å®šã§ããªã‘ã‚Œã°ã€ç©ºæ–‡å­—åˆ— ""ã€‘ã¨ã™ã‚‹ã€‚
             e. éƒ½é“åºœçœŒå (ä¾‹: "åºƒå³¶çœŒ")ã€ãŠã‚ˆã³ã€Œè¦³å…‰åœ°ã€ã®ã‚ˆã†ãªåœ°åä»¥å¤–ã®å˜èªã¯ã€çµ¶å¯¾ã«æŠ½å‡ºã—ãªã„ã€‘ã€‚
             f. ã€Œåˆ†ææŒ‡é‡ã€ã¨ç„¡é–¢ä¿‚ãªåœ°åŸŸã®åœ°åï¼ˆä¾‹: æŒ‡é‡ãŒã€Œåºƒå³¶ã€ãªã®ã«ãƒ†ã‚­ã‚¹ãƒˆãŒã€Œæ»‹è³€çœŒã€ï¼‰ã¯ã€æŠ½å‡ºã—ãªã„ã€‘ã€‚
        5. ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆæƒ…å ±ã®æé€ ï¼‰ç¦æ­¢ã€‚
        6. å‡ºåŠ›ã¯ã€JSONLå½¢å¼ã®ã¿ã€‘ï¼ˆid ã¨ categories ã‚’å«ã‚€è¾æ›¸ï¼‰ã€‚
        # å›ç­” (JSONLå½¢å¼ã®ã¿):
        """
    )
    chain = prompt | llm | StrOutputParser()
    
    try:
        invoke_params = {
            "categories": json.dumps(categories_to_tag, ensure_ascii=False),
            "geo_context": geo_context_str,
            "text_data_jsonl": "\n".join(input_texts_jsonl),
            "analysis_prompt": analysis_prompt
        }
        response_str = chain.invoke(invoke_params)
        logger.debug(f"AI Tagging - Raw response received.")

        results = []
        expected_keys = list(categories_to_tag.keys())
        
        # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ ````jsonl ... ``` ã‚’é™¤å»
        match = re.search(r'```(?:jsonl|json)?\s*([\s\S]*?)\s*```', response_str, re.DOTALL)
        jsonl_content = match.group(1).strip() if match else response_str.strip()

        for line in jsonl_content.strip().split('\n'):
            cleaned_line = line.strip()
            if not cleaned_line: continue
            try:
                data = json.loads(cleaned_line)
                row_result = {"id": data.get("id")}
                # AIãŒ 'categories' ã§ãƒ©ãƒƒãƒ—ã™ã‚‹å ´åˆã¨ã€ã—ãªã„å ´åˆã®ä¸¡æ–¹ã«å¯¾å¿œ
                tag_source = data.get('categories', data)
                
                if not isinstance(tag_source, dict):
                    raise json.JSONDecodeError(f"tag_source is not a dict: {tag_source}", "", 0)

                for key in expected_keys:
                    # AIã®å›ç­”ã‚­ãƒ¼ãŒ ' ã‚«ãƒ†ã‚´ãƒª ' ã®ã‚ˆã†ã«ç©ºç™½ã‚’å«ã‚€å ´åˆã«å¯¾å¿œ
                    found_key = None
                    for resp_key in tag_source.keys():
                        if str(resp_key).strip() == key:
                            found_key = resp_key
                            break
                    
                    raw_value = tag_source.get(found_key) if found_key else None

                    # --- "å¸‚åŒºç”ºæ‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰" ã®å‡¦ç† (å˜ä¸€æ–‡å­—åˆ—) ---
                    if key == "å¸‚åŒºç”ºæ‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰":
                        processed_value = ""
                        if isinstance(raw_value, list) and raw_value:
                            # ãƒªã‚¹ãƒˆã§è¿”ã£ã¦ããŸå ´åˆã€æœ€åˆã®è¦ç´ ã‚’æ¡ç”¨
                            processed_value = str(raw_value[0]).strip()
                        elif raw_value is not None and str(raw_value).strip():
                            # æ–‡å­—åˆ—ã§è¿”ã£ã¦ããŸå ´åˆ
                            processed_value = str(raw_value).strip()
                        
                        # è©²å½“ãªã—ç­‰ã®è¡¨ç¾ã‚’ç©ºæ–‡å­—ã«çµ±ä¸€
                        if processed_value.lower() in ["è©²å½“ãªã—", "none", "null", "", "n/a"]:
                            row_result[key] = ""
                        else:
                            row_result[key] = processed_value
                    
                    # --- ãã®ä»–ã®ã‚«ãƒ†ã‚´ãƒªã®å‡¦ç† (ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šæ–‡å­—åˆ—) ---
                    else:
                        processed_values = []
                        if isinstance(raw_value, list):
                            processed_values = sorted(list(set(
                                str(val).strip() for val in raw_value if str(val).strip()
                            )))
                        elif raw_value is not None and str(raw_value).strip():
                            # å˜ä¸€æ–‡å­—åˆ—ã§è¿”ã£ã¦ããŸå ´åˆã‚‚ãƒªã‚¹ãƒˆã«æ ¼ç´
                            processed_values = [str(raw_value).strip()]
                        
                        # æ—¢å­˜ã‚³ãƒ¼ãƒ‰ (L304) ã«åˆã‚ã›ã€ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã®æ–‡å­—åˆ—ã¨ã—ã¦æ ¼ç´
                        row_result[key] = ", ".join(processed_values)
                
                results.append(row_result)
                
            except (json.JSONDecodeError, AttributeError) as json_e:
                logger.warning(f"AIã‚¿ã‚°ä»˜ã‘å›ç­”ãƒ‘ãƒ¼ã‚¹å¤±æ•—: {cleaned_line} - Error: {json_e}")
                id_match = re.search(r'"id":\s*(\d+)', cleaned_line)
                if id_match:
                    # ãƒ‘ãƒ¼ã‚¹å¤±æ•—æ™‚ã¯ã€IDã®ã¿ã®ç©ºã®è¡Œã‚’è¿½åŠ  (ãƒãƒ¼ã‚¸ãŒå¤±æ•—ã—ãªã„ã‚ˆã†ã«)
                    results.append({"id": int(id_match.group(1))})
                    
        return pd.DataFrame(results) if results else pd.DataFrame(columns=['id'] + list(expected_keys))

    except Exception as e:
        logger.error(f"AIã‚¿ã‚°ä»˜ã‘ãƒãƒƒãƒå‡¦ç†ä¸­ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        st.error(f"AIã‚¿ã‚°ä»˜ã‘å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        return pd.DataFrame()  # å¤±æ•—æ™‚ã¯ç©ºã®DF


# --- 7. (â˜…) Step A: UIæç”»é–¢æ•° ---

def update_progress_ui(
    progress_placeholder: st.delta_generator.DeltaGenerator,
    log_placeholder: st.delta_generator.DeltaGenerator,
    tip_placeholder: st.delta_generator.DeltaGenerator,  # (â˜…) Tipsç”¨ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€
    processed_rows: int,
    total_rows: int,
    message_prefix: str
):
    """
    (Step A) ã®é€²æ—ãƒãƒ¼ã¨ãƒ­ã‚°ã‚¨ãƒªã‚¢ã‚’æ›´æ–°ã™ã‚‹ (DRY)
    (â˜…) è¦ä»¶: AIèª­ã¿è¾¼ã¿æ™‚é–“ã®é€²æ—ã‚’0ï½100ï¼…ã§è¡¨ç¤º
    (â˜…) è¦ä»¶: AI Tipsã‚’ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³è¡¨ç¤º
    """
    try:
        # total_rows ãŒ 0 ã®å ´åˆ DivisionByZero ã‚’é˜²ã
        if total_rows == 0:
            progress_percent = 1.0
        else:
            progress_percent = min(processed_rows / total_rows, 1.0)
            
        progress_text = f"[{message_prefix}] å‡¦ç†ä¸­: {processed_rows}/{total_rows} ä»¶ ({progress_percent:.0%})"
        progress_placeholder.progress(progress_percent, text=progress_text)

        # ãƒ­ã‚°è¡¨ç¤º (æœ€æ–°50ä»¶)
        log_text_for_ui = "\n".join(st.session_state.log_messages[-50:])
        log_placeholder.text_area(
            "å®Ÿè¡Œãƒ­ã‚° (æœ€æ–°50ä»¶):",
            log_text_for_ui,
            height=200,
            key=f"log_update_{message_prefix}_{processed_rows}", # é‡è¤‡ã‚­ãƒ¼ã‚’é¿ã‘ã‚‹
            disabled=True
        )
        
        # (â˜…) --- AI Tips ã®ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³è¡¨ç¤º ---
        if 'tips_list' not in st.session_state or not st.session_state.tips_list:
             # ä¸‡ãŒä¸€TipsãŒç©ºã®å ´åˆã¯AI Tipsé–¢æ•°ã‚’å‘¼ã³å‡ºã™
             st.session_state.tips_list = get_analysis_tips_list_from_ai()
             st.session_state.current_tip_index = 0
             st.session_state.last_tip_time = time.time()

        now = time.time()
        # 60ç§’ã”ã¨ï¼ˆã¾ãŸã¯TIPSãŒ1ä»¶ã—ã‹ãªã„å ´åˆï¼‰ã«TIPSã‚’æ›´æ–°
        if (now - st.session_state.last_tip_time > 60) or (len(st.session_state.tips_list) == 1):
            if len(st.session_state.tips_list) > 1:
                st.session_state.current_tip_index = (st.session_state.current_tip_index + 1) % len(st.session_state.tips_list)
            st.session_state.last_tip_time = now
        
        # (â˜…) ãƒªã‚¹ãƒˆãŒç©ºã§ãªã„ã‹ãƒã‚§ãƒƒã‚¯
        if st.session_state.tips_list:
            current_tip = st.session_state.tips_list[st.session_state.current_tip_index]
            tip_placeholder.info(f"ğŸ’¡ ãƒ‡ãƒ¼ã‚¿åˆ†æTIPS: {current_tip}")
        # (â˜…) --- ã“ã“ã¾ã§ãŒå¤‰æ›´ç‚¹ ---

    except Exception as e:
        # UIã®æ›´æ–°ã‚¨ãƒ©ãƒ¼ã¯ãƒ­ã‚°ã«è­¦å‘Šã®ã¿æ®‹ã—ã€å‡¦ç†ã¯ç¶šè¡Œ
        logger.warning(f"UI update failed: {e}")

def render_step_a():
    """(Step A) ã‚¿ã‚°ä»˜ã‘å‡¦ç†ã®UIã‚’æç”»ã™ã‚‹"""
    st.title("ğŸ·ï¸ Step A: AIã‚¿ã‚°ä»˜ã‘ & ã‚­ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³")

    # Step A å›ºæœ‰ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã‚’åˆæœŸåŒ–
    if 'cancel_analysis' not in st.session_state:
        st.session_state.cancel_analysis = False
    if 'generated_categories' not in st.session_state:
        st.session_state.generated_categories = {}
    if 'selected_categories' not in st.session_state:
        st.session_state.selected_categories = set()
    if 'analysis_prompt_A' not in st.session_state:
        st.session_state.analysis_prompt_A = ""
    if 'selected_text_col' not in st.session_state:
        st.session_state.selected_text_col = {}
    if 'tagged_df_A' not in st.session_state:
        st.session_state.tagged_df_A = pd.DataFrame()

    st.header("Step 1: åˆ†æå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
    uploaded_files = st.file_uploader(
        "åˆ†æã—ãŸã„ Excel / CSV ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆè¤‡æ•°å¯ï¼‰",
        type=['csv', 'xlsx', 'xls'],
        accept_multiple_files=True,
        key="uploader_A"
    )

    if not uploaded_files:
        st.info("åˆ†æã‚’é–‹å§‹ã™ã‚‹ã«ã¯ã€Excelã¾ãŸã¯CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
        return

    # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å‡¦ç†
    valid_files_data = {}
    error_messages = []
    for f in uploaded_files:
        df, err = read_file(f)
        if err:
            error_messages.append(f"**{f.name}**: {err}")
        else:
            valid_files_data[f.name] = df
            
    if error_messages:
        st.error("ä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯èª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸ:\n" + "\n".join(error_messages))
    if not valid_files_data:
        st.warning("èª­ã¿è¾¼ã¿å¯èƒ½ãªãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return

    st.header("Step 2: åˆ†ææŒ‡é‡ã®å…¥åŠ›")
    analysis_prompt = st.text_area(
        "AIãŒã‚¿ã‚°ä»˜ã‘ã¨ã‚­ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¡Œã†éš›ã®æŒ‡é‡ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆå¿…é ˆï¼‰:",
        value=st.session_state.analysis_prompt_A,
        height=100,
        placeholder="ä¾‹: åºƒå³¶çœŒã®è¦³å…‰ã«é–¢ã™ã‚‹Instagramã®æŠ•ç¨¿ã€‚ç„¡é–¢ä¿‚ãªåœ°åŸŸã®æŠ•ç¨¿ã‚„ã€å˜ãªã‚‹æŒ¨æ‹¶ãƒ»å®£ä¼ã¯é™¤å¤–ã—ãŸã„ã€‚",
        key="analysis_prompt_input_A"
    )
    st.session_state.analysis_prompt_A = analysis_prompt

    if not analysis_prompt.strip():
        st.warning("åˆ†ææŒ‡é‡ã¯å¿…é ˆã§ã™ã€‚AIãŒãƒ‡ãƒ¼ã‚¿ã‚’ç†è§£ã™ã‚‹ãŸã‚ã«ç›®çš„ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        return

    st.header("Step 3: AIã«ã‚ˆã‚‹ã‚«ãƒ†ã‚´ãƒªå€™è£œã®ç”Ÿæˆ")
    st.markdown(f"ï¼ˆ(â˜…) ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: `{MODEL_FLASH_LITE}`ï¼‰")
    if st.button("AIã«ã‚«ãƒ†ã‚´ãƒªå€™è£œã‚’ç”Ÿæˆã•ã›ã‚‹ (Step 3)", key="gen_cat_button", type="primary"):
        if not os.getenv("GOOGLE_API_KEY"):
            st.error("Google APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ï¼ˆ.envãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„ï¼‰")
        else:
            with st.spinner(f"AI ({MODEL_FLASH_LITE}) ãŒåˆ†ææŒ‡é‡ã‚’èª­ã¿è§£ãã€ã‚«ãƒ†ã‚´ãƒªã‚’è€ƒæ¡ˆä¸­..."):
                logger.info("AIã‚«ãƒ†ã‚´ãƒªç”Ÿæˆãƒœã‚¿ãƒ³ã‚¯ãƒªãƒƒã‚¯")
                # ã€Œå¸‚åŒºç”ºæ‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€ã¯å¿…é ˆã‚«ãƒ†ã‚´ãƒªã¨ã—ã¦å›ºå®š
                st.session_state.generated_categories = {"å¸‚åŒºç”ºæ‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰": "åœ°åè¾æ›¸(JAPAN_GEOGRAPHY_DB)ã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸå¸‚åŒºç”ºæ‘å"}
                
                # (â˜…) Step A ã® AI ã‚«ãƒ†ã‚´ãƒªç”Ÿæˆã‚’å‘¼ã³å‡ºã—
                ai_categories = get_dynamic_categories(analysis_prompt)
                
                if ai_categories:
                    st.session_state.generated_categories.update(ai_categories)
                    logger.info(f"AIã‚«ãƒ†ã‚´ãƒªç”ŸæˆæˆåŠŸ: {list(ai_categories.keys())}")
                    st.success("AIã«ã‚ˆã‚‹ã‚«ãƒ†ã‚´ãƒªå€™è£œã®ç”ŸæˆãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
                else:
                    st.error("AIã«ã‚ˆã‚‹ã‚«ãƒ†ã‚´ãƒªç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚AIã®å¿œç­”ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

    st.header("Step 4: åˆ†æã‚«ãƒ†ã‚´ãƒªã®é¸æŠ")
    if not st.session_state.generated_categories:
        st.info("Step 3 ã§ã‚«ãƒ†ã‚´ãƒªã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚")
        return
        
    st.markdown("ã‚¿ã‚°ä»˜ã‘ã—ãŸã„ã‚«ãƒ†ã‚´ãƒªã‚’ä»¥ä¸‹ã‹ã‚‰é¸æŠã—ã¦ãã ã•ã„ï¼ˆã€Œå¸‚åŒºç”ºæ‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€ã¯å¿…é ˆã§ã™ï¼‰")
    selected_cats = []
    cols = st.columns(3)
    categories_to_show = st.session_state.generated_categories.items()
    
    for i, (cat, desc) in enumerate(categories_to_show):
        with cols[i % 3]:
            is_checked = st.checkbox(
                cat,
                value=(cat == "å¸‚åŒºç”ºæ‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰" or cat in st.session_state.selected_categories),
                help=desc,
                key=f"cat_cb_{cat}",
                disabled=(cat == "å¸‚åŒºç”ºæ‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰")  # å¿…é ˆé …ç›®ã¯ç„¡åŠ¹åŒ–
            )
            if is_checked:
                selected_cats.append(cat)
    st.session_state.selected_categories = set(selected_cats)

    st.header("Step 5: åˆ†æå¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆåˆ—ã®æŒ‡å®š")
    selected_text_col_map = {}
    st.markdown("ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã«ã€ã‚¿ã‚°ä»˜ã‘å¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆãŒå«ã¾ã‚Œã‚‹åˆ—ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚")
    for f_name, df in valid_files_data.items():
        cols_list = list(df.columns)
        default_index = 0
        
        if st.session_state.selected_text_col.get(f_name) in cols_list:
            default_index = cols_list.index(st.session_state.selected_text_col.get(f_name))
        elif any(c in cols_list for c in ['text', 'body', 'content', 'æŠ•ç¨¿', 'æœ¬æ–‡']):
            try:
                default_index = next(i for i, c in enumerate(cols_list) if c in ['text', 'body', 'content', 'æŠ•ç¨¿', 'æœ¬æ–‡'])
            except StopIteration:
                default_index = 0
                
        selected_col = st.selectbox(f"**{f_name}** ã®ãƒ†ã‚­ã‚¹ãƒˆåˆ—:", cols_list, index=default_index, key=f"col_select_{f_name}")
        selected_text_col_map[f_name] = selected_col
    st.session_state.selected_text_col = selected_text_col_map

    st.header("Step 6: åˆ†æå®Ÿè¡Œ")
    st.markdown(f"ï¼ˆ(â˜…) ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: `{MODEL_FLASH_LITE}`ï¼‰")
    
    col_run, col_cancel = st.columns([1, 1])
    with col_cancel:
        if st.button("ã‚­ãƒ£ãƒ³ã‚»ãƒ«", key="cancel_button_A", use_container_width=True):
            st.session_state.cancel_analysis = True
            logger.warning("åˆ†æã‚­ãƒ£ãƒ³ã‚»ãƒ«ãƒœã‚¿ãƒ³ãŒæŠ¼ã•ã‚Œã¾ã—ãŸã€‚")
            st.warning("æ¬¡ã®ãƒãƒƒãƒå‡¦ç†å¾Œã«åˆ†æã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã—ã¾ã™...")
    
    with col_run:
        if st.button("åˆ†æå®Ÿè¡Œ (Step 6)", type="primary", key="run_analysis_A", use_container_width=True):
            st.session_state.cancel_analysis = False
            st.session_state.log_messages = []
            st.session_state.tagged_df_A = pd.DataFrame()
            
            # (â˜…) --- Tipsè¡¨ç¤ºã®åˆæœŸåŒ– ---
            tip_placeholder = st.empty()
            try:
                with st.spinner("åˆ†æTIPSã‚’AIã§ç”Ÿæˆä¸­..."):
                    if 'tips_list' not in st.session_state or not st.session_state.tips_list:
                        st.session_state.tips_list = get_analysis_tips_list_from_ai()
                
                if not st.session_state.tips_list: # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                    st.session_state.tips_list = ["ãƒ‡ãƒ¼ã‚¿åˆ†æTIPSã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚"]

                st.session_state.current_tip_index = random.randint(0, len(st.session_state.tips_list) - 1)
                st.session_state.last_tip_time = time.time()
                tip_placeholder.info(f"ğŸ’¡ ãƒ‡ãƒ¼ã‚¿åˆ†æTIPS: {st.session_state.tips_list[st.session_state.current_tip_index]}")
            except Exception as e:
                logger.error(f"TipsåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            # (â˜…) --- ã“ã“ã¾ã§ãŒå¤‰æ›´ç‚¹ ---

            try:
                with st.spinner(f"Step A: AIåˆ†æå‡¦ç†ä¸­ ({MODEL_FLASH_LITE})..."):
                    logger.info("Step A åˆ†æå®Ÿè¡Œãƒœã‚¿ãƒ³ã‚¯ãƒªãƒƒã‚¯")
                    progress_placeholder = st.progress(0.0, text="å‡¦ç†å¾…æ©Ÿä¸­...")
                    log_placeholder = st.empty()

                    # --- 1. ãƒ•ã‚¡ã‚¤ãƒ«çµåˆ ---
                    update_progress_ui(progress_placeholder, log_placeholder, tip_placeholder, 0, 100, "ãƒ•ã‚¡ã‚¤ãƒ«çµåˆ")
                    temp_dfs = []
                    for f_name, df in valid_files_data.items():
                        col_name = selected_text_col_map[f_name]
                        temp_df = df.rename(columns={col_name: 'ANALYSIS_TEXT_COLUMN'})
                        temp_dfs.append(temp_df)
                    
                    master_df = pd.concat(temp_dfs, ignore_index=True, sort=False)
                    master_df['id'] = master_df.index
                    if master_df.empty:
                        raise Exception("åˆ†æå¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

                    # --- 2. é‡è¤‡å‰Šé™¤ ---
                    initial_row_count = len(master_df)
                    master_df.drop_duplicates(subset=['ANALYSIS_TEXT_COLUMN'], keep='first', inplace=True)
                    deduped_row_count = len(master_df)
                    logger.info(f"é‡è¤‡å‰Šé™¤ å®Œäº†ã€‚ {initial_row_count}è¡Œ -> {deduped_row_count}è¡Œ")

                    # --- 3. (â˜…) AIé–¢é€£æ€§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° (ã‚­ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³) ---
                    total_filter_rows = len(master_df)
                    total_filter_batches = (total_filter_rows + FILTER_BATCH_SIZE - 1) // FILTER_BATCH_SIZE
                    all_filtered_results = []
                    
                    for i in range(0, total_filter_rows, FILTER_BATCH_SIZE):
                        if st.session_state.cancel_analysis:
                            raise Exception("åˆ†æãŒã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ")
                        
                        batch_df = master_df.iloc[i:i + FILTER_BATCH_SIZE]
                        current_batch_num = (i // FILTER_BATCH_SIZE) + 1
                        
                        update_progress_ui(
                            progress_placeholder, log_placeholder, tip_placeholder,
                            min(i + FILTER_BATCH_SIZE, total_filter_rows), total_filter_rows,
                            f"AIã‚­ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ (ãƒãƒƒãƒ {current_batch_num}/{total_filter_batches})"
                        )
                        
                        filtered_df = filter_relevant_data_by_ai(batch_df, analysis_prompt)
                        if filtered_df is not None and not filtered_df.empty:
                            all_filtered_results.append(filtered_df)
                        
                        time.sleep(FILTER_SLEEP_TIME)
                    
                    if not all_filtered_results:
                        raise Exception("AIãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")

                    filter_results_df = pd.concat(all_filtered_results, ignore_index=True)
                    relevant_ids = filter_results_df[filter_results_df['relevant'] == True]['id']
                    filtered_master_df = master_df[master_df['id'].isin(relevant_ids)].copy()
                    filtered_row_count = len(filtered_master_df)
                    logger.info(f"AIãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° å®Œäº†ã€‚ {deduped_row_count}è¡Œ -> {filtered_row_count}è¡Œ")

                    if filtered_master_df.empty:
                        st.warning("AIã‚­ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®çµæœã€åˆ†æå¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ãŒ0ä»¶ã«ãªã‚Šã¾ã—ãŸã€‚")
                        st.session_state.tagged_df_A = pd.DataFrame()
                        progress_placeholder.progress(1.0, text="å‡¦ç†å®Œäº† (å¯¾è±¡ãƒ‡ãƒ¼ã‚¿0ä»¶)")
                        return

                    # --- 4. (â˜…) AIã‚¿ã‚°ä»˜ã‘ ---
                    selected_category_definitions = {
                        cat: desc for cat, desc in st.session_state.generated_categories.items()
                        if cat in st.session_state.selected_categories
                    }
                    
                    master_df_for_tagging = filtered_master_df
                    total_rows = len(master_df_for_tagging)
                    all_tagged_results = []
                    total_batches = (total_rows + TAGGING_BATCH_SIZE - 1) // TAGGING_BATCH_SIZE
                    
                    for i in range(0, total_rows, TAGGING_BATCH_SIZE):
                        if st.session_state.cancel_analysis:
                            raise Exception("åˆ†æãŒã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ")
                        
                        batch_df = master_df_for_tagging.iloc[i:i + TAGGING_BATCH_SIZE]
                        current_batch_num = (i // TAGGING_BATCH_SIZE) + 1
                        
                        update_progress_ui(
                            progress_placeholder, log_placeholder, tip_placeholder,
                            min(i + TAGGING_BATCH_SIZE, total_rows), total_rows,
                            f"AIã‚¿ã‚°ä»˜ã‘ (ãƒãƒƒãƒ {current_batch_num}/{total_batches})"
                        )

                        tagged_df = perform_ai_tagging(batch_df, selected_category_definitions, analysis_prompt)
                        if tagged_df is not None and not tagged_df.empty:
                            all_tagged_results.append(tagged_df)
                        
                        time.sleep(TAGGING_SLEEP_TIME)

                    if not all_tagged_results:
                        raise Exception("AIã‚¿ã‚°ä»˜ã‘å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")

                    # --- 5. æœ€çµ‚ãƒãƒ¼ã‚¸ ---
                    logger.info("å…¨AIã‚¿ã‚°ä»˜ã‘çµæœçµåˆ...");
                    tagged_results_df = pd.concat(all_tagged_results, ignore_index=True)

                    logger.info("æœ€çµ‚ãƒãƒ¼ã‚¸å‡¦ç†é–‹å§‹...");
                    final_df = pd.merge(master_df_for_tagging, tagged_results_df, on='id', how='right')
                    
                    final_cols = list(master_df_for_tagging.columns) + [col for col in tagged_results_df.columns if col not in master_df_for_tagging.columns]
                    final_df = final_df[final_cols]

                    st.session_state.tagged_df_A = final_df
                    logger.info("Step A åˆ†æå‡¦ç† æ­£å¸¸çµ‚äº†");
                    st.success("AIã«ã‚ˆã‚‹åˆ†æå‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚");
                    progress_placeholder.progress(1.0, text="å‡¦ç†å®Œäº†")
                    
                    # (â˜…) --- ä¿®æ­£: æœ€å¾Œã®å‘¼ã³å‡ºã— ---
                    # (â˜…) ã“ã“ãŒ5å¼•æ•°ã«ãªã£ã¦ã„ãŸãŸã‚ã€6å¼•æ•°ã«ä¿®æ­£ã—ã¾ã™
                    update_progress_ui(
                        progress_placeholder, log_placeholder, tip_placeholder, 
                        total_rows, total_rows, "å‡¦ç†å®Œäº†"
                    )
                    # (â˜…) --- ã“ã“ã¾ã§ãŒä¿®æ­£ç‚¹ ---

                    tip_placeholder.empty() # å‡¦ç†å®Œäº†å¾Œã€Tipsã‚’æ¶ˆã™

            except Exception as e:
                logger.error(f"Step A åˆ†æå®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
                st.error(f"åˆ†æå®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                if 'progress_placeholder' in locals():
                    progress_placeholder.progress(1.0, text="ã‚¨ãƒ©ãƒ¼ã«ã‚ˆã‚Šå‡¦ç†ä¸­æ–­")
                if 'tip_placeholder' in locals():
                    tip_placeholder.empty() # ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚Tipsã‚’æ¶ˆã™

    # (â˜…) è¦ä»¶â‘£: ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆãƒªãƒ³ã‚¯ã‚’è¡¨ç¤º
    if not st.session_state.tagged_df_A.empty:
        st.header("Step 7: åˆ†æçµæœã®ç¢ºèªã¨ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ")
        st.dataframe(st.session_state.tagged_df_A.head(50))

        @st.cache_data
        def convert_df_to_csv(df: pd.DataFrame) -> bytes:
            """DataFrameã‚’UTF-8-SIGã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã®CSV (bytes) ã«å¤‰æ›ã™ã‚‹"""
            return df.to_csv(encoding="utf-8-sig", index=False).encode("utf-8-sig")

        csv_data = convert_df_to_csv(st.session_state.tagged_df_A)
        st.download_button(
            label="åˆ†æçµæœCSV (Curated_Data.csv) ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            data=csv_data,
            file_name="Curated_Data.csv",
            mime="text/csv",
        )
        st.info("ã“ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã€Step B ã§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦åˆ†æã‚’ç¶šã‘ã¦ãã ã•ã„ã€‚")

import networkx as nx # (â˜…) Step B (å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯) ã§å¿…è¦
from itertools import combinations # (â˜…) Step B (å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯) ã§å¿…è¦

def suggest_analysis_techniques_py(df: pd.DataFrame) -> List[Dict[str, Any]]:
    """
    (Step B) ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’åˆ†æã—ã€Pythonã§å®Ÿè¡Œå¯èƒ½ãªåŸºæœ¬çš„ãªåˆ†ææ‰‹æ³•ã‚’ææ¡ˆã™ã‚‹ã€‚
    (æ—§ `suggest_analysis_techniques` ã‚’ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°)
    """
    suggestions = []
    if df is None or df.empty:
        logger.error("suggest_analysis_techniques_py: DFãŒç©ºã§ã™ã€‚")
        return suggestions
        
    try:
        numeric_cols = df.select_dtypes(include=np.number).columns.tolist()
        object_cols = df.select_dtypes(include='object').columns.tolist()
        datetime_cols = []

        # æ—¥ä»˜åˆ—ã®å€™è£œã‚’ object å‹ã‹ã‚‰æ¢ã™
        for col in object_cols:
             if df[col].isnull().sum() / len(df) > 0.5: continue # æ¬ æãŒ5å‰²è¶…ãªã‚‰ã‚¹ã‚­ãƒƒãƒ—
             sample = df[col].dropna().head(50)
             if sample.empty: continue
             try:
                 # ã‚µãƒ³ãƒ—ãƒ«ã§æ—¥ä»˜å¤‰æ›ã‚’è©¦ã¿ã‚‹
                 pd.to_datetime(sample, errors='raise')
                 # æˆåŠŸã—ãŸã‚‰å…¨ä½“ã‚’ãƒã‚§ãƒƒã‚¯
                 temp_dt = pd.to_datetime(df[col], errors='coerce').dropna()
                 if not temp_dt.empty and (temp_dt.dt.year.nunique() > 1 or temp_dt.dt.month.nunique() > 1 or temp_dt.dt.day.nunique() > 1 or col.lower() in ['date', 'time', 'timestamp', 'æ—¥ä»˜', 'æ—¥æ™‚']):
                     datetime_cols.append(col)
                     logger.info(f"åˆ— '{col}' ã‚’æ—¥æ™‚åˆ—ã¨ã—ã¦èªè­˜ã—ã¾ã—ãŸã€‚")
             except (ValueError, TypeError, OverflowError, pd.errors.ParserError):
                 pass # æ—¥ä»˜ã§ãªã‘ã‚Œã°ç„¡è¦–

        numeric_cols = [col for col in numeric_cols if col != 'id'] # idåˆ—é™¤å¤–
        categorical_cols = [col for col in object_cols if col != 'ANALYSIS_TEXT_COLUMN' and col not in datetime_cols]
        flag_cols = [col for col in categorical_cols if col.endswith('ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰')]
        other_categorical = [col for col in categorical_cols if not col.endswith('ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰')]
        
        logger.info(f"ææ¡ˆåˆ†æ(PY) - æ•°å€¤:{numeric_cols}, ã‚«ãƒ†ã‚´ãƒª(ãƒ•ãƒ©ã‚°):{flag_cols}, ã‚«ãƒ†ã‚´ãƒª(ä»–):{other_categorical}, æ—¥æ™‚:{datetime_cols}")

        potential_suggestions = []

        # (â˜…) --- 1. å…¨ä½“ãƒ¡ãƒˆãƒªã‚¯ã‚¹ ---
        potential_suggestions.append({
            "priority": 1, "name": "å…¨ä½“ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹",
            "description": "æŠ•ç¨¿æ•°ã€ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆã€ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆå‚¾å‘ãªã©ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã®æ¦‚è¦ã‚’è¨ˆç®—ã—ã¾ã™ã€‚",
            "reason": "ãƒ‡ãƒ¼ã‚¿å…¨ä½“ã®çŠ¶æ³æŠŠæ¡ã«å¿…é ˆã§ã™ã€‚",
            "suitable_cols": [col for col in df.columns if 'ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆ' in col or 'ã„ã„ã­' in col or 'ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ' in col],
            "type": "python"
        })

        # å„ªå…ˆåº¦1: åŸºæœ¬é›†è¨ˆ
        if flag_cols:
            potential_suggestions.append({
                "priority": 1, "name": "å˜ç´”é›†è¨ˆï¼ˆé »åº¦åˆ†æï¼‰",
                "description": "å„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆã‚«ãƒ†ã‚´ãƒªï¼‰ãŒã©ã®ãã‚‰ã„ã®é »åº¦ã§å‡ºç¾ã—ãŸã‹ãƒˆãƒƒãƒ—Nã‚’åˆ†æã—ã¾ã™ã€‚",
                "reason": f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ—({len(flag_cols)}å€‹)ã‚ã‚Šã€‚åŸºæœ¬æŒ‡æ¨™ã§ã™ã€‚",
                "suitable_cols": flag_cols,
                "type": "python" # (â˜…) Pythonå®Ÿè¡Œãƒ•ãƒ©ã‚°
            })

        # (â˜…) --- 1. å¸‚åŒºç”ºæ‘åˆ¥æŠ•ç¨¿æ•° (å˜ç´”é›†è¨ˆã®å…·ä½“åŒ–) ---
        if 'å¸‚åŒºç”ºæ‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰' in flag_cols:
            potential_suggestions.append({
                "priority": 1, "name": "å¸‚åŒºç”ºæ‘åˆ¥æŠ•ç¨¿æ•°",
                "description": "ã€Œå¸‚åŒºç”ºæ‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€åˆ—ã®å‡ºç¾é »åº¦ã‚’åˆ†æã—ã¾ã™ã€‚",
                "reason": "åœ°åŸŸåˆ¥ã®æŠ•ç¨¿ãƒœãƒªãƒ¥ãƒ¼ãƒ ã‚’æŠŠæ¡ã—ã¾ã™ã€‚",
                "suitable_cols": ['å¸‚åŒºç”ºæ‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰'],
                "type": "python"
            })

        # å„ªå…ˆåº¦2: ã‚¯ãƒ­ã‚¹é›†è¨ˆ
        if len(flag_cols) >= 2:
            potential_suggestions.append({
                "priority": 2, "name": "ã‚¯ãƒ­ã‚¹é›†è¨ˆï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é–“ï¼‰",
                "description": "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é–“ã®çµ„ã¿åˆã‚ã›ã§å¤šãå‡ºç¾ã™ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¢ã‚Šã¾ã™ã€‚",
                "reason": f"è¤‡æ•°ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ—({len(flag_cols)}å€‹)ã‚ã‚Šã€é–¢é€£æ€§ã®ç™ºè¦‹ã«ã€‚",
                "suitable_cols": flag_cols,
                "type": "python"
            })
        if flag_cols and other_categorical:
             potential_suggestions.append({
                "priority": 2, "name": "ã‚¯ãƒ­ã‚¹é›†è¨ˆï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰Ã—å±æ€§ï¼‰",
                "description": f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰({flag_cols[0]}ãªã©)ã¨ä»–ã®å±æ€§({', '.join(other_categorical)})ã®é–¢ä¿‚æ€§ã‚’åˆ†æã—ã¾ã™ã€‚",
                "reason": f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ—ã¨ä»–ã‚«ãƒ†ã‚´ãƒªåˆ—({len(other_categorical)}å€‹)ã‚ã‚Šã€‚",
                "suitable_cols": flag_cols + other_categorical,
                "type": "python"
            })
            
        # (â˜…) --- 2. è©±é¡Œã‚«ãƒ†ã‚´ãƒªåˆ¥ è¦³å…‰åœ°TOP10 (ã‚¯ãƒ­ã‚¹é›†è¨ˆã®å…·ä½“åŒ–) ---
        # (â˜…) Step Aã§ 'è©±é¡Œã‚«ãƒ†ã‚´ãƒª' ã¨ 'è¦³å…‰åœ°ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰' ãŒç”Ÿæˆã•ã‚Œã¦ã„ã‚‹å‰æ
        if 'è©±é¡Œã‚«ãƒ†ã‚´ãƒª' in df.columns and 'è¦³å…‰åœ°ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰' in df.columns:
            potential_suggestions.append({
                "priority": 2, "name": "è©±é¡Œã‚«ãƒ†ã‚´ãƒªåˆ¥ è¦³å…‰åœ°TOP10",
                "description": "ã€Œè©±é¡Œã‚«ãƒ†ã‚´ãƒªã€ã¨ã€Œè¦³å…‰åœ°ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€ã‚’ã‚¯ãƒ­ã‚¹é›†è¨ˆã—ã€ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®äººæ°—è¦³å…‰åœ°ã‚’åˆ†æã—ã¾ã™ã€‚",
                "reason": "ã‚«ãƒ†ã‚´ãƒªã¨è¦³å…‰åœ°ã®é–¢é€£æ€§ã‚’åˆ†æã—ã¾ã™ã€‚",
                "suitable_cols": ['è©±é¡Œã‚«ãƒ†ã‚´ãƒª', 'è¦³å…‰åœ°ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰'],
                "type": "python"
            })

        # å„ªå…ˆåº¦3: æ™‚ç³»åˆ—åˆ†æ
        if datetime_cols and flag_cols:
            potential_suggestions.append({
                "priority": 3, "name": "æ™‚ç³»åˆ—ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æ",
                "description": f"ç‰¹å®šã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å‡ºç¾æ•°ãŒæ™‚é–“ï¼ˆ{datetime_cols[0]}ãªã©ï¼‰ã¨ã¨ã‚‚ã«ã©ã†å¤‰åŒ–ã—ãŸã‹ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’åˆ†æã—ã¾ã™ã€‚",
                "reason": f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ—ã¨æ—¥æ™‚åˆ—({len(datetime_cols)}å€‹)ã‚ã‚Šã€‚",
                "suitable_cols": {"datetime": datetime_cols, "keywords": flag_cols},
                "type": "python"
            })
            
        # (â˜…) --- 3. å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ ---
        if 'ANALYSIS_TEXT_COLUMN' in df.columns:
            potential_suggestions.append({
                "priority": 3, "name": "å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯",
                "description": "æŠ•ç¨¿ãƒ†ã‚­ã‚¹ãƒˆå†…ã®å˜èªã®å‡ºç¾ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æã—ã€é–¢é€£æ€§ã®é«˜ã„å˜èªã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚",
                "reason": "ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‹ã‚‰éš ã‚ŒãŸãƒˆãƒ”ãƒƒã‚¯ã‚„é–¢é€£æ€§ã‚’ç™ºè¦‹ã—ã¾ã™ã€‚",
                "suitable_cols": ['ANALYSIS_TEXT_COLUMN'],
                "type": "python"
            })
            
        # å„ªå…ˆåº¦4: ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°
        if 'ANALYSIS_TEXT_COLUMN' in df.columns:
            potential_suggestions.append({
                "priority": 4, "name": "ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°ï¼ˆé »å‡ºå˜èªï¼‰",
                "description": "åŸæ–‡ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰é »å‡ºã™ã‚‹å˜èªã‚’æŠ½å‡ºã—ã€ã©ã®ã‚ˆã†ãªè¨€è‘‰ãŒå¤šãä½¿ã‚ã‚Œã¦ã„ã‚‹ã‹å…¨ä½“åƒã‚’æŠŠæ¡ã—ã¾ã™ã€‚",
                "reason": "åŸæ–‡ãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚Šã€ã‚¿ã‚°ä»˜ã‘ä»¥å¤–ã®ã‚¤ãƒ³ã‚µã‚¤ãƒˆç™ºè¦‹ã«ã€‚",
                "suitable_cols": ['ANALYSIS_TEXT_COLUMN'],
                "type": "python"
            })

        # (â˜…) --- 4. è©±é¡Œã‚«ãƒ†ã‚´ãƒªåˆ¥ ã‚µãƒãƒª (Python + AI) ---
        if 'è©±é¡Œã‚«ãƒ†ã‚´ãƒª' in df.columns and 'ANALYSIS_TEXT_COLUMN' in df.columns:
            potential_suggestions.append({
                "priority": 4, "name": "è©±é¡Œã‚«ãƒ†ã‚´ãƒªåˆ¥ æŠ•ç¨¿æ•°ã¨ã‚µãƒãƒª",
                "description": "æŒ‡å®šã•ã‚ŒãŸè©±é¡Œã‚«ãƒ†ã‚´ãƒªï¼ˆã‚°ãƒ«ãƒ¡ã€è‡ªç„¶ãªã©ï¼‰ã”ã¨ã«æŠ•ç¨¿æ•°ã‚’é›†è¨ˆã—ã€AIãŒæŠ•ç¨¿å†…å®¹ã®ã‚µãƒãƒªã‚’ç”Ÿæˆã—ã¾ã™ã€‚",
                "reason": "ã‚«ãƒ†ã‚´ãƒªã”ã¨ã®ä¸»è¦ãªè©±é¡Œã‚’æŠŠæ¡ã—ã¾ã™ã€‚",
                "suitable_cols": ['è©±é¡Œã‚«ãƒ†ã‚´ãƒª', 'ANALYSIS_TEXT_COLUMN'],
                "type": "python" # (â˜…) AIã‚’å‘¼ã³å‡ºã™ãŒã€ãƒ¡ã‚¤ãƒ³ãƒ­ã‚¸ãƒƒã‚¯ã¯Python
            })

        # (â˜…) --- 4. è©±é¡Œã‚«ãƒ†ã‚´ãƒªåˆ¥ ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆTOP5 (Python + AI) ---
        engagement_cols = [col for col in numeric_cols if any(c in col.lower() for c in ['ã„ã„ã­', 'like', 'ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ', 'engagement'])]
        if 'è©±é¡Œã‚«ãƒ†ã‚´ãƒª' in df.columns and 'ANALYSIS_TEXT_COLUMN' in df.columns and engagement_cols:
            potential_suggestions.append({
                "priority": 4, "name": "è©±é¡Œã‚«ãƒ†ã‚´ãƒªåˆ¥ ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆTOP5ã¨æ¦‚è¦",
                "description": f"æŒ‡å®šã•ã‚ŒãŸè©±é¡Œã‚«ãƒ†ã‚´ãƒªã”ã¨ã«ã€ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆï¼ˆ{engagement_cols[0]}ï¼‰ãŒé«˜ã„TOP5æŠ•ç¨¿ã‚’æŠ½å‡ºã—ã€AIãŒãã®æ¦‚è¦ã‚’ç”Ÿæˆã—ã¾ã™ã€‚",
                "reason": "ã‚«ãƒ†ã‚´ãƒªã”ã¨ã«ã€Œãƒã‚ºã£ãŸã€æŠ•ç¨¿ã®å†…å®¹ã‚’æŠŠæ¡ã—ã¾ã™ã€‚",
                "suitable_cols": ['è©±é¡Œã‚«ãƒ†ã‚´ãƒª', 'ANALYSIS_TEXT_COLUMN'] + engagement_cols,
                "type": "python" # (â˜…) AIã‚’å‘¼ã³å‡ºã™ãŒã€ãƒ¡ã‚¤ãƒ³ãƒ­ã‚¸ãƒƒã‚¯ã¯Python
            })

        suggestions = sorted(potential_suggestions, key=lambda x: x['priority'])
        logger.info(f"Pythonãƒ™ãƒ¼ã‚¹ææ¡ˆ(ã‚½ãƒ¼ãƒˆå¾Œ): {[s['name'] for s in suggestions]}")
        return suggestions

    except Exception as e:
        logger.error(f"Pythonåˆ†ææ‰‹æ³•ææ¡ˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        st.warning(f"åˆ†ææ‰‹æ³•ææ¡ˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
    return suggestions

def suggest_analysis_techniques_ai(
    user_prompt: str,
    df: pd.DataFrame,
    existing_suggestions: List[Dict[str, Any]]
) -> List[Dict[str, Any]]:
    """
    (Step B) ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è‡ªç”±è¨˜è¿°ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«åŸºã¥ãã€AIãŒè¿½åŠ ã®åˆ†ææ‰‹æ³•ã‚’ææ¡ˆã™ã‚‹ã€‚
    (â˜…) ãƒ¢ãƒ‡ãƒ«: MODEL_FLASH_LITE (gemini-2.5-flash-lite)
    (æ—§ `get_suggestions_from_prompt` ã‚’ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°)
    """
    logger.info("AIãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ™ãƒ¼ã‚¹ã®åˆ†æææ¡ˆ (Flash Lite) ã‚’é–‹å§‹...")
    
    # (â˜…) Step B ã®è¦ä»¶ã«åŸºã¥ãã€FLASH_LITE ãƒ¢ãƒ‡ãƒ«ã‚’æ˜ç¤ºçš„ã«æŒ‡å®š
    llm = get_llm(model_name=MODEL_FLASH_LITE, temperature=0.1) # å°‘ã—ã ã‘å‰µé€ æ€§ã‚’æŒãŸã›ã‚‹
    if llm is None:
        logger.error("suggest_analysis_techniques_ai: LLM (Flash Lite) ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")
        return []

    try:
        col_info = []
        for col in df.columns:
            col_info.append(f"- {col} (å‹: {df[col].dtype}, ä¾‹: {df[col].dropna().iloc[0] if not df[col].dropna().empty else 'N/A'})")
        column_info_str = "\n".join(col_info[:15]) # åˆ—ãŒå¤šã™ãã¦ã‚‚å›°ã‚‹ã®ã§æœ€å¤§15
        
        existing_names = [s['name'] for s in existing_suggestions]
        
        prompt = PromptTemplate.from_template(
            """
            ã‚ãªãŸã¯ãƒ‡ãƒ¼ã‚¿åˆ†æã®å°‚é–€å®¶ã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã€Œåˆ†ææŒ‡ç¤ºã€ã¨ã€Œãƒ‡ãƒ¼ã‚¿æ§‹é€ ã€ã‚’èª­ã¿ã€å®Ÿè¡Œå¯èƒ½ãªã€Œåˆ†æã‚¿ã‚¹ã‚¯ã€ã‚’JSONãƒªã‚¹ãƒˆå½¢å¼ã§ææ¡ˆã—ã¦ãã ã•ã„ã€‚
            
            # ãƒ‡ãƒ¼ã‚¿æ§‹é€  (åˆ©ç”¨å¯èƒ½ãªåˆ—å):
            {column_info}
            
            # æ—¢ã«ææ¡ˆæ¸ˆã¿ã®ã‚¿ã‚¹ã‚¯ (ã“ã‚Œã‚‰ã¯ææ¡ˆã—ãªã„ã§ãã ã•ã„):
            {existing_tasks}
            
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®åˆ†ææŒ‡ç¤º:
            {user_prompt}
            
            # æŒ‡ç¤º:
            1. ã€Œãƒ¦ãƒ¼ã‚¶ãƒ¼ã®åˆ†ææŒ‡ç¤ºã€ã‚’è§£é‡ˆã—ã€å…·ä½“çš„ãªåˆ†æã‚¿ã‚¹ã‚¯ï¼ˆä¾‹ï¼šã€Œåºƒå³¶å¸‚ã¨è¦³å…‰åœ°ã®ç›¸é–¢åˆ†æã€ï¼‰ã«åˆ†è§£ã™ã‚‹ã€‚
            2. å„ã‚¿ã‚¹ã‚¯ã‚’ä»¥ä¸‹ã®JSONå½¢å¼ã§å®šç¾©ã™ã‚‹ã€‚
            3. `name`ã¯ã‚¿ã‚¹ã‚¯åã€`description`ã¯AIï¼ˆã‚ãªãŸè‡ªèº«ï¼‰ãŒã“ã®å¾Œå®Ÿè¡Œã™ã‚‹ã‚¿ã‚¹ã‚¯ã®å…·ä½“çš„ãªæŒ‡ç¤ºï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼‰ã¨ã™ã‚‹ã€‚
            4. `priority`ã¯ 5 å›ºå®šã€`type`ã¯ "ai" å›ºå®šã¨ã™ã‚‹ã€‚
            5. æŒ‡ç¤ºãŒç©ºã€ã¾ãŸã¯è§£é‡ˆä¸èƒ½ãªå ´åˆã¯ã€ç©ºãƒªã‚¹ãƒˆ [] ã‚’è¿”ã™ã€‚
            
            # å›ç­” (JSONãƒªã‚¹ãƒˆå½¢å¼ã®ã¿):
            [
              {{
                "priority": 5,
                "name": "ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡ç¤ºã«åŸºã¥ãã‚¿ã‚¹ã‚¯å1ï¼‰",
                "description": "ï¼ˆã“ã®ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã™ã‚‹ãŸã‚ã®AIã¸ã®å…·ä½“çš„ãªæŒ‡ç¤ºãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ1ï¼‰",
                "reason": "ãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡ç¤ºã«åŸºã¥ã",
                "suitable_cols": [],
                "type": "ai"
              }}
            ]
            """
        )
        chain = prompt | llm | StrOutputParser()
        response_str = chain.invoke({
            "column_info": column_info_str,
            "user_prompt": user_prompt,
            "existing_tasks": ", ".join(existing_names)
        })

        logger.info(f"AIè¿½åŠ ææ¡ˆ(ç”Ÿ): {response_str}")
        match = re.search(r'\[.*\]', response_str, re.DOTALL)
        if not match:
            logger.warning("AIãŒJSONãƒªã‚¹ãƒˆå½¢å¼ã§å¿œç­”ã—ã¾ã›ã‚“ã§ã—ãŸã€‚")
            return []
            
        json_str = match.group(0)
        ai_suggestions = json.loads(json_str)
        
        # 'type': 'ai' ãŒä»˜ä¸ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª (AIã®æºã‚‰ãå¸å)
        for s in ai_suggestions:
            s['type'] = 'ai'
            if 'priority' not in s: s['priority'] = 5
            
        logger.info(f"AIè¿½åŠ ææ¡ˆ(ãƒ‘ãƒ¼ã‚¹æ¸ˆ): {len(ai_suggestions)}ä»¶")
        return ai_suggestions

    except Exception as e:
        logger.error(f"AIè¿½åŠ ææ¡ˆã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        st.warning(f"AIè¿½åŠ ææ¡ˆã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return []

# (è¦ä»¶â‘¥: Pythonã§ã§ãã‚‹éƒ¨åˆ†ã¯Pythonã§)
import networkx as nx # (â˜…) Step B (å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯) ã§å¿…è¦
from itertools import combinations # (â˜…) Step B (å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯) ã§å¿…è¦
import math # (â˜…) ã‚°ãƒ©ãƒ•ã®ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆè¨ˆç®—ç”¨

# --- 8.0. (â˜…) æ–°è¦è¿½åŠ : ã‚°ãƒ©ãƒ•ç”Ÿæˆãƒ˜ãƒ«ãƒ‘ãƒ¼ ---

def generate_graph_image(
    df: pd.DataFrame,
    plot_type: str,
    x_col: Optional[str] = None,
    y_col: Optional[str] = None,
    title: str = "åˆ†æã‚°ãƒ©ãƒ•"
) -> Optional[str]:
    """
    (â˜…) DataFrameã‹ã‚‰matplotlibã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆã—ã€Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸç”»åƒæ–‡å­—åˆ—ã‚’è¿”ã™ã€‚
    """
    logger.info(f"ã‚°ãƒ©ãƒ•ç”Ÿæˆé–‹å§‹: {title} (ã‚¿ã‚¤ãƒ—: {plot_type})")
    if df is None or df.empty:
        logger.warning("ã‚°ãƒ©ãƒ•ç”Ÿæˆã‚¹ã‚­ãƒƒãƒ—: DataFrameãŒç©ºã§ã™ã€‚")
        return None

    # (â˜…) ã‚°ãƒ©ãƒ•æç”»é ˜åŸŸã®åˆæœŸåŒ– (ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚ºã‚‚æŒ‡å®š)
    plt.figure(figsize=(10, 7)) # (â˜…) å°‘ã—ç¸¦é•·ã«å¤‰æ›´
    plt.rcParams['font.size'] = 12
    
    try:
        if plot_type == 'bar' and x_col and y_col:
            # (â˜…) ä¸Šä½20ä»¶ã«çµã‚‹ (å¤šã™ãã‚‹ã¨æç”»ã§ããªã„ãŸã‚)
            df_plot = df.nlargest(20, y_col).sort_values(by=y_col, ascending=True)
            if df_plot.empty:
                raise ValueError("ã‚°ãƒ©ãƒ•æç”»å¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
                
            bars = plt.barh(df_plot[x_col], df_plot[y_col], color='#7280C1') # (â˜…) è¦‹æœ¬ã®è‰² [cite: 36]
            plt.xlabel('ä»¶æ•°')
            plt.ylabel(x_col)
            plt.grid(axis='x', linestyle='--', alpha=0.6)
            
            # (â˜…) ãƒãƒ¼ã®å³å´ã«æ•°å€¤ã‚’è¡¨ç¤º
            for bar in bars:
                plt.text(
                    bar.get_width() + (df_plot[y_col].max() * 0.01), # (â˜…) ãƒãƒ¼ã®å³å´ã«ã‚ªãƒ•ã‚»ãƒƒãƒˆ
                    bar.get_y() + bar.get_height() / 2,
                    f' {bar.get_width():.0f}',
                    va='center',
                    ha='left'
                )
        
        elif plot_type == 'timeseries' and x_col and y_col:
            # (â˜…) æ™‚ç³»åˆ—ã‚°ãƒ©ãƒ• (x_col = 'date', y_col = 'count', hue = 'keyword')
            try:
                df[x_col] = pd.to_datetime(df[x_col])
                df_pivot = df.pivot(index=x_col, columns='keyword', values=y_col).fillna(0)
                
                # (â˜…) ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå¤šã™ãã‚‹å ´åˆã€ä¸Šä½5ä»¶+ãã®ä»– ã«é›†ç´„
                if len(df_pivot.columns) > 6:
                    top_5_keywords = df_pivot.sum().nlargest(5).index
                    df_pivot['ãã®ä»–'] = df_pivot.drop(columns=top_5_keywords).sum(axis=1)
                    df_pivot = df_pivot[list(top_5_keywords) + ['ãã®ä»–']]
                
                df_pivot.plot(kind='line', ax=plt.gca(), linewidth=2.5) # (â˜…) ç·šã‚’å¤ªã
                plt.xlabel('æ—¥ä»˜')
                plt.ylabel('ä»¶æ•°')
                plt.legend(title='ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰', bbox_to_anchor=(1.05, 1), loc='upper left')
                plt.grid(axis='y', linestyle='--', alpha=0.6)
                
            except Exception as e:
                logger.error(f"æ™‚ç³»åˆ—ãƒ”ãƒœãƒƒãƒˆ/ãƒ—ãƒ­ãƒƒãƒˆã‚¨ãƒ©ãƒ¼: {e}")
                plt.plot(df[x_col], df[y_col]) # (â˜…) ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                plt.xlabel(x_col)
                plt.ylabel(y_col)

        elif plot_type == 'network':
            # (â˜…) å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ (df = ã‚¨ãƒƒã‚¸ãƒªã‚¹ãƒˆ)
            # (â˜…) Top 100 ã‚¨ãƒƒã‚¸ã«çµã‚‹
            df_plot = df.nlargest(100, 'weight')
            if df_plot.empty:
                raise ValueError("ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æç”»å¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

            G = nx.from_pandas_edgelist(df_plot, 'source', 'target', ['weight'])
            
            # (â˜…) ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£æ¤œå‡º (Louvain)
            try:
                partition = community.best_partition(G)
                num_communities = len(set(partition.values()))
                # (â˜…) è‰²ã®ãƒãƒƒãƒ—ã‚’å‹•çš„ã«ç”Ÿæˆ (è¦‹æœ¬PDFã®ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰é¢¨ã®è‰²åˆã„)
                colors = plt.cm.get_cmap('tab20', num_communities)
                node_colors = [colors(partition.get(node)) for node in G.nodes()]
            except Exception:
                node_colors = '#7280C1' # (â˜…) ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£æ¤œå‡ºå¤±æ•—æ™‚ã¯å˜è‰² [cite: 36]
            
            # (â˜…) ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã‚’èª¿æ•´ (kå€¤ã‚’ãƒãƒ¼ãƒ‰æ•°ã«å¿œã˜ã¦èª¿æ•´)
            k_val = 1.5 / math.sqrt(len(G.nodes()))
            pos = nx.spring_layout(G, k=max(k_val, 0.5), iterations=50, seed=42)
            
            # (â˜…) ãƒãƒ¼ãƒ‰ã®å¤§ãã•ã‚’é‡ã¿ (æ¥ç¶šå¼·åº¦) ã«åŸºã¥ã„ã¦è¨­å®š
            node_sizes = []
            try:
                for node in G.nodes():
                    total_weight = sum(data['weight'] for _, _, data in G.edges(node, data=True))
                    node_sizes.append(total_weight * 20) # (â˜…) ä¿‚æ•°èª¿æ•´
            except Exception:
                node_sizes = 500 # (â˜…) ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            
            # (â˜…) ã‚¨ãƒƒã‚¸ã®å¤ªã•ã‚’é‡ã¿ã«åŸºã¥ã„ã¦è¨­å®š
            edge_weights = [d['weight'] / df_plot['weight'].max() * 8 for u, v, d in G.edges(data=True)]

            nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color=node_colors, alpha=0.9)
            nx.draw_networkx_edges(G, pos, width=edge_weights, alpha=0.2, edge_color='grey')
            nx.draw_networkx_labels(G, pos, font_size=10, font_family='IPAGothic') # (â˜…) æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆæŒ‡å®š
            plt.axis('off')

        else:
            logger.warning(f"æœªå¯¾å¿œã®ãƒ—ãƒ­ãƒƒãƒˆã‚¿ã‚¤ãƒ—: {plot_type}")
            return None

        plt.title(title, fontsize=16, pad=20) # (â˜…) ã‚¿ã‚¤ãƒˆãƒ«
        plt.tight_layout() # (â˜…) ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆè‡ªå‹•èª¿æ•´

        # (â˜…) ã‚°ãƒ©ãƒ•ã‚’ãƒ¡ãƒ¢ãƒª (BytesIO) ã«ä¿å­˜
        buf = BytesIO()
        plt.savefig(buf, format='png', dpi=150) # (â˜…) DPIã‚’æŒ‡å®šã—ã¦è§£åƒåº¦ã‚’èª¿æ•´
        buf.seek(0)
        
        # (â˜…) Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ã¦æ–‡å­—åˆ—ã¨ã—ã¦è¿”ã™
        image_base64 = base64.b64encode(buf.getvalue()).decode('utf-8')
        logger.info(f"ã‚°ãƒ©ãƒ•ç”ŸæˆæˆåŠŸ: {title}")
        return image_base64

    except Exception as e:
        logger.error(f"ã‚°ãƒ©ãƒ•ç”Ÿæˆ ({title}) å¤±æ•—: {e}", exc_info=True)
        return None
    finally:
        plt.clf() # (â˜…) æç”»é ˜åŸŸã‚’ã‚¯ãƒªã‚¢
        plt.close('all') # (â˜…) Figureã‚’æ˜ç¤ºçš„ã«é–‰ã˜ã‚‹


# --- 8.1. (â˜…) Step B: Pythonåˆ†æãƒ˜ãƒ«ãƒ‘ãƒ¼ (ã‚°ãƒ©ãƒ•ç”Ÿæˆæ©Ÿèƒ½ã®çµ„è¾¼ã¿) ---

def run_simple_count(df: pd.DataFrame, suggestion: Dict[str, Any]) -> Dict[str, Any]:
    """(Step B) å˜ç´”é›†è¨ˆï¼ˆé »åº¦åˆ†æï¼‰ã‚’å®Ÿè¡Œã—ã€DataFrameã¨ã‚°ãƒ©ãƒ•(Base64)ã‚’è¿”ã™"""
    # (â˜…) æˆ»ã‚Šå€¤ã‚’ Dict[str, Any] ã«çµ±ä¸€
    results = {"data": pd.DataFrame(), "image_base64": None, "summary": ""}
    
    flag_cols = suggestion.get('suitable_cols', [])
    if not flag_cols:
        msg = "é›†è¨ˆå¯¾è±¡ã®åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
        logger.warning(f"run_simple_count: {msg}")
        results["summary"] = msg
        return results
    
    col_to_analyze = flag_cols[0]
    
    if col_to_analyze not in df.columns:
        msg = f"åˆ— '{col_to_analyze}' ãŒDFã«å­˜åœ¨ã—ã¾ã›ã‚“ã€‚"
        logger.warning(f"run_simple_count: {msg}")
        results["summary"] = msg
        return results
        
    try:
        s = df[col_to_analyze].astype(str).str.split(', ').explode()
        s = s[s.str.strip().isin(['', 'nan', 'None', 'N/A', 'è©²å½“ãªã—']) == False] # (â˜…) 'è©²å½“ãªã—' ã‚‚é™¤å¤–
        s = s.str.strip()
        
        if s.empty:
            msg = "é›†è¨ˆå¯¾è±¡ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
            logger.info(f"run_simple_count: {msg}")
            results["summary"] = msg
            return results
            
        counts = s.value_counts().head(50)
        counts_df = counts.reset_index()
        counts_df.columns = [col_to_analyze, 'count']
        
        results["data"] = counts_df
        results["summary"] = f"'{col_to_analyze}' ã®å˜ç´”é›†è¨ˆï¼ˆé »åº¦åˆ†æï¼‰ã‚’å®Ÿè¡Œã€‚ä¸Šä½ã¯ {counts_df.iloc[0,0]} ({counts_df.iloc[0,1]}ä»¶), {counts_df.iloc[1,0]} ({counts_df.iloc[1,1]}ä»¶) ã§ã—ãŸã€‚"
        
        # (â˜…) ã‚°ãƒ©ãƒ•ç”Ÿæˆ
        results["image_base64"] = generate_graph_image(
            df=counts_df,
            plot_type='bar',
            x_col=col_to_analyze,
            y_col='count',
            title=f"ã€Œ{col_to_analyze}ã€ é »å‡ºTOP20"
        )
        return results
            
    except Exception as e:
        logger.error(f"run_simple_count error: {e}", exc_info=True)
        results["summary"] = f"ã‚¨ãƒ©ãƒ¼: {e}"
    return results

def run_crosstab(df: pd.DataFrame, suggestion: Dict[str, Any]) -> Dict[str, Any]:
    """(Step B) ã‚¯ãƒ­ã‚¹é›†è¨ˆã‚’å®Ÿè¡Œã—ã€DataFrameã‚’è¿”ã™ (â˜…ã‚°ãƒ©ãƒ•ç”Ÿæˆã¯è¤‡é›‘ãªãŸã‚ãƒ‡ãƒ¼ã‚¿ã®ã¿)"""
    results = {"data": pd.DataFrame(), "image_base64": None, "summary": ""}
    
    cols = suggestion.get('suitable_cols', [])
    if len(cols) < 2:
        msg = "ã‚¯ãƒ­ã‚¹é›†è¨ˆã«ã¯2åˆ—ä»¥ä¸Šå¿…è¦ã§ã™ã€‚"
        logger.warning(f"run_crosstab: {msg}")
        results["summary"] = msg
        return results

    existing_cols = [col for col in cols if col in df.columns]
    if len(existing_cols) < 2:
        msg = f"DFå†…ã«å­˜åœ¨ã™ã‚‹åˆ—ãŒ2æœªæº€: {existing_cols}"
        logger.warning(f"run_crosstab: {msg}")
        results["summary"] = msg
        return results

    col1, col2 = existing_cols[0], existing_cols[1]
    
    try:
        # (â˜…) ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šæ–‡å­—åˆ—ã‚’ explode (åˆ†è§£) ã™ã‚‹å‡¦ç†ã‚’æ±ç”¨åŒ–
        df_exploded_1 = df.assign(**{col1: df[col1].astype(str).str.split(', ')}).explode(col1)
        df_exploded_2 = df_exploded_1.assign(**{col2: df_exploded_1[col2].astype(str).str.split(', ')}).explode(col2)

        # (â˜…) NaN/ç©ºæ–‡å­—ã‚’é™¤å»
        df_exploded_2[col1] = df_exploded_2[col1].str.strip()
        df_exploded_2[col2] = df_exploded_2[col2].str.strip()
        df_exploded_2 = df_exploded_2.replace('', np.nan).replace('nan', np.nan).replace('None', np.nan).dropna(subset=[col1, col2])
        
        crosstab_df = pd.crosstab(df_exploded_2[col1], df_exploded_2[col2])
        
        crosstab_long = crosstab_df.stack().reset_index()
        crosstab_long.columns = [col1, col2, 'count']
        crosstab_long = crosstab_long[crosstab_long['count'] > 0].sort_values(by='count', ascending=False)
        
        results["data"] = crosstab_long.head(100)
        
        # (â˜…) TODO: ã‚¯ãƒ­ã‚¹é›†è¨ˆã®ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—æç”»ã¯ã€ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã«ã‚ˆã£ã¦è¤‡é›‘ã«ãªã‚‹ãŸã‚ã€
        # StepCã®AIã«ã€Œã“ã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã§å¯è¦–åŒ–ã—ã¦ã€ã¨ä¾é ¼ã™ã‚‹æ–¹ãŒç¾å®Ÿçš„ã€‚
        # ã“ã“ã§ã¯ã‚°ãƒ©ãƒ•ç”Ÿæˆã¯ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹ã€‚
        results["summary"] = f"'{col1}' ã¨ '{col2}' ã®ã‚¯ãƒ­ã‚¹é›†è¨ˆã‚’å®Ÿè¡Œã€‚æœ€å¼·ã®çµ„ã¿åˆã‚ã›ã¯ {crosstab_long.iloc[0,0]} x {crosstab_long.iloc[0,1]} ({crosstab_long.iloc[0,2]}ä»¶) ã§ã—ãŸã€‚"
        logger.info("run_crosstab: ã‚°ãƒ©ãƒ•ç”Ÿæˆã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã—ãŸã€‚")
        
        return results
        
    except Exception as e:
        logger.error(f"run_crosstab error: {e}", exc_info=True)
        results["summary"] = f"ã‚¨ãƒ©ãƒ¼: {e}"
    return results

def run_timeseries(df: pd.DataFrame, suggestion: Dict[str, Any]) -> Dict[str, Any]:
    """(Step B) æ™‚ç³»åˆ—åˆ†æã‚’å®Ÿè¡Œã—ã€DataFrameã¨ã‚°ãƒ©ãƒ•(Base64)ã‚’è¿”ã™"""
    results = {"data": pd.DataFrame(), "image_base64": None, "summary": ""}
    
    cols_dict = suggestion.get('suitable_cols', {})
    if not isinstance(cols_dict, dict) or 'datetime' not in cols_dict or 'keywords' not in cols_dict:
        msg = "åˆ—æƒ…å ±ï¼ˆdatetime, keywordsï¼‰ãŒä¸ååˆ†ã§ã™ã€‚"
        logger.warning(f"run_timeseries: {msg}")
        results["summary"] = msg
        return results
        
    dt_cols = [col for col in cols_dict['datetime'] if col in df.columns]
    kw_cols = [col for col in cols_dict['keywords'] if col in df.columns]

    if not dt_cols:
        msg = "æ—¥æ™‚åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
        logger.warning(f"run_timeseries: {msg}"); results["summary"] = msg; return results
    if not kw_cols:
        msg = "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
        logger.warning(f"run_timeseries: {msg}"); results["summary"] = msg; return results

    dt_col = dt_cols[0]
    kw_col = kw_cols[0]

    try:
        df_copy = df[[dt_col, kw_col]].copy()
        df_copy[dt_col] = pd.to_datetime(df_copy[dt_col], errors='coerce')
        df_copy = df_copy.dropna(subset=[dt_col])
        
        df_exploded = df_copy.assign(**{kw_col: df_copy[kw_col].astype(str).str.split(', ')}).explode(kw_col)
        df_exploded[kw_col] = df_exploded[kw_col].str.strip()
        df_exploded = df_exploded[df_exploded[kw_col].isin(['', 'nan', 'None', 'N/A', 'è©²å½“ãªã—']) == False]
        
        if df_exploded.empty:
            msg = "æœ‰åŠ¹ãªæ—¥æ™‚/ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚"
            logger.info(f"run_timeseries: {msg}"); results["summary"] = msg; return results

        time_df = df_exploded.groupby([pd.Grouper(key=dt_col, freq='D'), kw_col]).size().rename("count").reset_index()
        
        time_df.columns = ['date', 'keyword', 'count']
        
        top_keywords = df_exploded[kw_col].value_counts().head(50).index
        time_df_filtered = time_df[time_df['keyword'].isin(top_keywords)]
        
        # (â˜…) ã‚°ãƒ©ãƒ•ç”Ÿæˆç”¨ã«æ—¥ä»˜ã‚’ dtå‹ã€JSONç”¨ã«æ–‡å­—åˆ—å‹ã‚’ç”¨æ„
        time_df_for_graph = time_df_filtered.copy()
        
        time_df_for_json = time_df_filtered.sort_values(by=['keyword', 'date'])
        time_df_for_json['date'] = time_df_for_json['date'].dt.strftime('%Y-%m-%d')
        
        results["data"] = time_df_for_json
        
        # (â˜…) ã‚°ãƒ©ãƒ•ç”Ÿæˆ
        results["image_base64"] = generate_graph_image(
            df=time_df_for_graph,
            plot_type='timeseries',
            x_col='date',
            y_col='count',
            title=f"ã€Œ{kw_col}ã€åˆ¥ æ™‚ç³»åˆ—ãƒˆãƒ¬ãƒ³ãƒ‰ (TOP5)"
        )
        results["summary"] = f"'{dt_col}' ã¨ '{kw_col}' ã§æ™‚ç³»åˆ—åˆ†æã‚’å®Ÿè¡Œã€‚TOP5ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ãƒˆãƒ¬ãƒ³ãƒ‰ã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆã—ã¾ã—ãŸã€‚"
        return results
            
    except Exception as e:
        logger.error(f"run_timeseries error: {e}", exc_info=True)
        results["summary"] = f"ã‚¨ãƒ©ãƒ¼: {e}"
    return results

def run_text_mining(df: pd.DataFrame, suggestion: Dict[str, Any]) -> Dict[str, Any]:
    """(Step B) ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°ï¼ˆé »å‡ºå˜èªï¼‰ã‚’å®Ÿè¡Œã—ã€DataFrameã¨ã‚°ãƒ©ãƒ•(Base64)ã‚’è¿”ã™"""
    results = {"data": pd.DataFrame(), "image_base64": None, "summary": ""}
    
    text_col = suggestion.get('suitable_cols', ['ANALYSIS_TEXT_COLUMN'])[0]
    if text_col not in df.columns or df[text_col].empty:
        msg = f"ãƒ†ã‚­ã‚¹ãƒˆåˆ— '{text_col}' ãŒãªã„ã‹ã€ç©ºã§ã™ã€‚"
        logger.warning(f"run_text_mining: {msg}"); results["summary"] = msg; return results

    nlp = load_spacy_model()
    if nlp is None:
        st.error("spaCyæ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
        results["summary"] = "spaCyæ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸã€‚"
        return results
            
    try:
        texts = df[text_col].dropna().astype(str)
        if texts.empty:
            results["summary"] = "ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ã€‚"
            return results
            
        words = []
        target_pos = {'NOUN', 'PROPN', 'ADJ'} # (åè©, å›ºæœ‰åè©, å½¢å®¹è©)
        stop_words = {
            'ã®', 'ã«', 'ã¯', 'ã‚’', 'ãŒ', 'ã§', 'ã¦', 'ã§ã™', 'ã¾ã™', 'ã“ã¨', 'ã‚‚ã®', 'ãã‚Œ', 'ã‚ã‚Œ',
            'ã“ã‚Œ', 'ãŸã‚', 'ã„ã‚‹', 'ã™ã‚‹', 'ã‚ã‚‹', 'ãªã„', 'ã„ã†', 'ã‚ˆã†', 'ãã†', 'ãªã©', 'ã•ã‚“',
            'çš„', 'äºº', 'è‡ªåˆ†', 'ç§', 'åƒ•', 'ä½•', 'ãã®', 'ã“ã®', 'ã‚ã®' # (â˜…)ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰è¿½åŠ  (å…ƒã®ã‚³ãƒ¼ãƒ‰L1200)
        }
        
        total_texts = len(texts)
        if 'progress_text' not in st.session_state:
             st.session_state.progress_text = ""
        st.session_state.progress_text = "ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚° (spaCy) å‡¦ç†ä¸­... 0%"

        for i, doc in enumerate(nlp.pipe(texts, disable=["parser", "ner"], batch_size=50)):
            for token in doc:
                if (token.pos_ in target_pos) and (not token.is_stop) and (token.lemma_ not in stop_words) and (len(token.lemma_) > 1):
                    words.append(token.lemma_)
            
            if (i + 1) % 100 == 0:
                percent = (i + 1) / total_texts
                st.session_state.progress_text = f"ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚° (spaCy) å‡¦ç†ä¸­... {percent:.0%}"

        if not words:
            msg = "æŠ½å‡ºå¯èƒ½ãªæœ‰åŠ¹ãªå˜èªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
            logger.warning(f"run_text_mining: {msg}"); results["summary"] = msg; return results

        word_counts = pd.Series(words).value_counts().head(100)
        word_counts_df = word_counts.reset_index()
        word_counts_df.columns = ['word', 'count']
        
        st.session_state.progress_text = "ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚° (spaCy) å®Œäº†ã€‚"
        
        results["data"] = word_counts_df
        results["summary"] = f"ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œã€‚é »å‡ºå˜èªã¯ '{word_counts_df.iloc[0,0]}' ({word_counts_df.iloc[0,1]}ä»¶), '{word_counts_df.iloc[1,0]}' ({word_counts_df.iloc[1,1]}ä»¶) ã§ã—ãŸã€‚"
        
        # (â˜…) ã‚°ãƒ©ãƒ•ç”Ÿæˆ (ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã®ä»£ã‚ã‚Šã«æ£’ã‚°ãƒ©ãƒ•)
        results["image_base64"] = generate_graph_image(
            df=word_counts_df,
            plot_type='bar',
            x_col='word',
            y_col='count',
            title="é »å‡ºå˜èª TOP20"
        )
        return results
        
    except Exception as e:
        logger.error(f"run_text_mining error: {e}", exc_info=True)
        results["summary"] = f"ã‚¨ãƒ©ãƒ¼: {e}"
    return results

# (â˜…) --- run_overall_metrics ---
def run_overall_metrics(df: pd.DataFrame, suggestion: Dict[str, Any]) -> Dict[str, Any]:
    """(Step B) ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—ã™ã‚‹ (â˜…æˆ»ã‚Šå€¤ã‚’Dict[str, Any]ã«çµ±ä¸€)"""
    logger.info("run_overall_metrics å®Ÿè¡Œ...")
    metrics = {}
    try:
        metrics["total_posts"] = len(df)

        engagement_cols = [col for col in df.columns if any(c in col.lower() for c in ['ã„ã„ã­', 'like', 'ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ', 'engagement', 'retweet', 'ãƒªãƒ„ã‚¤ãƒ¼ãƒˆ'])]
        total_engagement = 0
        if engagement_cols:
            for col in engagement_cols:
                if pd.api.types.is_numeric_dtype(df[col]):
                    total_engagement += df[col].sum()
            metrics["total_engagement"] = int(total_engagement)
        else:
            metrics["total_engagement"] = "N/A"

        sentiment_col = None
        if 'ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆ' in df.columns:
            sentiment_col = 'ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆ'
        elif len(df.columns) > 9 and ('ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆ' in str(df.columns[9]) or 'sentiment' in str(df.columns[9]).lower()): # (â˜…) Jåˆ— (ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹9) (å…ƒã®ã‚³ãƒ¼ãƒ‰L1141)
            sentiment_col = df.columns[9]
            
        if sentiment_col:
            # (â˜…) ãƒã‚¸/ãƒã‚¬ã®åˆ¤å®šã‚’æŸ”è»Ÿã«
            pos_count = int(df[df[sentiment_col].astype(str).str.contains('ãƒã‚¸ãƒ†ã‚£ãƒ–|Positive', case=False, na=False)].shape[0])
            neg_count = int(df[df[sentiment_col].astype(str).str.contains('ãƒã‚¬ãƒ†ã‚£ãƒ–|Negative', case=False, na=False)].shape[0])
            
            metrics["positive_posts"] = pos_count
            metrics["negative_posts"] = neg_count
            
            if (pos_count + neg_count) > 0:
                tendency = ((pos_count - neg_count) / (pos_count + neg_count)) * 100
                metrics["sentiment_tendency_percent"] = int(np.floor(tendency))
            else:
                metrics["sentiment_tendency_percent"] = 0
        else:
            logger.warning("åˆ— 'ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆ' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            metrics["positive_posts"] = "N/A"
            metrics["negative_posts"] = "N/A"
            metrics["sentiment_tendency_percent"] = "N/A"

        summary = f"å…¨ä½“ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—ã€‚ç·æŠ•ç¨¿æ•°: {metrics['total_posts']}ä»¶, ç·ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ: {metrics['total_engagement']}ã€‚"
        
        # (â˜…) æˆ»ã‚Šå€¤ã‚’çµ±ä¸€
        return {"data": metrics, "image_base64": None, "summary": summary}

    except Exception as e:
        logger.error(f"run_overall_metrics error: {e}", exc_info=True)
        return {"data": {"error": str(e)}, "image_base64": None, "summary": f"ã‚¨ãƒ©ãƒ¼: {e}"}

# (â˜…) --- run_cooccurrence_network ---
def run_cooccurrence_network(df: pd.DataFrame, suggestion: Dict[str, Any]) -> Dict[str, Any]:
    """(Step B) å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æ§‹ç¯‰ã—ã€ã‚¨ãƒƒã‚¸ãƒªã‚¹ãƒˆã®DataFrameã¨ã‚°ãƒ©ãƒ•(Base64)ã‚’è¿”ã™"""
    logger.info("run_cooccurrence_network å®Ÿè¡Œ...")
    results = {"data": pd.DataFrame(), "image_base64": None, "summary": ""}
    
    text_col = suggestion.get('suitable_cols', ['ANALYSIS_TEXT_COLUMN'])[0]
    if text_col not in df.columns or df[text_col].empty:
        msg = f"ãƒ†ã‚­ã‚¹ãƒˆåˆ— '{text_col}' ãŒãªã„ã‹ã€ç©ºã§ã™ã€‚"
        logger.warning(f"run_cooccurrence_network: {msg}"); results["summary"] = msg; return results

    nlp = load_spacy_model()
    if nlp is None:
        st.error("spaCyæ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
        results["summary"] = "spaCyæ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸã€‚"
        return results

    try:
        texts = df[text_col].dropna().astype(str)
        if texts.empty:
            results["summary"] = "ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ã€‚"
            return results

        target_pos = {'NOUN', 'PROPN', 'ADJ'}
        stop_words = {
            'ã®', 'ã«', 'ã¯', 'ã‚’', 'ãŒ', 'ã§', 'ã¦', 'ã§ã™', 'ã¾ã™', 'ã“ã¨', 'ã‚‚ã®', 'ãã‚Œ', 'ã‚ã‚Œ',
            'ã“ã‚Œ', 'ãŸã‚', 'ã„ã‚‹', 'ã™ã‚‹', 'ã‚ã‚‹', 'ãªã„', 'ã„ã†', 'ã‚ˆã†', 'ãã†', 'ãªã©', 'ã•ã‚“',
            'çš„', 'äºº', 'è‡ªåˆ†', 'ç§', 'åƒ•', 'ä½•', 'ãã®', 'ã“ã®', 'ã‚ã®'
        }
        
        all_words = []
        # (â˜…) spaCyã®å‡¦ç†ã‚’UIã«é€²æ—è¡¨ç¤º
        st.session_state.progress_text = "å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ (spaCy) å‡¦ç†ä¸­... 0%"
        total_texts = len(texts)
        
        doc_words_list = [] # (â˜…) æŠ•ç¨¿ã”ã¨ã®å˜èªãƒªã‚¹ãƒˆã‚’ä¿æŒ
        
        for i, doc in enumerate(nlp.pipe(texts, disable=["parser", "ner"], batch_size=50)):
            words_in_text = set() # (â˜…) æŠ•ç¨¿å†…ã§ãƒ¦ãƒ‹ãƒ¼ã‚¯ã«ã™ã‚‹
            for token in doc:
                if (token.pos_ in target_pos) and (not token.is_stop) and (token.lemma_ not in stop_words) and (len(token.lemma_) > 1):
                    words_in_text.add(token.lemma_)
            doc_words_list.append(list(words_in_text)) # (â˜…)
            
            if (i + 1) % 100 == 0:
                percent = (i + 1) / total_texts
                st.session_state.progress_text = f"å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ (spaCy) å‡¦ç†ä¸­... {percent:.0%}"

        # (â˜…) å…¨å˜èªãƒªã‚¹ãƒˆã‚’ä½œæˆ (top_nç”¨)
        all_words = [word for sublist in doc_words_list for word in sublist]
        if not all_words:
            msg = "æŠ½å‡ºå¯èƒ½ãªå˜èªãŒã‚ã‚Šã¾ã›ã‚“ã€‚"
            logger.warning(f"run_cooccurrence_network: {msg}"); results["summary"] = msg; return results

        top_n_words_limit = 100
        top_n_words_set = set(pd.Series(all_words).value_counts().head(top_n_words_limit).index)

        G = nx.Graph()
        
        # (â˜…) ãƒšã‚¢ã‚’ä½œæˆ (doc_words_list ã‚’ä½¿ç”¨)
        for words_in_text_set in doc_words_list:
            # (â˜…) top_n å˜èªã‚»ãƒƒãƒˆã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            filtered_words = [word for word in words_in_text_set if word in top_n_words_set]
            
            for word1, word2 in combinations(sorted(list(filtered_words)), 2):
                if G.has_edge(word1, word2):
                    G[word1][word2]['weight'] += 1
                else:
                    G.add_edge(word1, word2, weight=1)

        if G.number_of_nodes() == 0 or G.number_of_edges() == 0:
            msg = "æœ‰åŠ¹ãªã‚¨ãƒƒã‚¸ãŒæ§‹ç¯‰ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚"
            logger.info(f"run_cooccurrence_network: {msg}"); results["summary"] = msg; return results

        edge_list = []
        for u, v, data in G.edges(data=True):
            edge_list.append({"source": u, "target": v, "weight": data['weight']})
        
        edges_df = pd.DataFrame(edge_list)
        edges_df_sorted = edges_df.sort_values(by="weight", ascending=False).head(500)
        
        results["data"] = edges_df_sorted
        st.session_state.progress_text = "å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ ã‚°ãƒ©ãƒ•æç”»ä¸­..."
        
        # (â˜…) ã‚°ãƒ©ãƒ•ç”Ÿæˆ
        results["image_base64"] = generate_graph_image(
            df=edges_df_sorted,
            plot_type='network',
            title="å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ (ä¸Šä½100ã‚¨ãƒƒã‚¸)"
        )
        results["summary"] = f"å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆ†æã‚’å®Ÿè¡Œã€‚{len(G.nodes())}ãƒãƒ¼ãƒ‰, {len(G.edges())}ã‚¨ãƒƒã‚¸ã‚’æ¤œå‡ºã€‚ã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆã—ã¾ã—ãŸã€‚"
        return results

    except Exception as e:
        logger.error(f"run_cooccurrence_network error: {e}", exc_info=True)
        results["summary"] = f"ã‚¨ãƒ©ãƒ¼: {e}"
        return results

# (â˜…) --- run_topic_category_summary ---
def run_topic_category_summary(df: pd.DataFrame, suggestion: Dict[str, Any]) -> Dict[str, Any]:
    """(Step B) è©±é¡Œã‚«ãƒ†ã‚´ãƒªåˆ¥ã«æŠ•ç¨¿æ•°ã€ã‚µãƒãƒª(AI)ã€ä¸Šä½ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’åˆ†æã™ã‚‹ (â˜…æˆ»ã‚Šå€¤å¤‰æ›´)"""
    logger.info("run_topic_category_summary å®Ÿè¡Œ...")
    results = {"data": pd.DataFrame(), "image_base64": None, "summary": ""}
    
    # (â˜…) è¦‹æœ¬PDF P8 [cite: 281, 282, 283, 284, 285, 286] ã‚„ P5 [cite: 149, 167, 156] ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å‚è€ƒã«ã‚«ãƒ†ã‚´ãƒªã‚’å®šç¾©
    target_categories = ['ã‚°ãƒ«ãƒ¡', 'è‡ªç„¶', 'æ­´å²ãƒ»æ–‡åŒ–', 'ã‚¢ãƒ¼ãƒˆ', 'ã‚¤ãƒ™ãƒ³ãƒˆ', 'å®¿æ³Šãƒ»æ¸©æ³‰']
    topic_col = 'è©±é¡Œã‚«ãƒ†ã‚´ãƒª'
    keyword_col = 'é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰' # (â˜…) StepAã§ç”Ÿæˆã•ã‚Œã‚‹ã¨ä»®å®š
    
    if topic_col not in df.columns:
        msg = f"åˆ— '{topic_col}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚StepAã§ã€Œè©±é¡Œã‚«ãƒ†ã‚´ãƒªã€ãŒç”Ÿæˆã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
        logger.warning(f"run_topic_category_summary: {msg}")
        return {"data": pd.DataFrame([{"error": msg}]), "image_base64": None, "summary": msg}
    
    if keyword_col not in df.columns:
        if 'å¸‚åŒºç”ºæ‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰' in df.columns:
            keyword_col = 'å¸‚åŒºç”ºæ‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰'
        else:
            keyword_col = None
    
    results_list = []
    
    total_cats = len(target_categories)
    if 'progress_text' not in st.session_state:
            st.session_state.progress_text = ""
            
    for i, category in enumerate(target_categories):
        st.session_state.progress_text = f"è©±é¡Œã‚«ãƒ†ã‚´ãƒªåˆ†æä¸­ ({i+1}/{total_cats}): {category}"
        
        df_filtered = df[df[topic_col].astype(str).str.contains(category, na=False)]
        post_count = len(df_filtered)
        
        if post_count == 0:
            results_list.append({
                "category": category,
                "post_count": 0,
                "summary_ai": "N/A (æŠ•ç¨¿ãªã—)",
                "top_keywords": []
            })
            continue
        
        text_samples = df_filtered['ANALYSIS_TEXT_COLUMN'].dropna().sample(n=min(10, post_count), random_state=1).tolist()
        text_samples_str = "\n".join([f"- {text[:200]}..." for text in text_samples])
        
        ai_suggestion = {
            "description": f"ã€Œ{category}ã€ã‚«ãƒ†ã‚´ãƒªã«é–¢ã™ã‚‹ä»¥ä¸‹ã®æŠ•ç¨¿ã‚µãƒ³ãƒ—ãƒ«ã‚’èª­ã¿ã€ä¸»è¦ãªè©±é¡Œã‚’1ï½2æ–‡ã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚\nã‚µãƒ³ãƒ—ãƒ«:\n{text_samples_str}"
        }
        # (â˜…) run_ai_summary_batch ã®æˆ»ã‚Šå€¤ã¯AIã®ãƒ†ã‚­ã‚¹ãƒˆ(str)
        summary_ai = run_ai_summary_batch(df_filtered, ai_suggestion)
        
        top_keywords = []
        if keyword_col and keyword_col in df_filtered.columns:
            s = df_filtered[keyword_col].astype(str).str.split(', ').explode()
            s = s[s.str.strip().isin(['', 'nan', 'None', 'N/A', 'è©²å½“ãªã—']) == False]
            s = s.str.strip()
            if not s.empty:
                top_keywords = s.value_counts().head(5).index.tolist()
        
        results_list.append({
            "category": category,
            "post_count": post_count,
            "summary_ai": summary_ai,
            "top_keywords": top_keywords
        })
        
        # (â˜…) API Rate Limit å¯¾ç­–
        time.sleep(max(TAGGING_SLEEP_TIME / 2, 1.0)) # (â˜…) StepBã§ã¯å°‘ã—å¾…æ©Ÿæ™‚é–“ã‚’çŸ­ç¸®

    st.session_state.progress_text = "è©±é¡Œã‚«ãƒ†ã‚´ãƒªåˆ†æ å®Œäº†ã€‚"
    
    results_df = pd.DataFrame(results_list)
    
    # (â˜…) ã‚°ãƒ©ãƒ•ç”Ÿæˆ (ã‚«ãƒ†ã‚´ãƒªåˆ¥æŠ•ç¨¿æ•°ã®æ£’ã‚°ãƒ©ãƒ•)
    image_base64 = generate_graph_image(
        df=results_df,
        plot_type='bar',
        x_col='category',
        y_col='post_count',
        title="è©±é¡Œã‚«ãƒ†ã‚´ãƒªåˆ¥ æŠ•ç¨¿æ•°"
    )
    
    summary = f"è©±é¡Œã‚«ãƒ†ã‚´ãƒªåˆ¥ï¼ˆ{', '.join(target_categories)}ï¼‰ã®åˆ†æã‚’å®Ÿè¡Œã€‚æŠ•ç¨¿æ•°ã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆã—ã¾ã—ãŸã€‚"
    return {"data": results_df, "image_base64": image_base64, "summary": summary}

# (â˜…) --- run_topic_engagement_top5 ---
def run_topic_engagement_top5(df: pd.DataFrame, suggestion: Dict[str, Any]) -> Dict[str, Any]:
    """(Step B) è©±é¡Œã‚«ãƒ†ã‚´ãƒªåˆ¥ã«ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆTOP5æŠ•ç¨¿ã¨æ¦‚è¦(AI)ã‚’åˆ†æã™ã‚‹ (â˜…æˆ»ã‚Šå€¤å¤‰æ›´)"""
    logger.info("run_topic_engagement_top5 å®Ÿè¡Œ...")
    results = {"data": pd.DataFrame(), "image_base64": None, "summary": ""}

    target_categories = ['ã‚°ãƒ«ãƒ¡', 'è‡ªç„¶', 'æ­´å²ãƒ»æ–‡åŒ–', 'ã‚¢ãƒ¼ãƒˆ', 'ã‚¤ãƒ™ãƒ³ãƒˆ', 'å®¿æ³Šãƒ»æ¸©æ³‰']
    topic_col = 'è©±é¡Œã‚«ãƒ†ã‚´ãƒª'
    text_col = 'ANALYSIS_TEXT_COLUMN'

    if topic_col not in df.columns:
        msg = f"åˆ— '{topic_col}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
        return {"data": pd.DataFrame([{"error": msg}]), "image_base64": None, "summary": msg}
    
    suitable_cols = suggestion.get('suitable_cols', [])
    engagement_cols = [col for col in suitable_cols if any(c in col.lower() for c in ['ã„ã„ã­', 'like', 'ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ', 'engagement'])]
    
    if not engagement_cols:
        msg = "ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆåˆ— ('ã„ã„ã­' ç­‰) ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
        return {"data": pd.DataFrame([{"error": msg}]), "image_base64": None, "summary": msg}
    
    engagement_col = engagement_cols[0]
    if engagement_col not in df.columns or not pd.api.types.is_numeric_dtype(df[engagement_col]):
        msg = f"ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆåˆ— '{engagement_col}' ãŒæ•°å€¤åˆ—ã¨ã—ã¦å­˜åœ¨ã—ã¾ã›ã‚“ã€‚"
        return {"data": pd.DataFrame([{"error": msg}]), "image_base64": None, "summary": msg}

    results_list = []
    
    total_cats = len(target_categories)
    if 'progress_text' not in st.session_state:
            st.session_state.progress_text = ""

    for i, category in enumerate(target_categories):
        st.session_state.progress_text = f"ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆTOP5åˆ†æä¸­ ({i+1}/{total_cats}): {category}"
        
        df_filtered = df[df[topic_col].astype(str).str.contains(category, na=False)]
        post_count = len(df_filtered)
        
        if post_count == 0:
            continue
            
        df_top5 = df_filtered.nlargest(5, engagement_col, keep='first')
        top5_posts_data = []
        
        if df_top5.empty:
                results_list.append({
                "category": category,
                "post_count": post_count,
                "top_posts": []
            })
                continue

        for _, row in df_top5.iterrows():
            post_text = str(row[text_col])
            engagement_value = row[engagement_col]
            
            ai_suggestion = {
                "description": f"ä»¥ä¸‹ã®æŠ•ç¨¿ãƒ†ã‚­ã‚¹ãƒˆã‚’èª­ã¿ã€å†…å®¹ã‚’1æ–‡ã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚\nãƒ†ã‚­ã‚¹ãƒˆ: {post_text[:500]}..."
            }
            summary_ai = run_ai_summary_batch(df_filtered, ai_suggestion) # (â˜…) AIå‘¼ã³å‡ºã—
            
            top5_posts_data.append({
                "engagement": int(engagement_value),
                "summary_ai": summary_ai,
                "original_text_snippet": post_text[:100] # ç¢ºèªç”¨
            })
            
            time.sleep(max(TAGGING_SLEEP_TIME / 2, 1.0)) # (â˜…) API Rate Limit å¯¾ç­–

        results_list.append({
            "category": category,
            "post_count": post_count,
            "top_posts": top5_posts_data
        })

    st.session_state.progress_text = "ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆTOP5åˆ†æ å®Œäº†ã€‚"
    results_df = pd.DataFrame(results_list)
    
    summary = f"è©±é¡Œã‚«ãƒ†ã‚´ãƒªåˆ¥ã®é«˜ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆæŠ•ç¨¿TOP5ã‚’æŠ½å‡ºã—ã¾ã—ãŸã€‚"
    # (â˜…) ã“ã®åˆ†æã¯ã‚°ãƒ©ãƒ•åŒ–ãŒé›£ã—ã„ãŸã‚ã€ãƒ‡ãƒ¼ã‚¿ã®ã¿è¿”ã™
    return {"data": results_df, "image_base64": None, "summary": summary}


# --- 8.2. (â˜…) Step B: AIåˆ†æãƒ˜ãƒ«ãƒ‘ãƒ¼ (å¤‰æ›´ãªã—) ---
# ... (run_ai_summary_batch ã¯å¤‰æ›´ãªã—) ...


# --- 8.3. (â˜…) Step B: åˆ†æå®Ÿè¡Œãƒ«ãƒ¼ã‚¿ãƒ¼ (æˆ»ã‚Šå€¤ã®å‹ã‚’ä¿®æ­£) ---

def execute_analysis(
    analysis_name: str,
    df: pd.DataFrame,
    suggestion: Dict[str, Any]
) -> Dict[str, Any]: # (â˜…) æˆ»ã‚Šå€¤ã‚’ Dict[str, Any] ã«çµ±ä¸€
    """
    (Step B) åˆ†æåã«åŸºã¥ãã€é©åˆ‡ãªPythonã¾ãŸã¯AIã®å®Ÿè¡Œé–¢æ•°ã‚’å‘¼ã³å‡ºã™ãƒ«ãƒ¼ã‚¿ãƒ¼
    (â˜…) æˆ»ã‚Šå€¤ã¯ {"data": ..., "image_base64": ..., "summary": "..."} ã®å½¢å¼
    """
    try:
        analysis_type = suggestion.get('type', 'python')
        
        # (â˜…) --- Pythonå®Ÿè¡Œ ---
        if analysis_type == 'python':
            # (â˜…) å„é–¢æ•°ã¯ {"data": ..., "image_base64": ..., "summary": "..."} ã‚’è¿”ã™
            if analysis_name == "å…¨ä½“ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹":
                return run_overall_metrics(df, suggestion)
            elif analysis_name == "å¸‚åŒºç”ºæ‘åˆ¥æŠ•ç¨¿æ•°":
                return run_simple_count(df, suggestion)
            elif analysis_name == "å˜ç´”é›†è¨ˆï¼ˆé »åº¦åˆ†æï¼‰":
                return run_simple_count(df, suggestion)
            elif analysis_name in ["ã‚¯ãƒ­ã‚¹é›†è¨ˆï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é–“ï¼‰", "ã‚¯ãƒ­ã‚¹é›†è¨ˆï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰Ã—å±æ€§ï¼‰", "è©±é¡Œã‚«ãƒ†ã‚´ãƒªåˆ¥ è¦³å…‰åœ°TOP10"]:
                return run_crosstab(df, suggestion)
            elif analysis_name == "æ™‚ç³»åˆ—ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æ":
                return run_timeseries(df, suggestion)
            elif analysis_name == "ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°ï¼ˆé »å‡ºå˜èªï¼‰":
                return run_text_mining(df, suggestion)
            elif analysis_name == "å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯":
                return run_cooccurrence_network(df, suggestion)
            elif analysis_name == "è©±é¡Œã‚«ãƒ†ã‚´ãƒªåˆ¥ æŠ•ç¨¿æ•°ã¨ã‚µãƒãƒª":
                return run_topic_category_summary(df, suggestion)
            elif analysis_name == "è©±é¡Œã‚«ãƒ†ã‚´ãƒªåˆ¥ ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆTOP5ã¨æ¦‚è¦":
                return run_topic_engagement_top5(df, suggestion)
            else:
                logger.warning(f"Pythonåˆ†æ '{analysis_name}' ã®å®Ÿè¡Œãƒ­ã‚¸ãƒƒã‚¯ãŒå®šç¾©ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚AIåˆ†æã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™ã€‚")
                suggestion['description'] = f"ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«ã‚’ä½¿ã„ã€'{analysis_name}' ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚"
                # (â˜…) AIãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                ai_result_str = run_ai_summary_batch(df, suggestion)
                return {"data": ai_result_str, "image_base64": None, "summary": ai_result_str[:100] + "..."}
        
        # (â˜…) --- AIå®Ÿè¡Œ ---
        elif analysis_type == 'ai':
            ai_result_str = run_ai_summary_batch(df, suggestion)
            # (â˜…) AIå®Ÿè¡Œï¼ˆãƒ†ã‚­ã‚¹ãƒˆã®ã¿ï¼‰ã®å ´åˆã‚‚è¾æ›¸å½¢å¼ã«çµ±ä¸€
            return {"data": ai_result_str, "image_base64": None, "summary": ai_result_str[:100] + "..."}
            
        else:
            err_msg = f"ä¸æ˜ãªåˆ†æã‚¿ã‚¤ãƒ— ('{analysis_type}') ã§ã™: {analysis_name}"
            return {"data": err_msg, "image_base64": None, "summary": err_msg}
            
    except Exception as e:
        logger.error(f"execute_analysis ('{analysis_name}') å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        err_msg = f"åˆ†æ '{analysis_name}' ã®å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"
        return {"data": err_msg, "image_base64": None, "summary": err_msg}

# --- 8.4. (â˜…) Step B: JSONå‡ºåŠ›ãƒ˜ãƒ«ãƒ‘ãƒ¼ (ã‚°ãƒ©ãƒ•ç”»åƒå¯¾å¿œ) ---

def convert_results_to_json_string(results_dict: Dict[str, Any]) -> str:
    """
    (Step B) åˆ†æçµæœã®è¾æ›¸ (â˜…ç”»åƒBase64ã‚’å«ã‚€) ã‚’
    JSONLé¢¨ã®æ–‡å­—åˆ—ã«å¤‰æ›ã™ã‚‹ã€‚
    """
    final_output_lines = []
    
    # (â˜…) 1. å…¨ä½“ã‚µãƒãƒªãƒ¼ (AIãŒçŠ¶æ³ã‚’æŠŠæ¡ã™ã‚‹ãŸã‚)
    summary_info = {
        "analysis_task": "OverallSummary",
        "timestamp": pd.Timestamp.now().isoformat(),
        "total_results_count": len(results_dict),
        "analysis_names": list(results_dict.keys()),
        # (â˜…) å„ã‚¿ã‚¹ã‚¯ã®ã‚µãƒãƒªãƒ¼ã‚’è¿½åŠ 
        "analysis_summaries": {name: data.get("summary", "No summary.") for name, data in results_dict.items()}
    }
    final_output_lines.append(json.dumps(summary_info, ensure_ascii=False))

    # (â˜…) 2. å„åˆ†æçµæœã‚’JSONLã®1è¡Œã¨ã—ã¦è¿½åŠ 
    for name, result_data in results_dict.items():
        try:
            record = {"analysis_task": name}
            
            # (â˜…) result_data ã¯ {"data": ..., "image_base64": ..., "summary": ...} å½¢å¼
            data = result_data.get("data")
            image_base64 = result_data.get("image_base64")
            summary = result_data.get("summary")

            # (â˜…) ç”»åƒãƒ‡ãƒ¼ã‚¿ã¨ã‚µãƒãƒªãƒ¼ã‚’æ ¼ç´
            record["summary"] = summary
            # (â˜…) ç”»åƒãŒå¤§ãã™ãã‚‹å ´åˆã€JSONLã«å«ã‚ãªã„ (StepCã§æ‰±ãˆãªã„ãŸã‚)
            # (â˜…) 1MBã‚’é–¾å€¤ã¨ã™ã‚‹ (Base64ã¯å…ƒãƒ‡ãƒ¼ã‚¿ã®ç´„1.33å€)
            if image_base64 and len(image_base64) < (1024 * 1024 * 1.0):
                record["image_base64"] = image_base64
                record["image_note"] = "Base64 encoded PNG image attached."
            elif image_base64:
                record["image_base64"] = None
                record["image_note"] = "Image was generated but exceeded 1MB and was not included."
            else:
                record["image_base64"] = None
                record["image_note"] = "No image generated for this task."

            
            # (â˜…) ãƒ‡ãƒ¼ã‚¿æœ¬ä½“ (data) ã®å‡¦ç†
            if isinstance(data, pd.DataFrame):
                # (â˜…) ãƒ‡ãƒ¼ã‚¿ãŒå¤§ãã™ããªã„ã‚ˆã†ã«æœ€å¤§500ä»¶ã«åˆ¶é™ (ã‚°ãƒ©ãƒ•ã‚’æ¸¡ã™ãŸã‚ãƒ‡ãƒ¼ã‚¿ã¯å‰Šæ¸›)
                if len(data) > 500:
                    record["data"] = data.head(500).to_dict(orient='records')
                    record["note"] = f"Data truncated. Showing 500 of {len(data)} records."
                else:
                    record["data"] = data.to_dict(orient='records')
            
            elif isinstance(data, pd.Series):
                record["data"] = data.to_dict()
            
            elif isinstance(data, dict): # (â˜…) ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç”¨
                record["data"] = data

            elif isinstance(data, str): # (â˜…) AIã®å›ç­”
                record["data"] = data
            
            elif data is None or (hasattr(data, 'empty') and data.empty):
                record["data"] = None
                record["note"] = "No data returned from analysis."
            
            else:
                record["data"] = str(data)

            final_output_lines.append(json.dumps(record, ensure_ascii=False, default=str)) # default=strã§numpyå‹ç­‰ã«å¯¾å¿œ
        
        except Exception as e:
            logger.error(f"JSONå¤‰æ›ã‚¨ãƒ©ãƒ¼ ({name}): {e}", exc_info=True)
            error_record = {
                "analysis_task": name,
                "data": None,
                "image_base64": None, # (â˜…)
                "summary": f"Error during JSON serialization: {e}",
                "note": f"Error during JSON serialization: {e}"
            }
            final_output_lines.append(json.dumps(error_record, ensure_ascii=False))

    # (â˜…) å„è¡Œã‚’æ”¹è¡Œã§çµåˆã—ãŸã€å˜ä¸€ã®æ–‡å­—åˆ— (JSONLå½¢å¼) ã‚’è¿”ã™
    return "\n".join(final_output_lines)


# --- 8.5. (â˜…) Step B: UIæç”»é–¢æ•° (Tipsè¡¨ç¤ºã®çµ„è¾¼ã¿) ---

def render_step_b():
    """(Step B) åˆ†ææ‰‹æ³•ã®ææ¡ˆãƒ»å®Ÿè¡Œãƒ»ãƒ‡ãƒ¼ã‚¿å‡ºåŠ›UIã‚’æç”»ã™ã‚‹"""
    st.title("ğŸ“Š Step B: åˆ†æã®å®Ÿè¡Œã¨ãƒ‡ãƒ¼ã‚¿å‡ºåŠ›")

    # (â˜…) ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆåˆæœŸåŒ– (Tipsé–¢é€£ã‚’è¿½åŠ )
    if 'df_flagged_B' not in st.session_state:
        st.session_state.df_flagged_B = pd.DataFrame()
    if 'suggestions_B' not in st.session_state:
        st.session_state.suggestions_B = []
    if 'selected_analysis_B' not in st.session_state:
        st.session_state.selected_analysis_B = []
    if 'step_b_results' not in st.session_state:
        st.session_state.step_b_results = {}
    if 'step_b_json_output' not in st.session_state:
        st.session_state.step_b_json_output = None
    if 'progress_text' not in st.session_state:
         st.session_state.progress_text = ""
    # (â˜…) Tipsç”¨ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆ (StepAã§åˆæœŸåŒ–ã•ã‚Œã¦ã„ã‚Œã°æµç”¨ã•ã‚Œã‚‹)
    if 'tips_list' not in st.session_state:
        st.session_state.tips_list = []
    if 'current_tip_index' not in st.session_state:
        st.session_state.current_tip_index = 0
    if 'last_tip_time' not in st.session_state:
        st.session_state.last_tip_time = time.time()


    # --- 1. ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ (è¦ä»¶â‘¤) ---
    st.header("Step 1: ã‚­ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ¸ˆã¿CSVã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
    st.info(f"Step A ã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã—ãŸ CSV (Curated_Data.csv) ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
    uploaded_flagged_file = st.file_uploader(
        "ãƒ•ãƒ©ã‚°ä»˜ã‘æ¸ˆã¿CSVãƒ•ã‚¡ã‚¤ãƒ«",
        type=['csv'],
        key="step_b_uploader"
    )

    if uploaded_flagged_file:
        try:
            df, err = read_file(uploaded_flagged_file)
            if err:
                st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {err}")
                return
            st.session_state.df_flagged_B = df
            st.success(f"ãƒ•ã‚¡ã‚¤ãƒ«ã€Œ{uploaded_flagged_file.name}ã€èª­è¾¼å®Œäº† ({len(df)}è¡Œ)")
            with st.expander("ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ (å…ˆé ­5è¡Œ)"):
                st.dataframe(df.head())
        except Exception as e:
            logger.error(f"Step B ãƒ•ã‚¡ã‚¤ãƒ«èª­è¾¼ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            return
    else:
        st.warning("åˆ†æã‚’ç¶šã‘ã‚‹ã«ã¯ã€Step A ã§ç”Ÿæˆã—ãŸCSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
        return # (â˜…) DFãŒãƒ­ãƒ¼ãƒ‰ã•ã‚Œã‚‹ã¾ã§ä»¥ä¸‹ã¯å®Ÿè¡Œã—ãªã„

    df_B = st.session_state.df_flagged_B

    # --- 2. åˆ†ææ‰‹æ³•ã®ææ¡ˆ (è¦ä»¶â‘¤) ---
    st.header("Step 2: åˆ†ææ‰‹æ³•ã®ææ¡ˆ")
    st.markdown(f"ï¼ˆ(â˜…) AIææ¡ˆãƒ¢ãƒ‡ãƒ«: `{MODEL_FLASH_LITE}`ï¼‰")
    
    analysis_prompt_B = st.text_area(
        "ï¼ˆä»»æ„ï¼‰AIã«è¿½åŠ ã§æŒ‡ç¤ºã—ãŸã„åˆ†æã‚¿ã‚¹ã‚¯ã‚’å…¥åŠ›:",
        placeholder="ä¾‹: åºƒå³¶å¸‚ã¨è¦³å…‰åœ°ã®ç›¸é–¢é–¢ä¿‚ã‚’æ·±æ˜ã‚Šã—ãŸã„ã€‚\nä¾‹: ãƒã‚¸ãƒ†ã‚£ãƒ–ãªæ„è¦‹ã¨ãƒã‚¬ãƒ†ã‚£ãƒ–ãªæ„è¦‹ã®å…·ä½“ä¾‹ã‚’3ã¤ãšã¤æŠ½å‡ºã—ã¦ã€‚",
        key="step_b_prompt"
    )

    if st.button("ğŸ’¡ åˆ†ææ‰‹æ³•ã‚’ææ¡ˆã•ã›ã‚‹ (Step 2)", key="suggest_button_B", type="primary"):
        # (â˜…) --- Tipsè¡¨ç¤ºã®åˆæœŸåŒ– (StepBã‹ã‚‰é–‹å§‹ã™ã‚‹ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãŸã‚) ---
        if not st.session_state.tips_list:
            with st.spinner("åˆ†æTIPSã‚’AIã§ç”Ÿæˆä¸­..."):
                st.session_state.tips_list = get_analysis_tips_list_from_ai()
                if st.session_state.tips_list:
                    st.session_state.current_tip_index = random.randint(0, len(st.session_state.tips_list) - 1)
                    st.session_state.last_tip_time = time.time()
    
        with st.spinner(f"ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã¨æŒ‡ç¤ºå†…å®¹ã‚’åˆ†æã—ã€æ‰‹æ³•ã‚’ææ¡ˆä¸­ ({MODEL_FLASH_LITE})..."):
            st.session_state.step_b_results = {}
            st.session_state.step_b_json_output = None
            base_suggestions = suggest_analysis_techniques_py(df_B)
            ai_suggestions = []
            if analysis_prompt_B.strip():
                ai_suggestions = suggest_analysis_techniques_ai(
                    analysis_prompt_B, df_B, base_suggestions
                )
            base_names = {s['name'] for s in base_suggestions}
            filtered_ai_suggestions = [s for s in ai_suggestions if s['name'] not in base_names]
            all_suggestions = sorted(base_suggestions + filtered_ai_suggestions, key=lambda x: x['priority'])
            st.session_state.suggestions_B = all_suggestions
            st.success(f"åˆ†ææ‰‹æ³•ã®ææ¡ˆãŒå®Œäº†ã—ã¾ã—ãŸ ({len(all_suggestions)}ä»¶)ã€‚")

    # --- 3. åˆ†ææ‰‹æ³•ã®é¸æŠ ---
    if st.session_state.suggestions_B:
        st.header("Step 3: å®Ÿè¡Œã™ã‚‹åˆ†æã®é¸æŠ")
        
        default_selection = [s['name'] for s in st.session_state.suggestions_B[:min(len(st.session_state.suggestions_B), 5)]]
        
        st.session_state.selected_analysis_B = st.multiselect(
            "å®Ÿè¡Œã—ãŸã„åˆ†ææ‰‹æ³•ã‚’é¸æŠï¼ˆè¤‡æ•°å¯ï¼‰:",
            options=[s['name'] for s in st.session_state.suggestions_B],
            default=default_selection,
            key="multiselect_B"
        )
        
        with st.expander("é¸æŠã—ãŸæ‰‹æ³•ã®è©³ç´°ã‚’è¡¨ç¤º"):
            for s in st.session_state.suggestions_B:
                if s['name'] in st.session_state.selected_analysis_B:
                    st.markdown(f"**{s['name']}** (`type: {s.get('type', 'N/A')}`)")
                    st.caption(f"èª¬æ˜: {s.get('description', 'N/A')}")
                    st.caption(f"ç†ç”±: {s.get('reason', 'N/A')}")
                    st.markdown("---")

    # --- 4. åˆ†æã®å®Ÿè¡Œ (è¦ä»¶â‘¥) ---
    if st.session_state.selected_analysis_B:
        st.header("Step 4: åˆ†æã®å®Ÿè¡Œã¨ãƒ‡ãƒ¼ã‚¿ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ")
        
        if st.button("åˆ†æã‚’å®Ÿè¡Œ (Step 4)", key="execute_button_B", type="primary", use_container_width=True):
            selected_names = st.session_state.selected_analysis_B
            all_suggestions_map = {s['name']: s for s in st.session_state.suggestions_B}
            
            st.info(f"è¨ˆ {len(selected_names)} ä»¶ã®åˆ†æã‚’å®Ÿè¡Œã—ã¾ã™...")
            progress_bar = st.progress(0.0, text="åˆ†æå¾…æ©Ÿä¸­...")
            st.session_state.progress_text = ""
            
            # (â˜…) --- Tipsã¨é€²æ—ãƒ†ã‚­ã‚¹ãƒˆã®ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ ---
            tip_placeholder = st.empty()
            progress_text_placeholder = st.empty() 
            
            # (â˜…) TipsãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
            if not st.session_state.tips_list or len(st.session_state.tips_list) <= 1:
                st.session_state.tips_list = get_analysis_tips_list_from_ai()
                st.session_state.current_tip_index = random.randint(0, max(0, len(st.session_state.tips_list) - 1))
                st.session_state.last_tip_time = time.time()
            
            results_dict = {}
            
            for i, name in enumerate(selected_names):
                progress_percent = (i + 1) / len(selected_names)
                progress_bar.progress(progress_percent, text=f"åˆ†æä¸­ ({i+1}/{len(selected_names)}): {name}")
                
                st.session_state.progress_text = f"{name} ã‚’å®Ÿè¡Œä¸­..."
                
                # (â˜…) --- Tipsã¨é€²æ—ã®UIæ›´æ–° ---
                # (StepAã® update_progress_ui ã‚’æ¨¡å€£)
                now = time.time()
                # (â˜…) 60ç§’ -> 10ç§’ã«çŸ­ç¸® (TipsãŒé »ç¹ã«å¤‰ã‚ã‚‹ã‚ˆã†ã«)
                if (now - st.session_state.last_tip_time > 10) or (len(st.session_state.tips_list) == 1):
                    if len(st.session_state.tips_list) > 1:
                        st.session_state.current_tip_index = (st.session_state.current_tip_index + 1) % len(st.session_state.tips_list)
                    st.session_state.last_tip_time = now
                
                if st.session_state.tips_list: # (â˜…) ãƒªã‚¹ãƒˆãŒç©ºã§ãªã„ã‹ãƒã‚§ãƒƒã‚¯
                    current_tip = st.session_state.tips_list[st.session_state.current_tip_index]
                    tip_placeholder.info(f"ğŸ’¡ ãƒ‡ãƒ¼ã‚¿åˆ†æTIPS: {current_tip}")
                
                with progress_text_placeholder.container():
                        st.info(st.session_state.progress_text) # (â˜…) spaCyç­‰ã®é€²æ—ãŒã“ã“ã«è¡¨ç¤ºã•ã‚Œã‚‹
                # (â˜…) --- ã“ã“ã¾ã§ãŒå¤‰æ›´ç‚¹ ---
                
                # (â˜…) å®Ÿè¡Œ (æˆ»ã‚Šå€¤ã¯ {"data": ..., "image_base64": ..., "summary": ...})
                result_data = execute_analysis(name, df_B, all_suggestions_map[name])
                
                results_dict[name] = result_data
            
            # (â˜…) å®Œäº†å¾Œã€é€²æ—ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¯ãƒªã‚¢
            tip_placeholder.empty()
            progress_text_placeholder.empty()
            
            progress_bar.progress(1.0, text="åˆ†æå®Œäº†ï¼ æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ (JSON) ã‚’ç”Ÿæˆä¸­...")
            
            # (â˜…) è¦ä»¶â‘¦: æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ (JSON) ã®ç”Ÿæˆ
            try:
                # (â˜…) ã‚°ãƒ©ãƒ•ç”»åƒ(Base64)ã‚’å«ã‚€JSONæ–‡å­—åˆ—ã‚’ç”Ÿæˆ
                json_output_string = convert_results_to_json_string(results_dict)
                st.session_state.step_b_results = results_dict # (â˜…) ç”Ÿã®çµæœã‚‚ä¿å­˜
                st.session_state.step_b_json_output = json_output_string # (â˜…) JSONæ–‡å­—åˆ—ã‚’ä¿å­˜
                st.success("å…¨ã¦ã®åˆ†æãŒå®Œäº†ã—ã€Step Cç”¨ã®JSONãƒ‡ãƒ¼ã‚¿ï¼ˆã‚°ãƒ©ãƒ•ç”»åƒå«ã‚€ï¼‰ãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚")
            except Exception as e:
                logger.error(f"Step B JSONå‡ºåŠ›å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
                st.error(f"åˆ†æçµæœã®JSONå¤‰æ›ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

    # --- 5. ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ (è¦ä»¶â‘¦) ---
    if st.session_state.step_b_json_output:
        st.header("Step 5: åˆ†æãƒ‡ãƒ¼ã‚¿ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ")
        st.info(f"ä»¥ä¸‹ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ã¯ã€Step 4 ã§å®Ÿè¡Œã•ã‚ŒãŸ {len(st.session_state.step_b_results)} ä»¶ã®åˆ†æçµæœï¼ˆã‚°ãƒ©ãƒ•ç”»åƒBase64æ–‡å­—åˆ—ã‚’å«ã‚€ï¼‰ãŒã™ã¹ã¦å«ã¾ã‚Œã¦ã„ã¾ã™ã€‚")
        
        st.download_button(
            label="åˆ†æãƒ‡ãƒ¼ã‚¿ (analysis_data.json) ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            data=st.session_state.step_b_json_output,
            file_name="analysis_data.json", # (â˜…) æ‹¡å¼µå­ã‚’ .json ã«çµ±ä¸€
            mime="application/json", # (â˜…)
            type="primary",
            use_container_width=True
        )
        
        st.markdown("---")
        st.subheader("å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿ (JSONL) ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")
        
        # (â˜…) ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ç”¨ã«ã‚µãƒãƒªãƒ¼ã ã‘è¡¨ç¤º (Base64ã¯é‡ã™ãã‚‹ãŸã‚)
        preview_summaries = []
        try:
            for line in st.session_state.step_b_json_output.splitlines():
                line_data = json.loads(line)
                task_name = line_data.get("analysis_task")
                summary = line_data.get("summary", line_data.get("analysis_summaries", "No summary."))
                img_note = line_data.get("image_note", "No image.")
                
                if task_name == "OverallSummary":
                    preview_summaries.append(f"--- Overall Summary ---")
                    if isinstance(summary, dict):
                        for k, v in summary.items():
                             preview_summaries.append(f"  - {k}: {str(v)[:100]}...")
                    continue
                
                preview_summaries.append(f"[{task_name}] (Image: {img_note})")
                
        except Exception as e:
            preview_summaries = ["JSONãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®ç”Ÿæˆã«å¤±æ•—", str(e)]

        st.text_area(
            "JSONL (ã‚µãƒãƒªãƒ¼ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼):",
            value="\n".join(preview_summaries),
            height=300,
            key="json_preview_B_summary",
            disabled=True
        )
        st.success("ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€Step C (AIãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ) ã«é€²ã‚“ã§ãã ã•ã„ã€‚")

# --- 9. (â˜…) Step C: AIãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ (Proãƒ¢ãƒ‡ãƒ«) ---
# (è¦ä»¶: Step Cã¯ gemini-2.5-pro ã‚’ä½¿ç”¨)

def generate_step_c_prompt(jsonl_data_string: str) -> str:
    """
    (Step C) ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸ Step B ã® JSONL ãƒ‡ãƒ¼ã‚¿ã‚’ã€Œä¸‹èª­ã¿ã€ã—ã€
    gemini-2.5-pro ã¸ã®é«˜å“è³ªãªæŒ‡ç¤ºãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è‡ªå‹•ç”Ÿæˆã™ã‚‹ã€‚
    (â˜…) ã“ã®ã€Œä¸‹èª­ã¿ã€è‡ªä½“ã¯é«˜é€Ÿãª flash-lite ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹
    (â˜…) ã”è¦æœ›ã«åŸºã¥ãã€å“è³ªå‘ä¸Šã®ãŸã‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆãƒ­ã‚¸ãƒƒã‚¯ã‚’å¤§å¹…ã«å¼·åŒ–
    """
    logger.info("Step C ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè‡ªå‹•ç”Ÿæˆ (Flash Lite) å®Ÿè¡Œ...")
    
    llm = get_llm(model_name=MODEL_FLASH_LITE, temperature=0.1)
    if llm is None:
        logger.error("generate_step_c_prompt: LLM (Flash Lite) ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")
        return "# AIãƒ¢ãƒ‡ãƒ«(Flash Lite)ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"

    # (â˜…) JSONLã‹ã‚‰åˆ†æã‚¿ã‚¹ã‚¯åï¼ˆ"analysis_task"ï¼‰ã‚’æŠ½å‡º
    task_names = []
    summary_data = {}
    try:
        for line in jsonl_data_string.splitlines():
            try:
                task_data = json.loads(line)
                task_name = task_data.get("analysis_task")
                if task_name == "OverallSummary":
                    summary_data = task_data.get("data", {})
                    # analysis_summaries ã‹ã‚‰ã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆã‚’å–å¾— (ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯)
                    if not task_names and "analysis_summaries" in task_data:
                         task_names = list(task_data.get("analysis_summaries", {}).keys())
                elif task_name:
                    task_names.append(task_name)
            except json.JSONDecodeError:
                continue
        
        task_names_str = ", ".join(list(set(task_names))) # é‡è¤‡å‰Šé™¤
        if not task_names_str:
            task_names_str = "ï¼ˆJSONLå†…ã®å„åˆ†æã‚¿ã‚¹ã‚¯ï¼‰"
            
        summary_context = json.dumps(summary_data, ensure_ascii=False, indent=2)
        if len(summary_context) > 2000:
             summary_context = summary_context[:2000] + "\n... (ã‚µãƒãƒªçœç•¥)"
        
    except Exception as e:
        logger.error(f"Step B JSONLã®ãƒ‘ãƒ¼ã‚¹ (ä¸‹èª­ã¿) ã«å¤±æ•—: {e}")
        task_names_str = "ï¼ˆJSONLå†…ã®å„åˆ†æã‚¿ã‚¹ã‚¯ï¼‰"
        summary_context = "{}"

    # (â˜…) --- gemini-2.5-pro ã¸ã®ã€Œè¶…ã€è©³ç´°ãªæŒ‡ç¤ºãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ ---
    # (â˜…) ã”è¦æœ›ã«åŸºã¥ãã€1ä¸‡æ–‡å­—ã‚’è¶…ãˆã‚‹è©³ç´°ãªãƒ¬ãƒãƒ¼ãƒˆã‚’å‡ºåŠ›ã•ã›ã‚‹ãŸã‚ã€
    # (â˜…) å‡ºåŠ›JSONå½¢å¼ã€å½¹å‰²ã€å„ã‚¿ã‚¹ã‚¯ã®å‡¦ç†æ–¹æ³•ã‚’å³å¯†ã«å®šç¾©ã—ã¾ã™ã€‚
    
    prompt_template = """
ã‚ãªãŸã¯ã€é«˜åãªãƒ‡ãƒ¼ã‚¿ã‚¢ãƒŠãƒªã‚¹ãƒˆå…¼çµŒå–¶ã‚³ãƒ³ã‚µãƒ«ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆä¾‹ï¼šè¦³å…‰å”ä¼šã€äº‹æ¥­ä¼šç¤¾ï¼‰ã«æå‡ºã™ã‚‹ãŸã‚ã®ã€é«˜å“è³ªãªã€Œãƒ‡ãƒ¼ã‚¿åˆ†æãƒ¬ãƒãƒ¼ãƒˆã€ã‚’ä½œæˆã™ã‚‹ä»»å‹™ã‚’è² ã£ã¦ã„ã¾ã™ã€‚

ã“ã‚Œã‹ã‚‰ã€éƒ¨ä¸‹ãŒPythonã¨AIï¼ˆFlash Liteï¼‰ã§ä¸€æ¬¡åˆ†æã—ãŸçµæœï¼ˆã‚°ãƒ©ãƒ•ç”»åƒBase64ã€ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«ã€ã‚µãƒãƒªã‚’å«ã‚€JSONLï¼‰ãŒæä¾›ã•ã‚Œã¾ã™ã€‚
ã‚ãªãŸã®ä»•äº‹ã¯ã€ã“ã®ä¸€æ¬¡åˆ†æçµæœã‚’ã€å°‚é–€å®¶ã®è¦–ç‚¹ã€‘ã§è§£é‡ˆã—ç›´ã—ã€ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®æ„æ€æ±ºå®šã«è³‡ã™ã‚‹ã€Œã‚¤ãƒ³ã‚µã‚¤ãƒˆã€ã¨ã€Œæˆ¦ç•¥çš„æè¨€ã€ã‚’å«ã‚€ã€è©³ç´°ã‹ã¤æ§‹é€ åŒ–ã•ã‚ŒãŸãƒ¬ãƒãƒ¼ãƒˆï¼ˆJSONå½¢å¼ï¼‰ã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ã§ã™ã€‚

# 0. å…¨ä½“ã‚µãƒãƒªãƒ¼ï¼ˆå‚è€ƒï¼‰
åˆ†æå¯¾è±¡ã¨ãªã£ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å…¨ä½“åƒã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ã€‚
{summary_context}

# 1. å®Ÿè¡Œã‚¿ã‚¹ã‚¯
æä¾›ã•ã‚Œã‚‹ã€Œåˆ†æãƒ‡ãƒ¼ã‚¿ï¼ˆJSONLï¼‰ã€ã®å„è¡Œï¼ˆå„åˆ†æã‚¿ã‚¹ã‚¯ï¼‰ã‚’ã€æ¼ã‚Œãªãã€‘å‡¦ç†ã—ã€ä»¥ä¸‹ã®ã€Œå‡ºåŠ›JSONå½¢å¼ã€ã«å¾“ã£ã¦ã€ã‚¹ãƒ©ã‚¤ãƒ‰æ§‹æˆæ¡ˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

# 2. å‡ºåŠ›JSONå½¢å¼ï¼ˆå³å®ˆï¼‰
å‡ºåŠ›ã¯ã€ä»¥ä¸‹ã®ã‚­ãƒ¼ã‚’æŒã¤è¾æ›¸ã®ã€ãƒªã‚¹ãƒˆï¼ˆé…åˆ—ï¼‰ã€‘å½¢å¼ `[ {{...}}, {{...}} ]` ã¨ã—ã¾ã™ã€‚
JSONä»¥å¤–ã®ãƒ†ã‚­ã‚¹ãƒˆï¼ˆä¾‹ï¼šã€Œæ‰¿çŸ¥ã—ã¾ã—ãŸã€ï¼‰ã¯ã€çµ¶å¯¾ã«ã€‘å«ã‚ãªã„ã§ãã ã•ã„ã€‚

[
  {{
    "slide_title": "ï¼ˆã‚¹ãƒ©ã‚¤ãƒ‰ã®ã‚¿ã‚¤ãƒˆãƒ«ï¼‰",
    "slide_layout": "ï¼ˆ"title_and_content" ã¾ãŸã¯ "text_and_image" ã¾ãŸã¯ "title_only"ï¼‰",
    "slide_content": [
      "ï¼ˆã“ã®ã‚¹ãƒ©ã‚¤ãƒ‰ã§ä¼ãˆã‚‹ã¹ãã€ã‚¤ãƒ³ã‚µã‚¤ãƒˆã€‘ã‚„ã€è€ƒå¯Ÿã€‘ã‚’ç®‡æ¡æ›¸ãã§è©³ç´°ã«è¨˜è¿°ï¼‰",
      "ï¼ˆãƒ‡ãƒ¼ã‚¿ã‚’å˜ç´”ã«ç¾…åˆ—ã™ã‚‹ã®ã§ã¯ãªãã€å°‚é–€å®¶ã¨ã—ã¦ã®è§£é‡ˆã‚’åŠ ãˆã‚‹ã“ã¨ï¼‰",
      "ï¼ˆå¿…è¦ã§ã‚ã‚Œã°1ã‚¹ãƒ©ã‚¤ãƒ‰ã‚ãŸã‚Š1000æ–‡å­—ä»¥ä¸Šã®è©³ç´°ãªè¨˜è¿°ã‚’è¡Œã†ã“ã¨ï¼‰"
    ],
    "image_base64": "ï¼ˆ"analysis_task" ã« "image_base64" ãŒå­˜åœ¨ã™ã‚Œã°ã€ãã®Base64æ–‡å­—åˆ—ã‚’ã“ã“ã«ã‚³ãƒ”ãƒ¼ã™ã‚‹ã€‚å­˜åœ¨ã—ãªã‘ã‚Œã° null ã¨ã™ã‚‹ï¼‰"
  }}
]

# 3. å¿…é ˆã‚¹ãƒ©ã‚¤ãƒ‰æ§‹æˆï¼ˆã“ã®é †ç•ªã§æ§‹æˆã™ã‚‹ã“ã¨ï¼‰

### A. å°å…¥ã‚¹ãƒ©ã‚¤ãƒ‰ (3ã€œ4æš)
1.  **è¡¨ç´™ (layout: "title_only")**: ã€ŒSNSãƒ‡ãƒ¼ã‚¿åˆ†æãƒ¬ãƒãƒ¼ãƒˆï¼ˆä»®ï¼‰ã€ã®ã‚ˆã†ãªã‚¿ã‚¤ãƒˆãƒ«ã€‚
2.  **ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ãƒ»ã‚µãƒãƒªãƒ¼ (layout: "title_and_content")**:
    * `analysis_task: "OverallSummary"` ã®ãƒ‡ãƒ¼ã‚¿ï¼ˆç‰¹ã« `total_posts` ã‚„ `total_engagement`ï¼‰ã‚’å‚ç…§ã—ã€åˆ†æã®æœ€ã‚‚é‡è¦ãªç™ºè¦‹ï¼ˆKGI/KPIï¼‰ã‚’3ã€œ5ç‚¹ã®ç®‡æ¡æ›¸ãã§è¦ç´„ã—ã¾ã™ã€‚
    * ã€æ¥µã‚ã¦é‡è¦ã€‘ã“ã“ã§ã®ã‚¤ãƒ³ã‚µã‚¤ãƒˆã¯ã€å˜ãªã‚‹æ•°å­—ã®å ±å‘Šã§ã¯ãªãã€ã€Œä½•ã‚’æ„å‘³ã™ã‚‹ã®ã‹ã€ã‚’è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚
3.  **åˆ†ææ¦‚è¦ (layout: "title_and_content")**:
    * åˆ†æã®ç›®çš„ï¼ˆä¾‹ï¼šã€ŒSNSæŠ•ç¨¿ã‹ã‚‰è¦³å…‰å®¢ã®å‹•å‘ã¨é–¢å¿ƒäº‹ã‚’ç‰¹å®šã™ã‚‹ã€ï¼‰ã¨ã€åˆ†æå¯¾è±¡ãƒ‡ãƒ¼ã‚¿ï¼ˆä¾‹ï¼šã€ŒInstagramæŠ•ç¨¿ XXXä»¶ã€ï¼‰ã‚’è¨˜è¿°ã—ã¾ã™ã€‚
4.  **ã‚¢ã‚¸ã‚§ãƒ³ãƒ€ (layout: "title_and_content")**:
    * ã“ã®å¾Œã®ãƒ¬ãƒãƒ¼ãƒˆã®ç›®æ¬¡ï¼ˆä¾‹ï¼šã€Œ1. å…¨ä½“å‚¾å‘ã€ã€Œ2. ä¸»è¦ã‚«ãƒ†ã‚´ãƒªåˆ†æã€ã€Œ3. æˆ¦ç•¥çš„æè¨€ã€ï¼‰ã‚’è¨˜è¿°ã—ã¾ã™ã€‚

### B. è©³ç´°åˆ†æã‚¹ãƒ©ã‚¤ãƒ‰ (JSONLã®å„ã‚¿ã‚¹ã‚¯ã”ã¨ã«1æšä»¥ä¸Š)
`analysis_task` ãŒ "OverallSummary" ä»¥å¤–ã®å„ã‚¿ã‚¹ã‚¯ï¼ˆä¾‹ï¼š {task_names}ï¼‰ã«ã¤ã„ã¦ã€ä»¥ä¸‹ã®æŒ‡ç¤ºã«å¾“ã£ã¦ã‚¹ãƒ©ã‚¤ãƒ‰ã‚’ç”Ÿæˆã—ã¾ã™ã€‚

5.  **ã‚¿ã‚¹ã‚¯å: `run_simple_count` / `run_text_mining` / `run_topic_category_summary` (ã‚°ãƒ©ãƒ•ãŒã‚ã‚‹ã‚¿ã‚¹ã‚¯)**
    * `slide_layout`: "text_and_image"
    * `slide_title`: JSONLã® `analysis_task` åï¼ˆä¾‹ï¼šã€Œé »å‡ºå˜èªåˆ†æã€ï¼‰ã‚’ã€åˆ†ã‹ã‚Šã‚„ã™ã„æ—¥æœ¬èªã‚¿ã‚¤ãƒˆãƒ«ï¼ˆä¾‹ï¼šã€Œä¸»è¦ãªé »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®åˆ†æã€ï¼‰ã«ç¿»è¨³ãƒ»ä¿®æ­£ã—ã¦è¨˜è¼‰ã—ã¾ã™ã€‚
    * `image_base64`: JSONLã® `image_base64` ã‚’ã€å¿…ãšã‚³ãƒ”ãƒ¼ã€‘ã—ã¾ã™ã€‚
    * `slide_content`:
        * ã€æœ€é‡è¦ã€‘ã‚°ãƒ©ãƒ•ï¼ˆ`image_base64`ï¼‰ã¨ãƒ‡ãƒ¼ã‚¿ï¼ˆ`data`ï¼‰ã‚’è©³ç´°ã«åˆ†æã—ã€ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒç†è§£ã™ã¹ãã€ã‚¤ãƒ³ã‚µã‚¤ãƒˆï¼ˆç™ºè¦‹ï¼‰ã€‘ã‚’3ã€œ5ç‚¹ä»¥ä¸Šã®è©³ç´°ãªç®‡æ¡æ›¸ãã§è¨˜è¿°ã—ã¾ã™ã€‚
        * ï¼ˆæ‚ªã„ä¾‹ï¼šã€Œã‚°ãƒ©ãƒ•ã®é€šã‚Šã€AãŒ1ä½ã§ã—ãŸã€‚ã€ï¼‰
        * ï¼ˆè‰¯ã„ä¾‹ï¼šã€Œåˆ†æã®çµæœã€AãŒåœ§å€’çš„å¤šæ•°ã‚’å ã‚ã¦ãŠã‚Šã€ã“ã‚Œã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®é–¢å¿ƒãŒAã«é›†ä¸­ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¤ºå”†ã—ã¦ã„ã¾ã™ã€‚ç‰¹ã«2ä½ã®Bã¨ã®å·®ã¯...ã€ï¼‰
        * `data`ï¼ˆJSONé…åˆ—ï¼‰ã‹ã‚‰ã€ä¸Šä½3ä½ã¾ã§ã®å…·ä½“çš„ãªæ•°å€¤ã‚„é …ç›®åã‚’å¼•ç”¨ã—ã€è€ƒå¯Ÿã«åšã¿ã‚’æŒãŸã›ã¦ãã ã•ã„ã€‚

6.  **ã‚¿ã‚¹ã‚¯å: `run_cooccurrence_network` (å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯)**
    * `slide_layout`: "text_and_image"
    * `slide_title`: ã€Œå…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆ†æï¼šè©±é¡Œã®é–¢é€£æ€§ã€
    * `image_base64`: JSONLã® `image_base64` ã‚’ã€å¿…ãšã‚³ãƒ”ãƒ¼ã€‘ã—ã¾ã™ã€‚
    * `slide_content`:
        * ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³ï¼ˆ`image_base64`ï¼‰ã‚’è§£é‡ˆã—ã€ã©ã®å˜èªãŒä¸­å¿ƒã«ã‚ã‚‹ã‹ï¼ˆä¸­å¿ƒæ€§ï¼‰ã€ã©ã®å˜èªåŒå£«ãŒå¼·ãçµã³ã¤ã„ã¦ã„ã‚‹ã‹ï¼ˆã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ï¼‰ã‚’æŒ‡æ‘˜ã—ã¾ã™ã€‚
        * ï¼ˆä¾‹ï¼šã€Œã€Aã€ã¨ã€Bã€ã€ã¾ãŸã€Cã€ã¨ã€Dã€ã®ã‚°ãƒ«ãƒ¼ãƒ—ãŒå½¢æˆã•ã‚Œã¦ãŠã‚Šã€ã“ã‚Œã¯...ã¨ã„ã†2ã¤ã®ç•°ãªã‚‹æ–‡è„ˆã§èªã‚‰ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¤ºã—ã¾ã™ã€‚ã€ï¼‰
        * `data`ï¼ˆã‚¨ãƒƒã‚¸ãƒªã‚¹ãƒˆï¼‰ã‹ã‚‰ã€`weight`ï¼ˆé‡ã¿ï¼‰ãŒç‰¹ã«é«˜ã„çµ„ã¿åˆã‚ã›ã‚’å…·ä½“çš„ã«å¼•ç”¨ã—ã¦ãã ã•ã„ã€‚

7.  **ã‚¿ã‚¹ã‚¯å: `run_crosstab` / `run_topic_engagement_top5` (ã‚°ãƒ©ãƒ•ãŒãªã„ã‚¿ã‚¹ã‚¯)**
    * `slide_layout`: "title_and_content"
    * `slide_title`: ã‚¿ã‚¹ã‚¯åã‚’åˆ†ã‹ã‚Šã‚„ã™ã„æ—¥æœ¬èªã‚¿ã‚¤ãƒˆãƒ«ï¼ˆä¾‹ï¼šã€Œã‚«ãƒ†ã‚´ãƒªåˆ¥ ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆTOP5ã€ï¼‰ã«ä¿®æ­£ã—ã¦è¨˜è¼‰ã—ã¾ã™ã€‚
    * `image_base64`: null
    * `slide_content`:
        * `data`ï¼ˆJSONé…åˆ—ï¼‰ã®åˆ†æçµæœã‹ã‚‰ã€ç‰¹ç­†ã™ã¹ããƒ‘ã‚¿ãƒ¼ãƒ³ã€ç›¸é–¢ã€ã¾ãŸã¯å…·ä½“ä¾‹ï¼ˆä¾‹ï¼šã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆãŒæœ€ã‚‚é«˜ã‹ã£ãŸæŠ•ç¨¿ã®AIè¦ç´„ï¼‰ã‚’å¼•ç”¨ã—ã€è©³ç´°ãªã‚¤ãƒ³ã‚µã‚¤ãƒˆã‚’è¨˜è¿°ã—ã¾ã™ã€‚

8.  **ã‚¿ã‚¹ã‚¯å: `run_ai_summary_batch` (AIã«ã‚ˆã‚‹è‡ªç”±åˆ†æ)**
    * `slide_layout`: "title_and_content"
    * `slide_title`: AIãŒå®Ÿè¡Œã—ãŸã‚¿ã‚¹ã‚¯åï¼ˆä¾‹ï¼šã€Œãƒã‚¸ãƒ†ã‚£ãƒ–ãªæ„è¦‹ã®å…·ä½“ä¾‹ã€ï¼‰
    * `image_base64`: null
    * `slide_content`:
        * AIã®å›ç­”ï¼ˆ`data`ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®æ–‡å­—åˆ—ï¼‰ã‚’ã€ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå‘ã‘ã«åˆ†ã‹ã‚Šã‚„ã™ãç®‡æ¡æ›¸ãã§å†æ§‹æˆã—ã€æç¤ºã—ã¾ã™ã€‚

### C. çµè«–ã‚¹ãƒ©ã‚¤ãƒ‰ (2æš)
9.  **çµè«– (layout: "title_and_content")**:
    * å…¨ã¦ã®è©³ç´°åˆ†æï¼ˆã‚¹ãƒ©ã‚¤ãƒ‰Bç¾¤ï¼‰ã‹ã‚‰å°ãå‡ºã•ã‚Œã‚‹ã€å…¨ä½“çš„ãªçµè«–ã€‘ã‚’ã€å¼·å›ºãªæ ¹æ‹ ï¼ˆåˆ†æãƒ‡ãƒ¼ã‚¿ã¸ã®è¨€åŠï¼‰ã¨ã¨ã‚‚ã«è¦ç´„ã—ã¾ã™ã€‚
    * ï¼ˆä¾‹ï¼šã€ŒSNSåˆ†æã®çµæœã€å¼·ã¿ã¯ã€Xã€ã§ã‚ã‚Šã€èª²é¡Œã¯ã€Yã€ã§ã‚ã‚‹ã“ã¨ãŒæ˜ã‚‰ã‹ã«ãªã£ãŸã€‚ã€ï¼‰

10. **æˆ¦ç•¥çš„æè¨€ (layout: "title_and_content")**:
    * ã€æœ€é‡è¦ã€‘ã‚ãªãŸãŒçµŒå–¶ã‚³ãƒ³ã‚µãƒ«ã‚¿ãƒ³ãƒˆã¨ã—ã¦ã€ã“ã®åˆ†æçµæœã«åŸºã¥ãã€ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒæ¬¡ã«å–ã‚‹ã¹ãã€å…·ä½“çš„ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆæ–½ç­–ï¼‰ã€‘ã‚’3ã€œ5ç‚¹ã€ææ¡ˆã—ã¦ãã ã•ã„ã€‚
    * ï¼ˆä¾‹ï¼šã€Œ1. å¼·ã¿ã§ã‚ã‚‹ã€Xã€ã‚’ã•ã‚‰ã«ä¼¸ã°ã™ãŸã‚ã€...ãªã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ã‚’æ¨å¥¨ã™ã‚‹ã€‚ã€ï¼‰
    * ï¼ˆä¾‹ï¼šã€Œ2. èª²é¡Œã§ã‚ã‚‹ã€Yã€ã‚’å…‹æœã™ã‚‹ãŸã‚ã€...ãªã‚¿ãƒ¼ã‚²ãƒƒãƒˆå±¤ã¸ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’å¼·åŒ–ã™ã‚‹ã€‚ã€ï¼‰

# 4. æœ€çµ‚ç¢ºèª
* **æƒ…å ±é‡**: ãƒ¬ãƒãƒ¼ãƒˆå…¨ä½“ã§1ä¸‡æ–‡å­—ã‚’è¶…ãˆã‚‹ã‚ˆã†ãªã€è©³ç´°ã§å……å®Ÿã—ãŸå†…å®¹ã‚’æœŸå¾…ã—ã¾ã™ã€‚`slide_content` ã®å„é …ç›®ã¯ã€å˜èªã§ã¯ãªãã€å®Œå…¨ãªã€Œæ–‡ç« ã€ã¾ãŸã¯ã€Œè©³ç´°ãªç®‡æ¡æ›¸ãã€ã«ã—ã¦ãã ã•ã„ã€‚
* **å³æ ¼ãªå½¢å¼**: å‡ºåŠ›ã¯ `[` ã§å§‹ã¾ã‚Š `]` ã§çµ‚ã‚ã‚‹ã€JSONé…åˆ—å½¢å¼ã®ã¿ã¨ã—ã¾ã™ã€‚

ã“ã®æŒ‡ç¤ºã«å¾“ã„ã€æœ€é«˜ã®åˆ†æãƒ¬ãƒãƒ¼ãƒˆï¼ˆJSONï¼‰ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
"""

    prompt = PromptTemplate.from_template(prompt_template)
    
    try:
        # (â˜…) Flash Lite ã« Pro ã¸ã®æŒ‡ç¤ºæ›¸ã‚’ç”Ÿæˆã•ã›ã‚‹
        generated_prompt = prompt.invoke({
            "summary_context": summary_context,
            "task_names": task_names_str
        })
        
        logger.info("Step C ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè‡ªå‹•ç”Ÿæˆ å®Œäº†ã€‚")
        return generated_prompt

    except Exception as e:
        logger.error(f"generate_step_c_prompt error: {e}", exc_info=True)
        # (â˜…) --- ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®æœ€å°é™ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ ---
        return f"# (â˜…) ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè‡ªå‹•ç”Ÿæˆå¤±æ•—: {e}\n\n# æŒ‡ç¤º:\nã‚ãªãŸã¯å„ªç§€ãªçµŒå–¶ã‚³ãƒ³ã‚µãƒ«ã‚¿ãƒ³ãƒˆã§ã™ã€‚æä¾›ã•ã‚Œã‚‹ã€Œåˆ†æãƒ‡ãƒ¼ã‚¿ï¼ˆJSONLï¼‰ã€ã‚’èª­ã¿ã€ä»¥ä¸‹ã®JSONå½¢å¼ã§PowerPointç”¨ã®åˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚\n\n[ {{ \"slide_title\": \"...\", \"slide_layout\": \"title_and_content\", \"slide_content\": [\"...\", \"...\"], \"image_base64\": null }} ]"

def run_step_c_analysis(
    analysis_prompt: str,
    jsonl_data_string: str
) -> str:
    """
    (Step C) gemini-2.5-pro ã‚’ä½¿ç”¨ã—ã¦ã€åˆ†æãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ§‹é€ åŒ–ãƒ¬ãƒãƒ¼ãƒˆ (JSON) ã‚’ç”Ÿæˆã™ã‚‹ã€‚
    (â˜…) ãƒ¢ãƒ‡ãƒ«: MODEL_PRO (gemini-2.5-pro)
    """
    logger.info("Step C AIãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ (Pro) å®Ÿè¡Œ...")
    
    # (â˜…) è¦ä»¶: Step C ã§ã¯ Pro ãƒ¢ãƒ‡ãƒ«ã‚’æ˜ç¤ºçš„ã«ä½¿ç”¨
    # (â˜…) ã”è¦æœ›ã«åŸºã¥ãã€1ä¸‡æ–‡å­—è¶…ã®å‡ºåŠ›ã‚’å¾—ã‚‹ãŸã‚ã€temperatureã‚’å°‘ã—ä¸Šã’(0.2)ã€è©³ç´°ãªè¨˜è¿°ã‚’ä¿ƒã™
    llm = get_llm(model_name=MODEL_PRO, temperature=0.2) 
    if llm is None:
        logger.error("run_step_c_analysis: LLM (Pro) ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")
        st.error(f"AIãƒ¢ãƒ‡ãƒ«({MODEL_PRO})ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚APIã‚­ãƒ¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        return '{"error": "AIãƒ¢ãƒ‡ãƒ«(Pro)ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"}'

    # (â˜…) æœ€çµ‚çš„ã« Pro ãƒ¢ãƒ‡ãƒ«ã«æ¸¡ã™ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    # æŒ‡ç¤ºãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ (FlashãŒç”Ÿæˆ) + å®Œå…¨ãªJSONLãƒ‡ãƒ¼ã‚¿
    final_prompt_to_pro = f"""
    {analysis_prompt}
    
    # åˆ†æãƒ‡ãƒ¼ã‚¿ (JSONL):
    {jsonl_data_string}
    
    # å›ç­” (æŒ‡ç¤ºã•ã‚ŒãŸJSONå½¢å¼ã®ã¿):
    """
    
    # (â˜…) ãƒˆãƒ¼ã‚¯ãƒ³æ•°ï¼ˆå‚è€ƒå€¤ï¼‰ã®ãƒ­ã‚®ãƒ³ã‚°
    # (æ—¥æœ¬èªã¯1æ–‡å­—ã‚ãŸã‚Šç´„1ã€œ2ãƒˆãƒ¼ã‚¯ãƒ³ã¨ä»®å®š)
    estimated_tokens = len(final_prompt_to_pro) * 1.5 
    logger.info(f"Step C (Pro) å®Ÿè¡Œã€‚æ¨å®šå…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {estimated_tokens:,.0f}")
    if estimated_tokens > 800000: # (â˜…) Pro 1M context window ã® 80%
         logger.warning(f"å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³ãŒéå¸¸ã«å¤šã„ ({estimated_tokens:,.0f})ã€‚å‡¦ç†ã«æ™‚é–“ãŒã‹ã‹ã‚‹ã‹ã€å¤±æ•—ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")

    
    try:
        response = llm.invoke(final_prompt_to_pro)
        response_str = response.content # (â˜…) .content ã‚’å–å¾—
        
        logger.info("Step C AIãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ (Pro) å®Œäº†ã€‚")
        logger.debug(f"Step C ç”Ÿå›ç­” (å…ˆé ­500æ–‡å­—): {response_str[:500]}")
        
        # (â˜…) å›ç­”ã‹ã‚‰ JSON ãƒªã‚¹ãƒˆ ( [...] ) ã‚’æŠ½å‡º
        # (â˜…) AIãŒãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³(```json ... ```)ã‚’ä»˜ã‘ã¦ã‚‚å¯¾å¿œã§ãã‚‹ã‚ˆã†ã«ä¿®æ­£
        match = re.search(r'```(?:json)?\s*(\[.*\])\s*```', response_str, re.DOTALL)
        if not match:
             match = re.search(r'\[.*\]', response_str, re.DOTALL) # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãªã—ã®å ´åˆ

        if match:
            json_report_str = match.group(1)
            
            # (â˜…) JSONã¨ã—ã¦æœ‰åŠ¹ã‹æ¤œè¨¼
            try:
                json.loads(json_report_str)
                logger.info(f"Step C (Pro) å›ç­”ã®JSONãƒ‘ãƒ¼ã‚¹æˆåŠŸã€‚ (ã‚µã‚¤ã‚º: {len(json_report_str):,} bytes)")
                return json_report_str # (â˜…) æˆåŠŸ: JSONæ–‡å­—åˆ—ã‚’è¿”ã™
            except json.JSONDecodeError as json_e:
                logger.error(f"AI (Pro) ã®å›ç­”ãŒJSONãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—: {json_e}")
                logger.error(f"AI (Pro) ç”Ÿå›ç­” (ãƒ‘ãƒ¼ã‚¹å¤±æ•—ç®‡æ‰€å‘¨è¾º): {json_report_str[max(0, json_e.pos-100):json_e.pos+100]}")
                return f'[{{"slide_title": "AIå›ç­”ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼", "slide_layout": "title_and_content", "slide_content": ["AIã®å›ç­”ãŒJSONå½¢å¼ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚", "{str(json_e)}", "Raw: {response_str[:500]}..."], "image_base64": null}}]'
        
        else:
            logger.error("AI (Pro) ã®å›ç­”ã«JSONãƒªã‚¹ãƒˆ [...] ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            return f'[{{"slide_title": "AIå›ç­”å½¢å¼ã‚¨ãƒ©ãƒ¼", "slide_layout": "title_and_content", "slide_content": ["AIãŒJSONãƒªã‚¹ãƒˆå½¢å¼ [...] ã§å›ç­”ã—ã¾ã›ã‚“ã§ã—ãŸã€‚", "Raw: {response_str[:500]}..."], "image_base64": null}}]'

    except Exception as e:
        logger.error(f"run_step_c_analysis error: {e}", exc_info=True)
        st.error(f"AIãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ (Pro) å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        return f'[{{"slide_title": "å®Ÿè¡Œæ™‚ã‚¨ãƒ©ãƒ¼", "slide_layout": "title_and_content", "slide_content": ["{str(e)}"], "image_base64": null}}]'


def render_step_c():
    """(Step C) AIãƒ¬ãƒãƒ¼ãƒˆç”ŸæˆUIã‚’æç”»ã™ã‚‹"""
    st.title(f"ğŸ–‹ï¸ Step C: AIåˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ (using {MODEL_PRO})")

    # Step C å›ºæœ‰ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆ
    if 'step_c_jsonl_data' not in st.session_state:
        st.session_state.step_c_jsonl_data = None
    if 'step_c_prompt' not in st.session_state:
        st.session_state.step_c_prompt = None
    if 'step_c_report_json' not in st.session_state:
        st.session_state.step_c_report_json = None

    # --- 1. ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ (è¦ä»¶â‘§) ---
    st.header("Step 1: åˆ†æãƒ‡ãƒ¼ã‚¿ (JSON) ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
    st.info("Step B ã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã—ãŸ `analysis_data.json` ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
    uploaded_report_file = st.file_uploader(
        "åˆ†æãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ« (analysis_data.json)",
        type=['json', 'jsonl', 'txt'], # (â˜…) å½¢å¼ã®å¤šæ§˜æ€§ã«å¯¾å¿œ
        key="step_c_uploader"
    )

    if uploaded_report_file:
        try:
            # (â˜…) èª­ã¿è¾¼ã‚“ã ã‚‰ã€ä»¥å‰ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚„çµæœã‚’ãƒªã‚»ãƒƒãƒˆ
            if st.session_state.step_c_jsonl_data is None:
                jsonl_data_string = uploaded_report_file.getvalue().decode('utf-8')
                st.session_state.step_c_jsonl_data = jsonl_data_string
                st.session_state.step_c_prompt = None # (â˜…) ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒªã‚»ãƒƒãƒˆ
                st.session_state.step_c_report_json = None # (â˜…) çµæœã‚’ãƒªã‚»ãƒƒãƒˆ
                st.success(f"ãƒ•ã‚¡ã‚¤ãƒ«ã€Œ{uploaded_report_file.name}ã€èª­è¾¼å®Œäº†")
            
            # (â˜…) ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸã‚‰ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è‡ªå‹•ç”Ÿæˆ (è¦ä»¶â‘§)
            if st.session_state.step_c_prompt is None: # ã¾ã ç”Ÿæˆã•ã‚Œã¦ã„ãªã„å ´åˆã®ã¿
                with st.spinner(f"AI ({MODEL_FLASH_LITE}) ãŒ Step B ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¸‹èª­ã¿ã—ã€Proãƒ¢ãƒ‡ãƒ«ã¸ã®æŒ‡ç¤ºã‚’ç”Ÿæˆä¸­..."):
                    st.session_state.step_c_prompt = generate_step_c_prompt(st.session_state.step_c_jsonl_data)
                st.success("Proãƒ¢ãƒ‡ãƒ«ã¸ã®æŒ‡ç¤ºãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒè‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚")

        except Exception as e:
            logger.error(f"Step C ãƒ•ã‚¡ã‚¤ãƒ«èª­è¾¼ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            return
    else:
        # (â˜…) ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚¯ãƒªã‚¢ã•ã‚ŒãŸã‚‰ã€é–¢é€£ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã‚‚ã‚¯ãƒªã‚¢
        st.session_state.step_c_jsonl_data = None
        st.session_state.step_c_prompt = None
        st.session_state.step_c_report_json = None
        st.warning("åˆ†æã‚’ç¶šã‘ã‚‹ã«ã¯ã€Step B ã§ç”Ÿæˆã—ãŸ JSON ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
        return

    # --- 2. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ç¢ºèªãƒ»ç·¨é›† (è¦ä»¶â‘¨) ---
    st.header(f"Step 2: AI ({MODEL_PRO}) ã¸ã®æŒ‡ç¤ºãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ")
    if st.session_state.step_c_prompt:
        st.markdown(f"AI ({MODEL_FLASH_LITE}) ãŒä»¥ä¸‹ã®æŒ‡ç¤ºãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è‡ªå‹•ç”Ÿæˆã—ã¾ã—ãŸã€‚å®Ÿè¡Œå‰ã«ç·¨é›†å¯èƒ½ã§ã™ã€‚")
        
        edited_prompt = st.text_area(
            "AIã¸ã®æŒ‡ç¤ºãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆç·¨é›†å¯ï¼‰:",
            value=st.session_state.step_c_prompt,
            height=400, # (â˜…) ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒé•·ããªã£ãŸãŸã‚é«˜ã•ã‚’å¢—ã‚„ã™
            key="step_c_prompt_editor"
        )
        st.session_state.step_c_prompt = edited_prompt # ç·¨é›†å†…å®¹ã‚’å³åº§ã«ä¿å­˜
    else:
        st.warning("ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒã‚ã‚Šã¾ã›ã‚“ã€‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å†ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
        return

    # --- 3. åˆ†æãƒ¬ãƒãƒ¼ãƒˆã®å®Ÿè¡Œ (è¦ä»¶â‘¨) ---
    st.header("Step 3: AIåˆ†æãƒ¬ãƒãƒ¼ãƒˆã®å®Ÿè¡Œ")
    st.markdown(f"**ï¼ˆ(â˜…) è­¦å‘Š: {MODEL_PRO} ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚å®Ÿè¡Œã«ã¯æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ï¼‰**")
    
    if st.button(f"åˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ (Step 3)", key="execute_button_C", type="primary", use_container_width=True):
        if not st.session_state.step_c_jsonl_data or not st.session_state.step_c_prompt:
            st.error("ãƒ‡ãƒ¼ã‚¿ã¾ãŸã¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            return
        
        # (â˜…) è¦ä»¶: èª­ã¿è¾¼ã¿æ™‚é–“ (é€²æ—) ã‚’è¡¨ç¤º
        with st.spinner(f"AI ({MODEL_PRO}) ãŒåˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆä¸­ã§ã™... (æ•°åˆ†ã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™)"):
            st.session_state.step_c_report_json = run_step_c_analysis(
                st.session_state.step_c_prompt,
                st.session_state.step_c_jsonl_data
            )
        st.success("AIã«ã‚ˆã‚‹åˆ†æãƒ¬ãƒãƒ¼ãƒˆãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸï¼")

    # --- 4. çµæœã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã¨ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ (è¦ä»¶â‘©) ---
    if st.session_state.step_c_report_json:
        st.header("Step 4: åˆ†æãƒ¬ãƒãƒ¼ãƒˆï¼ˆJSONï¼‰ã®ç¢ºèªã¨ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ")
        st.info("ä»¥ä¸‹ã®æ§‹é€ åŒ–JSONã¯ã€Step D (PowerPointç”Ÿæˆ) ã§ä½¿ç”¨ã—ã¾ã™ã€‚")

        # (â˜…) è¦ä»¶â‘©: ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        st.download_button(
            label="åˆ†æãƒ¬ãƒãƒ¼ãƒˆ (report_for_powerpoint.json) ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            data=st.session_state.step_c_report_json,
            file_name="report_for_powerpoint.json",
            mime="application/json",
            type="primary",
            use_container_width=True
        )

        st.markdown("---")
        st.subheader("ç”Ÿæˆã•ã‚ŒãŸãƒ¬ãƒãƒ¼ãƒˆ ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")
        
        try:
            # (â˜…) ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ç”¨ã«JSONã‚’ãƒ‘ãƒ¼ã‚¹
            report_data = json.loads(st.session_state.step_c_report_json)
            if isinstance(report_data, list) and all(isinstance(item, dict) for item in report_data):
                st.text_area(
                    "AIãŒç”Ÿæˆã—ãŸæ§‹é€ åŒ–JSON (ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼: å…ˆé ­5000æ–‡å­—)",
                    value=st.session_state.step_c_report_json[:5000] + "...",
                    height=300,
                    key="json_preview_C",
                    disabled=True
                )
                
                st.markdown("---")
                st.subheader(f"ã‚¹ãƒ©ã‚¤ãƒ‰æ§‹æˆ ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ ({len(report_data)}æš)")
                for i, slide in enumerate(report_data):
                    # (â˜…) æ–°ã—ã„JSONå½¢å¼ã«å¯¾å¿œ
                    title = slide.get('slide_title', 'ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ãªã—ï¼‰')
                    layout = slide.get('slide_layout', 'N/A')
                    content_preview = slide.get('slide_content', [])[0] if slide.get('slide_content') else "ï¼ˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãªã—ï¼‰"
                    has_image = "æœ‰ã‚Š" if slide.get("image_base64") else "ç„¡ã—"
                    
                    with st.expander(f"**{i+1}: {title}** (Layout: {layout}, Image: {has_image})"):
                        st.markdown(f"**å†…å®¹ (æŠœç²‹):**\n- {content_preview}...")
            else:
                st.error("AIã®å›ç­”ãŒæœŸå¾…ã—ãŸã‚¹ãƒ©ã‚¤ãƒ‰ã®ãƒªã‚¹ãƒˆå½¢å¼ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
                st.text_area("AIã®ç”Ÿå›ç­” (JSON):", value=st.session_state.step_c_report_json, height=200, disabled=True)
                
        except Exception as e:
            st.error(f"ãƒ¬ãƒãƒ¼ãƒˆã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            st.text_area("AIã®ç”Ÿå›ç­” (ãƒ‘ãƒ¼ã‚¹å¤±æ•—):", value=st.session_state.step_c_report_json, height=200, disabled=True)
            
        st.success("ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€Step D (PowerPointç”Ÿæˆ) ã«é€²ã‚“ã§ãã ã•ã„ã€‚")

# --- 10. (â˜…) Step D: PowerPointç”Ÿæˆ (Pro / Flashãƒ¢ãƒ‡ãƒ«) ---
# (â˜…) ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯æ¬¡ã®ãƒ–ãƒ­ãƒƒã‚¯ã§æç¤ºã—ã¾ã™ã€‚

def find_layout_by_name(prs: pptx.presentation.Presentation, layout_name_variants: List[str]) -> Optional[pptx.slide.SlideLayout]:
    """
    (â˜…) ãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒã‚¹ã‚¿ãƒ¼ã‹ã‚‰ã€æŒ‡å®šã•ã‚ŒãŸãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆåã®ãƒªã‚¹ãƒˆã«ä¸€è‡´ã™ã‚‹ã‚¹ãƒ©ã‚¤ãƒ‰ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã‚’æ¢ã™ã€‚
    ï¼ˆä¾‹: "text_and_image" -> "ãƒ†ã‚­ã‚¹ãƒˆã¨ç”»åƒ", "Text and Image" ãªã©ã«å¯¾å¿œï¼‰
    """
    logger.info(f"ã‚¹ãƒ©ã‚¤ãƒ‰ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆæ¤œç´¢: {layout_name_variants}")
    layout_name_lower = [name.lower() for name in layout_name_variants]
    
    for layout in prs.slide_layouts:
        if layout.name.lower() in layout_name_lower:
            logger.info(f"  -> ä¸€è‡´: '{layout.name}' ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
            return layout
            
    logger.warning(f"  -> {layout_name_variants} ã«ä¸€è‡´ã™ã‚‹ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
    return None

def create_powerpoint_presentation(
    template_file: Optional[BytesIO],
    report_data: List[Dict[str, Any]]
) -> BytesIO:
    """
    (Step D) ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ(.pptx)ã¨ã‚¹ãƒ©ã‚¤ãƒ‰æ§‹æˆ(JSON)ã«åŸºã¥ãã€
    python-pptx ã‚’ä½¿ç”¨ã—ã¦æœ€çµ‚çš„ãªPowerPointãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆã™ã‚‹ã€‚
    (â˜…) ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆå´©ã‚Œã¨ã‚°ãƒ©ãƒ•æŒ¿å…¥ã«å¯¾å¿œã™ã‚‹ãŸã‚ã€ãƒ­ã‚¸ãƒƒã‚¯ã‚’å¤§å¹…ã«æ”¹ä¿®
    """
    logger.info("PowerPointç”Ÿæˆå‡¦ç† é–‹å§‹...")
    
    try:
        # (â˜…) 1. ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®èª­ã¿è¾¼ã¿
        if template_file:
            template_file.seek(0)
            prs = Presentation(template_file)
            logger.info("ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½¿ç”¨ã—ã¦PPTXã‚’ç”Ÿæˆã—ã¾ã™ã€‚")
        else:
            prs = Presentation()
            logger.info("ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½¿ç”¨ã—ã¦PPTXã‚’ç”Ÿæˆã—ã¾ã™ã€‚")

        # (â˜…) 2. æ¨™æº–ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã®äº‹å‰å®šç¾©
        # (â˜…) StepCã®AIãŒç”Ÿæˆã™ã‚‹ "slide_layout" æ–‡å­—åˆ—ã¨ã€pptxãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå†…ã®å®Ÿéš›ã®åå‰ã‚’ãƒãƒƒãƒ”ãƒ³ã‚°
        layout_map = {
            "title_only": find_layout_by_name(prs, ["title slide", "ã‚¿ã‚¤ãƒˆãƒ« ã‚¹ãƒ©ã‚¤ãƒ‰", "title only", "ã‚¿ã‚¤ãƒˆãƒ«ã®ã¿"]),
            "title_and_content": find_layout_by_name(prs, ["title and content", "ã‚¿ã‚¤ãƒˆãƒ«ã¨ã‚³ãƒ³ãƒ†ãƒ³ãƒ„", "ã‚¿ã‚¤ãƒˆãƒ«ã¨å†…å®¹"]),
            "text_and_image": find_layout_by_name(prs, ["two content", "2ã¤ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„", "text and image", "ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆ"]),
            # (â˜…) ç›®æ¬¡ç”¨ã®ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã‚‚æ¢ã™
            "agenda": find_layout_by_name(prs, ["section header", "ã‚»ã‚¯ã‚·ãƒ§ãƒ³è¦‹å‡ºã—", "ç›®æ¬¡", "agenda"]),
        }
        
        # (â˜…) ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ (æœ€ã‚‚æ±ç”¨çš„ãªã‚‚ã®)
        fallback_layout = layout_map["title_and_content"] or prs.slide_layouts[1]
        if layout_map["text_and_image"] is None:
            logger.warning("ã€Œãƒ†ã‚­ã‚¹ãƒˆã¨ç”»åƒã€ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚'title_and_content' ã§ä»£ç”¨ã—ã¾ã™ã€‚")
            layout_map["text_and_image"] = fallback_layout
        if layout_map["title_only"] is None:
            layout_map["title_only"] = prs.slide_layouts[0]
        if layout_map["agenda"] is None:
            layout_map["agenda"] = fallback_layout # ç›®æ¬¡ã‚‚æ±ç”¨ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã§ä»£ç”¨


        # (â˜…) 3. ã‚¹ãƒ©ã‚¤ãƒ‰ã®ç”Ÿæˆ (JSONãƒ‡ãƒ¼ã‚¿ã‚’ãƒ«ãƒ¼ãƒ—)
        
        # (â˜…) --- 3.1. è¡¨ç´™ã‚¹ãƒ©ã‚¤ãƒ‰ ---
        # (JSONã®0ç•ªç›®ãŒ "title_only" ã§ã‚ã‚‹ã“ã¨ã‚’æœŸå¾…)
        first_slide_data = report_data[0]
        if first_slide_data.get("slide_layout") == "title_only":
            slide = prs.slides.add_slide(layout_map["title_only"])
            try:
                slide.shapes.title.text = first_slide_data.get("slide_title", "åˆ†æãƒ¬ãƒãƒ¼ãƒˆ")
            except: pass # (â˜…) ã‚µãƒ–ã‚¿ã‚¤ãƒˆãƒ«ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãŒãªã„å ´åˆãªã©
            try:
                # (â˜…) ã‚µãƒ–ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆå¤§æŠµ idx=1ï¼‰ã«ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®æœ€åˆã®è¦ç´ ã‚’å…¥ã‚Œã‚‹
                if slide.placeholders[1]:
                     slide.placeholders[1].text = first_slide_data.get("slide_content", [""])[0]
            except: pass
            
            # (â˜…) è¡¨ç´™ã®å¾Œã¯ report_data ã‹ã‚‰å‰Šé™¤
            report_data = report_data[1:]
        
        
        # (â˜…) --- 3.2. ç›®æ¬¡(Agenda)ã‚¹ãƒ©ã‚¤ãƒ‰ã®è‡ªå‹•ç”Ÿæˆ ---
        # (ã”æŒ‡æ‘˜ã®ã€Œç›®æ¬¡ãŒãªã„ã€å•é¡Œã«å¯¾å¿œ)
        try:
            logger.info("ç›®æ¬¡ã‚¹ãƒ©ã‚¤ãƒ‰ã‚’è‡ªå‹•ç”Ÿæˆã—ã¾ã™...")
            agenda_slide = prs.slides.add_slide(layout_map["agenda"] or fallback_layout)
            agenda_slide.shapes.title.text = "ç›®æ¬¡"
            
            # (â˜…) ç›®æ¬¡ç”¨ã®æœ¬æ–‡ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ã‚’æ¢ã™
            agenda_body_shape = None
            for shape in agenda_slide.placeholders:
                 if shape.placeholder_format.idx > 0: # 0ã¯ã‚¿ã‚¤ãƒˆãƒ«
                     agenda_body_shape = shape
                     break
            
            if agenda_body_shape:
                tf = agenda_body_shape.text_frame
                tf.clear()
                # (â˜…) JSONã®æ®‹ã‚Šã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’ç®‡æ¡æ›¸ãã«ã™ã‚‹
                for i, slide_data in enumerate(report_data):
                    p = tf.add_paragraph()
                    p.text = f"{i+1}. {slide_data.get('slide_title', 'ï¼ˆç„¡é¡Œã®ã‚¹ãƒ©ã‚¤ãƒ‰ï¼‰')}"
                    p.level = 0
            else:
                 logger.warning("ç›®æ¬¡ã‚¹ãƒ©ã‚¤ãƒ‰ã®æœ¬æ–‡ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                 
        except Exception as e:
            logger.error(f"ç›®æ¬¡ã‚¹ãƒ©ã‚¤ãƒ‰ã®è‡ªå‹•ç”Ÿæˆã«å¤±æ•—: {e}")


        # (â˜…) --- 3.3. ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¹ãƒ©ã‚¤ãƒ‰ (æ®‹ã‚Š) ---
        for i, slide_data in enumerate(report_data):
            slide_title = slide_data.get("slide_title", f"ã‚¹ãƒ©ã‚¤ãƒ‰ {i+3}")
            slide_layout_key = slide_data.get("slide_layout", "title_and_content")
            slide_content = slide_data.get("slide_content", ["ï¼ˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãªã—ï¼‰"])
            image_base64 = slide_data.get("image_base64")

            # (â˜…) ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆãƒãƒƒãƒ”ãƒ³ã‚°
            # (â˜…) ç”»åƒãŒã‚ã‚‹ã®ã«ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆãŒ "title_and_content" ã®å ´åˆã€"text_and_image" ã«å¼·åˆ¶å¤‰æ›´
            if image_base64 and slide_layout_key == "title_and_content":
                logger.info(f"ã‚¹ãƒ©ã‚¤ãƒ‰ '{slide_title}': ç”»åƒãŒã‚ã‚‹ãŸã‚ã€ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã‚’ 'text_and_image' ã«å¤‰æ›´ã—ã¾ã™ã€‚")
                slide_layout_key = "text_and_image"
            
            # (â˜…) ãƒãƒƒãƒ—ã‹ã‚‰ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã‚’å–å¾—ã€ãªã‘ã‚Œã°ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            layout_to_use = layout_map.get(slide_layout_key) or fallback_layout
            
            slide = prs.slides.add_slide(layout_to_use)
            
            try:
                slide.shapes.title.text = slide_title
            except Exception as e:
                logger.warning(f"ã‚¹ãƒ©ã‚¤ãƒ‰ {i+3} ã®ã‚¿ã‚¤ãƒˆãƒ«è¨­å®šå¤±æ•—: {e}")

            # (â˜…) --- ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¨ç”»åƒã®é…ç½® (æœ€é‡è¦) ---
            try:
                # (â˜…) ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ (idx > 0) ã‚’å…¨ã¦è¦‹ã¤ã‘ã‚‹
                content_placeholders = [
                    shape for shape in slide.placeholders 
                    if shape.placeholder_format.idx > 0
                ]
                
                # (â˜…) ç”»åƒãŒã‚ã‚‹å ´åˆã®å‡¦ç† (text_and_image)
                if image_base64 and layout_key == "text_and_image" and len(content_placeholders) >= 2:
                    
                    # (â˜…) ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€[0] (é€šå¸¸ã¯å·¦å´) ã«ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥ã‚Œã‚‹
                    tf = content_placeholders[0].text_frame
                    tf.clear()
                    p = tf.paragraphs[0]
                    p.text = str(slide_content[0])
                    for item in slide_content[1:]:
                        p = tf.add_paragraph()
                        p.text = str(item)
                    
                    # (â˜…) ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€[1] (é€šå¸¸ã¯å³å´) ã«ç”»åƒã‚’å…¥ã‚Œã‚‹
                    try:
                        img_bytes = base64.b64decode(image_base64)
                        img_stream = BytesIO(img_bytes)
                        
                        # (â˜…) .insert_picture() ã‚’ä½¿ç”¨ (ã“ã‚ŒãŒæ­£ã—ã„æ–¹æ³•)
                        placeholder_for_image = content_placeholders[1]
                        placeholder_for_image.insert_picture(img_stream)
                        logger.info(f"ã‚¹ãƒ©ã‚¤ãƒ‰ '{slide_title}': ã‚°ãƒ©ãƒ•ç”»åƒã®æŒ¿å…¥ã«æˆåŠŸã€‚")

                    except Exception as e:
                        logger.error(f"ã‚¹ãƒ©ã‚¤ãƒ‰ '{slide_title}': ã‚°ãƒ©ãƒ•ç”»åƒã®æŒ¿å…¥ã«å¤±æ•—: {e}")
                        # (â˜…) å¤±æ•—æ™‚ã¯ã€ç”»åƒãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ã«ã‚‚ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥ã‚Œã‚‹
                        content_placeholders[1].text = f"ï¼ˆç”»åƒæŒ¿å…¥ã‚¨ãƒ©ãƒ¼: {e}ï¼‰"

                # (â˜…) ç”»åƒãŒãªã„ (ã¾ãŸã¯ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆãŒä¸ä¸€è‡´) ã®å ´åˆã®å‡¦ç†
                else:
                    if not content_placeholders:
                         logger.warning(f"ã‚¹ãƒ©ã‚¤ãƒ‰ '{slide_title}': ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
                         continue
                         
                    # (â˜…) æœ€åˆã®ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ã«ãƒ†ã‚­ã‚¹ãƒˆã‚’æµã—è¾¼ã‚€
                    tf = content_placeholders[0].text_frame
                    tf.clear()
                    p = tf.paragraphs[0]
                    p.text = str(slide_content[0])
                    for item in slide_content[1:]:
                        p = tf.add_paragraph()
                        p.text = str(item)

            except Exception as e:
                logger.error(f"ã‚¹ãƒ©ã‚¤ãƒ‰ {i+3} ('{slide_title}') ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„/ç”»åƒè¨­å®šä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)

        logger.info("PowerPointç”Ÿæˆå‡¦ç† å®Œäº†ã€‚")

        # (â˜…) 4. ãƒ¡ãƒ¢ãƒªï¼ˆBytesIOï¼‰ã«ä¿å­˜ã—ã¦è¿”ã™
        file_stream = BytesIO()
        prs.save(file_stream)
        file_stream.seek(0)
        return file_stream

    except Exception as e:
        logger.error(f"create_powerpoint_presentation å…¨ä½“ã§ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        st.error(f"PowerPointã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return None

def run_step_d_ai_correction(
    current_report_json: str,
    correction_prompt: str
) -> str:
    """
    (Step D) ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ä¿®æ­£æŒ‡ç¤ºã«åŸºã¥ãã€AIãŒã‚¹ãƒ©ã‚¤ãƒ‰æ§‹æˆ(JSON)ã‚’ä¿®æ­£ã—ã¦è¿”ã™ã€‚
    (â˜…) ãƒ¢ãƒ‡ãƒ«: MODEL_PRO (gemini-2.5-pro) ã¾ãŸã¯ MODEL_FLASH
    (â˜…) AIã¸ã®æŒ‡ç¤ºãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ã€æ–°ã—ã„JSONæ§‹é€ ã«åˆã‚ã›ã‚‹
    """
    logger.info("Step D AIã«ã‚ˆã‚‹JSONä¿®æ­£ (Pro) å®Ÿè¡Œ...")
    
    # (â˜…) ä¿®æ­£ã¯ Proãƒ¢ãƒ‡ãƒ« (é«˜å“è³ª) ã‚’æ¨å¥¨
    llm = get_llm(model_name=MODEL_PRO, temperature=0.1)
    if llm is None:
        logger.error("run_step_d_ai_correction: LLM (Pro) ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")
        st.error(f"AIãƒ¢ãƒ‡ãƒ«({MODEL_PRO})ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚APIã‚­ãƒ¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        return current_report_json # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å…ƒã®JSONã‚’è¿”ã™

    prompt = PromptTemplate.from_template(
        """
        ã‚ãªãŸã¯ã€PowerPointãƒ¬ãƒãƒ¼ãƒˆã®JSONæ§‹æˆãƒ‡ãƒ¼ã‚¿ã‚’ç·¨é›†ã™ã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
        ä»¥ä¸‹ã®ã€Œç¾åœ¨ã®ãƒ¬ãƒãƒ¼ãƒˆJSONã€ã«å¯¾ã—ã€ã€Œä¿®æ­£æŒ‡ç¤ºã€ã‚’é©ç”¨ã—ã€ä¿®æ­£å¾Œã®ã€JSONã®ã¿ã€‘ã‚’å›ç­”ã—ã¦ãã ã•ã„ã€‚

        # 1. æŒ‡ç¤º
        * ã€Œä¿®æ­£æŒ‡ç¤ºã€ã‚’æ­£ç¢ºã«å®Ÿè¡Œã™ã‚‹ï¼ˆä¾‹ï¼šã€Œã‚¹ãƒ©ã‚¤ãƒ‰ã‚’å‰Šé™¤ã€ã€Œé †ç•ªã‚’å¤‰æ›´ã€ã€Œæ–‡ç« ã‚’è¦ç´„ã€ï¼‰ã€‚
        * å½¢å¼ã¯ã€å…¥åŠ›ã¨ã€å…¨ãåŒã˜JSONãƒªã‚¹ãƒˆå½¢å¼ã€‘ `[ {{ ... }} ]` ã‚’ç¶­æŒã™ã‚‹ã€‚
        * JSONä»¥å¤–ã®ä½™è¨ˆãªãƒ†ã‚­ã‚¹ãƒˆï¼ˆã€Œä¿®æ­£ã—ã¾ã—ãŸã€ãªã©ï¼‰ã¯ã€çµ¶å¯¾ã«ã€‘å«ã‚ãªã„ã€‚

        # 2. JSONã®æ§‹é€  (å³å®ˆ)
        å„ã‚¹ãƒ©ã‚¤ãƒ‰ã¯ä»¥ä¸‹ã®ã‚­ãƒ¼ã‚’æŒã¤è¾æ›¸ã§ã™ã€‚ã“ã®æ§‹é€ ã‚’çµ¶å¯¾ã«å´©ã•ãªã„ã§ãã ã•ã„ã€‚
        {{
          "slide_title": "ï¼ˆã‚¿ã‚¤ãƒˆãƒ«æ–‡å­—åˆ—ï¼‰",
          "slide_layout": "ï¼ˆ"title_and_content" ã‚„ "text_and_image" ãªã©ã®æ–‡å­—åˆ—ï¼‰",
          "slide_content": [ "ï¼ˆç®‡æ¡æ›¸ãã®æ–‡å­—åˆ—ãƒªã‚¹ãƒˆï¼‰" ],
          "image_base64": "ï¼ˆBase64æ–‡å­—åˆ—ã€ã¾ãŸã¯ nullï¼‰"
        }}

        # 3. ç¾åœ¨ã®ãƒ¬ãƒãƒ¼ãƒˆJSON:
        {current_json}

        # 4. ä¿®æ­£æŒ‡ç¤º:
        {user_prompt}

        # 5. å›ç­” (ä¿®æ­£å¾Œã®JSONã®ã¿):
        """
    )
    chain = prompt | llm | StrOutputParser()
    
    try:
        response_str = chain.invoke({
            "current_json": current_report_json,
            "user_prompt": correction_prompt
        })
        
        # (â˜…) å›ç­”ã‹ã‚‰ JSON ãƒªã‚¹ãƒˆ ( [...] ) ã‚’æŠ½å‡º (ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å¯¾å¿œ)
        match = re.search(r'```(?:json)?\s*(\[.*\])\s*```', response_str, re.DOTALL)
        if not match:
             match = re.search(r'\[.*\]', response_str, re.DOTALL) # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãªã—ã®å ´åˆ

        if match:
            json_report_str = match.group(0)
            
            # (â˜…) JSONã¨ã—ã¦æœ‰åŠ¹ã‹æ¤œè¨¼
            try:
                json.loads(json_report_str)
                logger.info("Step D AIã«ã‚ˆã‚‹JSONä¿®æ­£ å®Œäº†ã€‚")
                return json_report_str # (â˜…) æˆåŠŸ: ä¿®æ­£å¾Œã®JSONæ–‡å­—åˆ—ã‚’è¿”ã™
            except json.JSONDecodeError as json_e:
                logger.error(f"AI (Pro) ã®JSONä¿®æ­£å›ç­”ãŒãƒ‘ãƒ¼ã‚¹å¤±æ•—: {json_e}")
                st.error("AIãŒJSONå½¢å¼ã§å›ç­”ã—ã¾ã›ã‚“ã§ã—ãŸã€‚ä¿®æ­£ã¯é©ç”¨ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
                return current_report_json # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å…ƒã®JSONã‚’è¿”ã™
        
        else:
            logger.error("AI (Pro) ã®JSONä¿®æ­£å›ç­”ã« [...] ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            st.error("AIã®å›ç­”å½¢å¼ãŒä¸æ­£ã§ã™ã€‚ä¿®æ­£ã¯é©ç”¨ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
            return current_report_json # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å…ƒã®JSONã‚’è¿”ã™

    except Exception as e:
        logger.error(f"run_step_d_ai_correction error: {e}", exc_info=True)
        st.error(f"AIã«ã‚ˆã‚‹ä¿®æ­£å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        return current_report_json # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å…ƒã®JSONã‚’è¿”ã™

def render_step_d():
    """(Step D) PowerPointç”ŸæˆUIã‚’æç”»ã™ã‚‹"""
    st.title(f"ãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ (PowerPoint) ç”Ÿæˆ (Step D)")

    # Step D å›ºæœ‰ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆ
    if 'step_d_template_file' not in st.session_state:
        st.session_state.step_d_template_file = None
    if 'step_d_report_data' not in st.session_state:
        st.session_state.step_d_report_data = [] # (â˜…) JSONã‚’ãƒ‘ãƒ¼ã‚¹ã—ãŸã€Œè¾æ›¸ã®ãƒªã‚¹ãƒˆã€
    if 'step_d_generated_pptx' not in st.session_state:
        st.session_state.step_d_generated_pptx = None
    # (â˜…) Tipsç”¨ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆ (StepA/Bã§åˆæœŸåŒ–ã•ã‚Œã¦ã„ã‚Œã°æµç”¨ã•ã‚Œã‚‹)
    if 'tips_list' not in st.session_state:
        st.session_state.tips_list = []
    if 'current_tip_index' not in st.session_state:
        st.session_state.current_tip_index = 0
    if 'last_tip_time' not in st.session_state:
        st.session_state.last_tip_time = time.time()

    # --- 1. ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ (è¦ä»¶â‘ª) ---
    st.header("Step 1: ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ PowerPoint ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
    st.info("ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰ä½¿ç”¨ã—ãŸã„ .pptx ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãŒã‚ã‚Œã°ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚ãªã‘ã‚Œã°ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‡ã‚¶ã‚¤ãƒ³ã§ç”Ÿæˆã•ã‚Œã¾ã™ã€‚")
    template_file = st.file_uploader(
        "PowerPoint ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ (.pptx)",
        type=['pptx'],
        key="step_d_template_uploader"
    )
    
    if template_file:
        st.session_state.step_d_template_file = BytesIO(template_file.getvalue())
        st.success(f"ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã€Œ{template_file.name}ã€ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")
    
    # --- 2. Step C åˆ†æçµæœã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ (è¦ä»¶â‘«) ---
    st.header("Step 2: Step C åˆ†æãƒ¬ãƒãƒ¼ãƒˆ (JSON) ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
    st.info("Step C ã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã—ãŸ `report_for_powerpoint.json` ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
    report_file = st.file_uploader(
        "åˆ†æãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ« (report_for_powerpoint.json)",
        type=['json'],
        key="step_d_report_uploader"
    )

    if report_file:
        try:
            # (â˜…) ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ–°ã—ãã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸã‚‰ã€æ—¢å­˜ã®æ§‹æˆã‚’ãƒªã‚»ãƒƒãƒˆ
            if not st.session_state.step_d_report_data:
                report_json_string = report_file.getvalue().decode('utf-8')
                report_data = json.loads(report_json_string)
                
                # (â˜…) æ­£å¸¸ãªã‚¹ãƒ©ã‚¤ãƒ‰æ§‹æˆ (è¾æ›¸ã®ãƒªã‚¹ãƒˆ) ã‹ã‚’ãƒã‚§ãƒƒã‚¯
                if isinstance(report_data, list) and all(isinstance(item, dict) for item in report_data):
                    st.session_state.step_d_report_data = report_data
                    st.success(f"åˆ†æãƒ¬ãƒãƒ¼ãƒˆã€Œ{report_file.name}ã€ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ ({len(report_data)}ã‚¹ãƒ©ã‚¤ãƒ‰)ã€‚")
                else:
                    st.error("ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸJSONãŒæœŸå¾…ã™ã‚‹å½¢å¼ï¼ˆã‚¹ãƒ©ã‚¤ãƒ‰ã®ãƒªã‚¹ãƒˆï¼‰ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
                    st.session_state.step_d_report_data = []
        except Exception as e:
            logger.error(f"Step D JSONãƒ¬ãƒãƒ¼ãƒˆèª­è¾¼ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            st.error(f"åˆ†æãƒ¬ãƒãƒ¼ãƒˆã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            st.session_state.step_d_report_data = []
    
    if not st.session_state.step_d_report_data:
        # (â˜…) ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚¯ãƒªã‚¢ã•ã‚ŒãŸã‚‰ã€é–¢é€£ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã‚‚ã‚¯ãƒªã‚¢
        st.session_state.step_d_report_data = []
        st.session_state.step_d_generated_pptx = None
        st.warning("PowerPointã‚’ç”Ÿæˆã™ã‚‹ã«ã¯ã€Step C ã§ç”Ÿæˆã—ãŸ JSON ãƒ¬ãƒãƒ¼ãƒˆã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
        return

    # --- 3. ç›®æ¬¡ï¼ˆã‚¹ãƒ©ã‚¤ãƒ‰æ§‹æˆï¼‰ã®ç·¨é›† (è¦ä»¶â‘«) ---
    st.header("Step 3: ã‚¹ãƒ©ã‚¤ãƒ‰æ§‹æˆã®ç¢ºèªãƒ»ç·¨é›†")
    st.info("ï¼ˆ(â˜…) ãƒã‚¦ã‚¹ã®ãƒ‰ãƒ©ãƒƒã‚°ï¼†ãƒ‰ãƒ­ãƒƒãƒ—ã§ã‚¹ãƒ©ã‚¤ãƒ‰ã®é †ç•ªã‚’å…¥ã‚Œæ›¿ãˆã‚‹ã“ã¨ãŒã§ãã¾ã™ï¼‰")

    try:
        # (â˜…) --- sort_items ãŒ list[str] ã‚’è¦æ±‚ã™ã‚‹ã‚¨ãƒ©ãƒ¼ã¸ã®å¯¾å¿œ ---
        headers_list = []
        header_to_item_map = {}
        
        if not st.session_state.step_d_report_data:
             st.warning("JSONãƒ‡ãƒ¼ã‚¿ãŒç©ºã‹ã€æ­£ã—ãèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
             return

        for i, item in enumerate(st.session_state.step_d_report_data):
            if not isinstance(item, dict):
                st.error(f"ãƒ‡ãƒ¼ã‚¿å½¢å¼ã‚¨ãƒ©ãƒ¼: {item} ã¯è¾æ›¸ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
                continue
            
            # (â˜…) æ–°ã—ã„JSONå½¢å¼ã«å¯¾å¿œã—ã¦è¡¨ç¤ºã‚’ãƒªãƒƒãƒã«
            title = item.get('slide_title', 'ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ãªã—ï¼‰')
            layout = item.get('slide_layout', 'N/A')
            has_image = "ğŸ–¼ï¸" if item.get("image_base64") else "ğŸ“„"
            # (â˜…) ãƒ˜ãƒƒãƒ€ãƒ¼ (Markdownæ–‡å­—åˆ—)ã€‚ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä»˜ã‘ã¦ä¸€æ„æ€§ã‚’æ‹…ä¿
            header_str = f"**{i+1}: {title}** (Layout: `{layout}`, {has_image})"
            headers_list.append(header_str)
            header_to_item_map[header_str] = item

        # (â˜…) 2. sort_items ã«ã€Œæ–‡å­—åˆ—ã®ãƒªã‚¹ãƒˆ (list[str])ã€ã‚’æ¸¡ã™
        if not all(isinstance(h, str) for h in headers_list):
            st.error("å†…éƒ¨ã‚¨ãƒ©ãƒ¼: ãƒ˜ãƒƒãƒ€ãƒ¼ãƒªã‚¹ãƒˆã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
            return

        sorted_headers = sort_items(
            items=headers_list, # (â˜…) list[str] ã‚’æ¸¡ã™
            key="sortable_slides_v3" # (â˜…) ã‚­ãƒ¼ã‚’å¤‰æ›´
        )
        
        # (â˜…) 3. ä¸¦ã³æ›¿ãˆã‚‰ã‚ŒãŸãƒ˜ãƒƒãƒ€ãƒ¼ã®ãƒªã‚¹ãƒˆã‚’ä½¿ã„ã€å…ƒã®è¾æ›¸ã®ãƒªã‚¹ãƒˆã‚’å†æ§‹ç¯‰
        cleaned_sorted_data = []
        for header in sorted_headers:
            if header in header_to_item_map:
                cleaned_sorted_data.append(header_to_item_map[header])
            else:
                logger.error(f"ãƒãƒƒãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: ã‚½ãƒ¼ãƒˆå¾Œã®ãƒ˜ãƒƒãƒ€ãƒ¼ '{header}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            
        # (â˜…) 4. ä¸¦ã³æ›¿ãˆã‚‰ã‚ŒãŸã‚¯ãƒªãƒ¼ãƒ³ãªçµæœã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã«ä¸Šæ›¸ãä¿å­˜
        st.session_state.step_d_report_data = cleaned_sorted_data
        
    except Exception as e:
        logger.error(f"streamlit-sortables å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        st.error(f"ã‚¹ãƒ©ã‚¤ãƒ‰ç·¨é›†UIã®æç”»ã«å¤±æ•—: {e}ã€‚")
        if st.session_state.step_d_report_data:
            for item in st.session_state.step_d_report_data:
                st.markdown(f"- **{item.get('slide_title', 'N/A')}**")

    # --- 4. ä¿®æ­£æŒ‡ç¤º (AIã«ã‚ˆã‚‹JSONç·¨é›†) (è¦ä»¶â‘­) ---
    st.header("Step 4: (ã‚ªãƒ—ã‚·ãƒ§ãƒ³) AIã«ã‚ˆã‚‹å†…å®¹ã®ä¿®æ­£æŒ‡ç¤º")
    st.markdown(f"ï¼ˆ(â˜…) ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: `{MODEL_PRO}`ï¼‰")
    
    with st.expander("AIã«ã‚¹ãƒ©ã‚¤ãƒ‰å†…å®¹ã®ä¿®æ­£ã‚’æŒ‡ç¤ºã™ã‚‹"):
        correction_prompt = st.text_area(
            "ä¿®æ­£å†…å®¹ã‚’å…·ä½“çš„ã«æŒ‡ç¤ºã—ã¦ãã ã•ã„:",
            placeholder=(
                "ä¾‹: ã€Œã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ãƒ»ã‚µãƒãƒªãƒ¼ã€ã‚¹ãƒ©ã‚¤ãƒ‰ã®ç®‡æ¡æ›¸ãã‚’3ç‚¹ã«è¦ç´„ã—ã¦ã€‚\n"
                "ä¾‹: ã€Œå…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã€ã‚¹ãƒ©ã‚¤ãƒ‰ã‚’å‰Šé™¤ã—ã¦ã€‚\n"
                "ä¾‹: å…¨ã¦ã®ã€Œçµè«–ã€ã‚’ã€Œææ¡ˆã€ã¨ã„ã†è¨€è‘‰ã«ç½®ãæ›ãˆã¦ã€‚"
            ),
            key="step_d_correction_prompt"
        )
        
        if st.button("AIã§ã‚¹ãƒ©ã‚¤ãƒ‰æ§‹æˆã‚’ä¿®æ­£", key="run_ai_correction_D", type="secondary"):
            if correction_prompt.strip():
                with st.spinner(f"AI ({MODEL_PRO}) ãŒã‚¹ãƒ©ã‚¤ãƒ‰æ§‹æˆ (JSON) ã‚’ä¿®æ­£ä¸­..."):
                    current_json_str = json.dumps(st.session_state.step_d_report_data, ensure_ascii=False)
                    corrected_json_str = run_step_d_ai_correction(current_json_str, correction_prompt)
                    
                    try:
                        corrected_data = json.loads(corrected_json_str)
                        if isinstance(corrected_data, list):
                            st.session_state.step_d_report_data = corrected_data
                            st.success("AIã«ã‚ˆã‚‹ã‚¹ãƒ©ã‚¤ãƒ‰æ§‹æˆã®ä¿®æ­£ãŒå®Œäº†ã—ã¾ã—ãŸã€‚Step 3 ã®æ§‹æˆãŒæ›´æ–°ã•ã‚Œã¦ã„ã¾ã™ã€‚")
                            st.rerun() # UIã‚’å³æ™‚æ›´æ–°
                        else:
                            st.error("AIãŒãƒªã‚¹ãƒˆå½¢å¼ã§ãªã„ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã—ã¾ã—ãŸã€‚ä¿®æ­£ã¯ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸã€‚")
                    except Exception as e:
                        st.error(f"AIã®å›ç­”ã®ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—: {e}ã€‚ä¿®æ­£ã¯ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸã€‚")
            else:
                st.warning("ä¿®æ­£æŒ‡ç¤ºã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

    # --- 5. PowerPointç”Ÿæˆãƒ»ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ (è¦ä»¶â‘¬, â‘­) ---
    st.header("Step 5: PowerPointã®ç”Ÿæˆã¨ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ")
    
    # (â˜…) --- Tipsè¡¨ç¤ºã®ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ ---
    tip_placeholder_d = st.empty()
    
    if st.button("PowerPointã‚’ç”Ÿæˆ (Step 5)", key="generate_pptx_D", type="primary", use_container_width=True):
        st.session_state.step_d_generated_pptx = None # å¤ã„ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªã‚¢
        
        # (â˜…) --- Tipsè¡¨ç¤ºã®åˆæœŸåŒ– ---
        if not st.session_state.tips_list or len(st.session_state.tips_list) <= 1:
            with st.spinner("åˆ†æTIPSã‚’AIã§ç”Ÿæˆä¸­..."):
                st.session_state.tips_list = get_analysis_tips_list_from_ai()
                if st.session_state.tips_list:
                    st.session_state.current_tip_index = random.randint(0, len(st.session_state.tips_list) - 1)
                    st.session_state.last_tip_time = time.time()
        # (â˜…) --- ã“ã“ã¾ã§ãŒå¤‰æ›´ç‚¹ ---
        
        with st.spinner("PowerPointãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆä¸­..."):
            # (â˜…) --- Tipsã®æ›´æ–°ãƒ­ã‚¸ãƒƒã‚¯ ---
            now = time.time()
            if (now - st.session_state.last_tip_time > 10): # 10ç§’ã”ã¨ã«æ›´æ–°
                if len(st.session_state.tips_list) > 1:
                    st.session_state.current_tip_index = (st.session_state.current_tip_index + 1) % len(st.session_state.tips_list)
                st.session_state.last_tip_time = now
            if st.session_state.tips_list:
                current_tip = st.session_state.tips_list[st.session_state.current_tip_index]
                tip_placeholder_d.info(f"ğŸ’¡ ãƒ‡ãƒ¼ã‚¿åˆ†æTIPS: {current_tip}")
            # (â˜…) --- ã“ã“ã¾ã§ãŒå¤‰æ›´ç‚¹ ---

            # (â˜…) --- ä¿®æ­£ã•ã‚ŒãŸPPTXç”Ÿæˆé–¢æ•°ã‚’å‘¼ã³å‡ºã— ---
            generated_file_stream = create_powerpoint_presentation(
                st.session_state.step_d_template_file,
                st.session_state.step_d_report_data # (â˜…) ãƒ‰ãƒ©ãƒƒã‚°ï¼†ãƒ‰ãƒ­ãƒƒãƒ—ã§ä¸¦ã³æ›¿ãˆæ¸ˆã¿ã®ãƒ‡ãƒ¼ã‚¿ã‚’æ¸¡ã™
            )
            
            tip_placeholder_d.empty() # (â˜…) å‡¦ç†å®Œäº†å¾Œã€Tipsã‚’æ¶ˆã™
            
            if generated_file_stream:
                st.session_state.step_d_generated_pptx = generated_file_stream.getvalue()
                st.success("PowerPointãƒ•ã‚¡ã‚¤ãƒ«ã®ç”ŸæˆãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
            else:
                st.error("PowerPointãƒ•ã‚¡ã‚¤ãƒ«ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚")

    if st.session_state.step_d_generated_pptx:
        st.download_button(
            label="ç”Ÿæˆã•ã‚ŒãŸ PowerPoint ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            data=st.session_state.step_d_generated_pptx,
            file_name="AI_Analysis_Report_v2.pptx", # (â˜…) v2
            mime="application/vnd.openxmlformats-officedocument.presentationml.presentation",
            use_container_width=True
        )
        st.balloons()


# --- 11. (â˜…) Mainé–¢æ•° (ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ) ---
def main():
    """Streamlitã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    
    # (â˜…) --- ä¿®æ­£: st.set_page_config() ã‚’æœ€åˆã«å®Ÿè¡Œ ---
    st.set_page_config(page_title="AI Data Analysis App", layout="wide")
    
    # (â˜…) --- ä¿®æ­£: ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã®åˆæœŸåŒ–ã‚’ *å…¨ã¦* ã“ã“ã«ç§»å‹• ---
    # (â˜…) ã‚¨ãƒ©ãƒ¼(AttributeError: 'log_messages') ä¿®æ­£ã®ãŸã‚ã€
    # (â˜…) load_dotenv() ãŒ logger ã‚’å‘¼ã³å‡ºã™ *å‰* ã« 'log_messages' ã‚’åˆæœŸåŒ–ã™ã‚‹
    if 'log_messages' not in st.session_state:
        st.session_state.log_messages = []
    if 'current_step' not in st.session_state:
        st.session_state.current_step = 'A' # åˆæœŸã‚¹ãƒ†ãƒƒãƒ—
    if 'tips_list' not in st.session_state:
        st.session_state.tips_list = []
    if 'current_tip_index' not in st.session_state:
        st.session_state.current_tip_index = 0
    if 'last_tip_time' not in st.session_state:
        st.session_state.last_tip_time = time.time()
    # (â˜…) --- ã“ã“ã¾ã§ãŒä¿®æ­£ç‚¹ ---

    # (â˜…) --- .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã‚€ ---
    try:
        load_dotenv()
        logger.info(".env ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿è©¦è¡Œå®Œäº†ã€‚")
    except Exception as e:
        logger.warning(f".env ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}") # (â˜…) ã“ã‚Œã§å®‰å…¨ã«å‘¼ã³å‡ºã›ã‚‹
    
    # (â˜…) ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆ (ä¸Šè¨˜ã«ç§»å‹•ã—ãŸãŸã‚ã€ã“ã“ã¯ç©º)

    # --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ (ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³) ---
    with st.sidebar:
        st.title("AI ãƒ¬ãƒãƒ¼ãƒ†ã‚£ãƒ³ã‚° App")
        st.markdown("---")
        
        st.header("âš™ï¸ AI è¨­å®š")
        
        if not os.getenv("GOOGLE_API_KEY"):
            st.warning(
                "Google APIã‚­ãƒ¼ãŒ.envãƒ•ã‚¡ã‚¤ãƒ«ã«è¨­å®šã•ã‚Œã¦ã„ãªã„ã‹ã€èª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸã€‚\n\n"
                "(.envãƒ•ã‚¡ã‚¤ãƒ«ã« `GOOGLE_API_KEY='ã‚ãªãŸã®APIã‚­ãƒ¼'` ã¨è¨˜è¼‰ã—ã¦ãã ã•ã„)"
            )
        else:
            st.success("Google APIã‚­ãƒ¼ èª­è¾¼å®Œäº†")
            # (â˜…) ã‚¢ãƒ—ãƒªèµ·å‹•æ™‚ã«LLMã¨spaCyã®ãƒ­ãƒ¼ãƒ‰ã‚’è©¦ã¿ã‚‹
            if get_llm(MODEL_FLASH_LITE) is None: 
                st.error("LLMã®åˆæœŸåŒ–ã«å¤±æ•—ã€‚APIã‚­ãƒ¼ãŒæ­£ã—ã„ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            if load_spacy_model() is None:
                st.error("spaCyãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã€‚")
        
        st.markdown("---")
        
        # (â˜…) --- Step Aã€œD ã®ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ãƒœã‚¿ãƒ³ ---
        st.header("ğŸ”„ ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³")
        current_step = st.session_state.current_step
        
        if st.button(
            "Step A: AIã‚¿ã‚°ä»˜ã‘", key="nav_A", use_container_width=True,
            type=("primary" if current_step == 'A' else "secondary")
        ):
            if st.session_state.current_step != 'A':
                st.session_state.current_step = 'A'; st.rerun()

        if st.button(
            "Step B: åˆ†æå®Ÿè¡Œ", key="nav_B", use_container_width=True,
            type=("primary" if current_step == 'B' else "secondary")
        ):
            if st.session_state.current_step != 'B':
                st.session_state.current_step = 'B'; st.rerun()

        if st.button(
            "Step C: AIãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ", key="nav_C", use_container_width=True,
            type=("primary" if current_step == 'C' else "secondary")
        ):
            if st.session_state.current_step != 'C':
                st.session_state.current_step = 'C'; st.rerun()

        if st.button(
            "Step D: PowerPointç”Ÿæˆ", key="nav_D", use_container_width=True,
            type=("primary" if current_step == 'D' else "secondary")
        ):
            if st.session_state.current_step != 'D':
                st.session_state.current_step = 'D'; st.rerun()
                
        # (â˜…) --- ã‚°ãƒ­ãƒ¼ãƒãƒ«Tipsã®ç”Ÿæˆãƒˆãƒªã‚¬ãƒ¼ ---
        st.markdown("---")
        if st.button("åˆ†æTIPSã‚’æ›´æ–°", key="reload_tips", use_container_width=True):
             with st.spinner("AI (Flash Lite) ã§TIPSã‚’ç”Ÿæˆä¸­..."):
                 # (â˜…) ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¦å¼·åˆ¶çš„ã«å†ç”Ÿæˆ
                 st.cache_data.clear()
                 st.session_state.tips_list = get_analysis_tips_list_from_ai()
                 st.session_state.current_tip_index = 0
                 st.session_state.last_tip_time = time.time()
                 st.success("TIPSã‚’æ›´æ–°ã—ã¾ã—ãŸï¼")


    # --- ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ (ã‚¹ãƒ†ãƒƒãƒ—ã«å¿œã˜ã¦æç”») ---
    if st.session_state.current_step == 'A':
        render_step_a()
    elif st.session_state.current_step == 'B':
        render_step_b()
    elif st.session_state.current_step == 'C':
        render_step_c()
    elif st.session_state.current_step == 'D':
        render_step_d()
    else:
        st.error("ä¸æ˜ãªã‚¹ãƒ†ãƒƒãƒ—ã§ã™ã€‚Step Aã«æˆ»ã‚Šã¾ã™ã€‚")
        st.session_state.current_step = 'A'; st.rerun()

if __name__ == "__main__":
    main()